@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{
      ren2020codebleumethodautomaticevaluation,
      title={CodeBLEU: a Method for Automatic Evaluation of Code Synthesis}, 
      author={Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
      year={2020},
      eprint={2009.10297},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2009.10297}, 
}

@article{di2025use,
  title={On the use of large language models in model-driven engineering},
  author={Di Rocco, Juri and Di Ruscio, Davide and Di Sipio, Claudio and Nguyen, Phuong T and Rubei, Riccardo},
  journal={Software and Systems Modeling},
  pages={1--26},
  year={2025},
  publisher={Springer}
}

@article{ CodeXHug,
  author = {Claudio, {Di Sipio} and Juri, {Di Rocco} and Davide, {Di Ruscio} and Stefano, Palombo},
  title = {CodeXHug: A curated dataset of Hugging Face pre-trained models exploited in GitHub ecosystem},
  year = {2024}
}

@misc{Richardson_Beautiful_Soup,
  author       = {Leonard Richardson},
  title        = {Beautiful Soup},
  howpublished = {\url{https://pypi.org/project/beautifulsoup4/}},
  note         = {Versione 4.12.3},
  year         = 2023,
}

@inproceedings{ait_hfcommunity_2023,
    title        = {{HFCommunity}: {A} {Tool} to {Analyze} the {Hugging} {Face} {Hub} {Community}},
    shorttitle   = {{HFCommunity}},
    author       = {Ait, Adem and Izquierdo, Javier Luis C\'{a}novas and Cabot, Jordi},
    year         = 2023,
    month        = mar,
    booktitle    = {2023 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
    pages        = {728--732},
    doi          = {10.1109/SANER56733.2023.00080},
    url          = {https://ieeexplore.ieee.org/document/10123660},
    urldate      = {2023-10-06},
    note         = {ISSN: 2640-7574},
    abstract     = {In recent years, empirical studies on software engineering practices have primarily relied on general-purpose social coding platforms such as GITHUB or GITLAB. With the emergence of Machine Learning (ML), platforms specifically designed for hosting and developing ML-based projects have appeared, being HUGGING FACE HUB one of the most popular ones. HUGGING FACE HUB focuses on facilitating the sharing of datasets, pre-trained ML models and applications built with them (spaces in HUGGING FACE HUB terminology). Besides, the Hub is adding more and more collaborative features, such as issues and pull requests, to facilitate the building of these artifacts within the platform itself. With over 100K repositories, and growing fast, HUGGING FACE HUB is therefore becoming a promising source of data on all aspects of ML projects and the community interactions around them. As such, we believe it is a promising source for all types of empirical studies aimed at analyzing the collaborative development and evolution of ML artifacts. Nevertheless, apart from the API provided by the platform, there are no easy-to-use solutions to collect and explore the different facets of HUGGING FACE HUB data, including the repositories, discussions and code evolution. To overcome this situation, in this paper we present HFCOMMUNITY, a relational database populated with HUGGING FACE HUB data to facilitate empirical analysis on the growing number of ML-related development projects.},
    file         = {IEEE Xplore Abstract Record:C\:\\Users\\claud\\Zotero\\storage\\2SFKRETL\\10123660.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\claud\\Zotero\\storage\\4VFL74U8\\Ait et al. - 2023 - HFCommunity A Tool to Analyze the Hugging Face Hu.pdf:application/pdf}
}

@inproceedings{10.3115/1073083.1073135,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}
@manual{python-concurrent-futures,
  title        = {ThreadPoolExecutor - Python Documentation},
  author       = {{Python Software Foundation}},
  year         = {2024},
  note         = {},
  url          = {https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor}
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@article{journals/corr/abs-2302-13971,
  added-at = {2025-02-11T00:00:00.000+0100},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurélien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  biburl = {https://www.bibsonomy.org/bibtex/2a469f9b2aa11c5f832b2ee67a7141821/dblp},
  ee = {https://www.wikidata.org/entity/Q117812816},
  interhash = {03a85d2a0612b9704acf6884edbe60aa},
  intrahash = {a469f9b2aa11c5f832b2ee67a7141821},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2025-02-17T07:12:13.000+0100},
  title = {LLaMA: Open and Efficient Foundation Language Models.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr2302.html#abs-2302-13971},
  volume = {abs/2302.13971},
  year = 2023
}

@misc{pygithub,
  author = {PyGithub Developers},
  title = {PyGithub Documentation},
  url = {https://pygithub.readthedocs.io/en/stable/index.html},
}

@misc{github_rest,
  author = {GitHub},
  title = {GitHub {REST API} Documentation},
  url = {https://docs.github.com/en/rest?apiVersion=2022-11-28},
}
@misc{pymongo,
  author = {MongoDB, Inc.},
  title = {PyMongo Documentation},
  url = {https://pymongo.readthedocs.io/en/stable/},
}
@article{hou2024large,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={8},
  pages={1--79},
  year={2024},
  publisher={ACM New York, NY}
}
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6/",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}."
}
@misc{huggingface,
  author = {Hugging {Face}},
  title = {Hugging {Face} Website},
  url = {https://huggingface.co/},
}
@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}
@misc{mielke2021wordscharactersbriefhistory,
      title={Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP}, 
      author={Sabrina J. Mielke and Zaid Alyafeai and Elizabeth Salesky and Colin Raffel and Manan Dey and Matthias Gallé and Arun Raja and Chenglei Si and Wilson Y. Lee and Benoît Sagot and Samson Tan},
      year={2021},
      eprint={2112.10508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.10508}, 
}
@misc{peeperkorn2024temperaturecreativityparameterlarge,
      title={Is Temperature the Creativity Parameter of Large Language Models?}, 
      author={Max Peeperkorn and Tom Kouwenhoven and Dan Brown and Anna Jordanous},
      year={2024},
      eprint={2405.00492},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.00492}, 
}
@misc{feng2020codebertpretrainedmodelprogramming,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      eprint={2002.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08155}, 
}
@misc{huggingface_model_cards,
  author = {{Hugging Face}},
  title = {Hugging Face Model Cards},
  year = {2025},
  url = {https://huggingface.co/docs/hub/model-cards},
  note = {Ultimo accesso: 2 marzo 2025}
}
@misc{liang2024whatsdocumentedaisystematic,
      title={What's documented in AI? Systematic Analysis of 32K AI Model Cards}, 
      author={Weixin Liang and Nazneen Rajani and Xinyu Yang and Ezinwanne Ozoani and Eric Wu and Yiqun Chen and Daniel Scott Smith and James Zou},
      year={2024},
      eprint={2402.05160},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2402.05160}, 
}
@misc{huggingface_transformers_tokenizer,
  author       = {Hugging Face},
  title        = {Transformers: Tokenizer},
  year         = 2025,
  url          = {https://huggingface.co/docs/transformers/en/main_classes/tokenizer}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}
@article{Sennrich2015NeuralMT,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Rico Sennrich and Barry Haddow and Alexandra Birch},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.07909},
  url={https://api.semanticscholar.org/CorpusID:1114678}
}
@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}
@misc{all-MiniLM-L6-v2,
  author       = {Sentence-Transformers Team},
  title        = {all-MiniLM-L6-v2},
  year         = {2020},
  url          = {https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}
}
@misc{Ruman2024,
  author = {Ruman},
  title = {Setting Top-K, Top-P and Temperature in LLMs},
  year = {2024},
  howpublished = {Medium post},
  url = {https://rumn.medium.com/setting-top-k-top-p-and-temperature-in-llms-3da3a8f74832},
  note = {Accessed: 2025-03-03}
}
@misc{python_ast,
  author       = "{Python Software Foundation}",
  title        = "The {AST} Module",
  year         = "2025",
  url          = "https://docs.python.org/3/library/ast.html",
  note         = "Accessed: 2025-03-04"
}
@article{nguyen2021recommending,
  title={Recommending api function calls and code snippets to support software development},
  author={Nguyen, Phuong T and Di Rocco, Juri and Di Sipio, Claudio and Di Ruscio, Davide and Di Penta, Massimiliano},
  journal={IEEE Transactions on Software Engineering},
  volume={48},
  number={7},
  pages={2417--2438},
  year={2021},
  publisher={IEEE}
}