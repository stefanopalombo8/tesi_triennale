{
    "sentence-transformers/all-MiniLM-L12-v2": {
        "code_bleu": [
            {
                "codebleu": 0.19128619696261573
            },
            {
                "ngram_match_score": 0.009490693307261855
            },
            {
                "weighted_ngram_match_score": 0.011516306620395477
            },
            {
                "syntax_match_score": 0.3770491803278688
            },
            {
                "dataflow_match_score": 0.3670886075949367
            }
        ],
        "cosine_similarity": 0.974054217338562,
        "meteor": 0.09134536918391657
    },
    "roberta-large": {
        "code_bleu": [
            {
                "codebleu": 0.2080920615622155
            },
            {
                "ngram_match_score": 0.0006958774170994696
            },
            {
                "weighted_ngram_match_score": 0.0011011196164454672
            },
            {
                "syntax_match_score": 0.44074074074074077
            },
            {
                "dataflow_match_score": 0.3898305084745763
            }
        ],
        "cosine_similarity": 0.9513201117515564,
        "meteor": 0.02243829468960359
    },
    "sentence-transformers/all-MiniLM-L6-v2": {
        "code_bleu": [
            {
                "codebleu": 0.18956966121618352
            },
            {
                "ngram_match_score": 0.004715223249874999
            },
            {
                "weighted_ngram_match_score": 0.004964127155427568
            },
            {
                "syntax_match_score": 0.36885245901639346
            },
            {
                "dataflow_match_score": 0.379746835443038
            }
        ],
        "cosine_similarity": 0.9875752925872803,
        "meteor": 0.06792452830188678
    },
    "openai/clip-vit-large-patch14": {
        "code_bleu": [
            {
                "codebleu": 0.12484922619591601
            },
            {
                "ngram_match_score": 0.002019772503828753
            },
            {
                "weighted_ngram_match_score": 0.004845256141401827
            },
            {
                "syntax_match_score": 0.11475409836065574
            },
            {
                "dataflow_match_score": 0.37777777777777777
            }
        ],
        "cosine_similarity": 0.9313489198684692,
        "meteor": 0.06684491978609626
    },
    "bert-base-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.16579995882798187
            },
            {
                "ngram_match_score": 0.020302035162201103
            },
            {
                "weighted_ngram_match_score": 0.03172391439956952
            },
            {
                "syntax_match_score": 0.05185185185185185
            },
            {
                "dataflow_match_score": 0.559322033898305
            }
        ],
        "cosine_similarity": 0.9821874499320984,
        "meteor": 0.05065162574651625
    },
    "distilbert/distilbert-base-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.22438071140085816
            },
            {
                "ngram_match_score": 0.0005949765361019625
            },
            {
                "weighted_ngram_match_score": 0.002389262664317526
            },
            {
                "syntax_match_score": 0.5555555555555556
            },
            {
                "dataflow_match_score": 0.3389830508474576
            }
        ],
        "cosine_similarity": 0.975761353969574,
        "meteor": 0.024834437086092714
    },
    "gpt2": {
        "code_bleu": [
            {
                "codebleu": 0.1969281672410842
            },
            {
                "ngram_match_score": 0.0015396647726504777
            },
            {
                "weighted_ngram_match_score": 0.0027628659428383788
            },
            {
                "syntax_match_score": 0.3548387096774194
            },
            {
                "dataflow_match_score": 0.42857142857142855
            }
        ],
        "cosine_similarity": 0.9680807590484619,
        "meteor": 0.03658131027602261
    },
    "sentence-transformers/all-mpnet-base-v2": {
        "code_bleu": [
            {
                "codebleu": 0.20589861839290666
            },
            {
                "ngram_match_score": 0.005750418516205211
            },
            {
                "weighted_ngram_match_score": 0.005953621355483693
            },
            {
                "syntax_match_score": 0.36885245901639346
            },
            {
                "dataflow_match_score": 0.4430379746835443
            }
        ],
        "cosine_similarity": 0.9885951280593872,
        "meteor": 0.06756756756756756
    },
    "xlm-roberta-base": {
        "code_bleu": [
            {
                "codebleu": 0.2836963167032579
            },
            {
                "ngram_match_score": 0.002253661328525286
            },
            {
                "weighted_ngram_match_score": 0.002482344400762362
            },
            {
                "syntax_match_score": 0.37142857142857144
            },
            {
                "dataflow_match_score": 0.7586206896551724
            }
        ],
        "cosine_similarity": 0.9546974897384644,
        "meteor": 0.03164556962025317
    },
    "distilbert-base-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.2596245130732699
            },
            {
                "ngram_match_score": 0.0055655465542722845
            },
            {
                "weighted_ngram_match_score": 0.010773058155630988
            },
            {
                "syntax_match_score": 0.5814814814814815
            },
            {
                "dataflow_match_score": 0.4406779661016949
            }
        ],
        "cosine_similarity": 0.9760518670082092,
        "meteor": 0.03981485752114189
    },
    "distilgpt2": {
        "code_bleu": [
            {
                "codebleu": 0.21431250462260687
            },
            {
                "ngram_match_score": 0.001092609934257845
            },
            {
                "weighted_ngram_match_score": 0.002551559436860978
            },
            {
                "syntax_match_score": 0.4124293785310734
            },
            {
                "dataflow_match_score": 0.4411764705882353
            }
        ],
        "cosine_similarity": 0.9621188640594482,
        "meteor": 0.03096539162112933
    },
    "distilbert-base-uncased-finetuned-sst-2-english": {
        "code_bleu": [
            {
                "codebleu": 0.22269257576807328
            },
            {
                "ngram_match_score": 0.005547364907138795
            },
            {
                "weighted_ngram_match_score": 0.04644298173813905
            },
            {
                "syntax_match_score": 0.09803921568627451
            },
            {
                "dataflow_match_score": 0.7407407407407407
            }
        ],
        "cosine_similarity": 0.9507007598876953,
        "meteor": 0.05307855626326964
    },
    "microsoft/trocr-base-handwritten": {
        "code_bleu": [
            {
                "codebleu": 0.03488372093023256
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.13953488372093023
            }
        ],
        "cosine_similarity": 0.9503777027130127,
        "meteor": 0.0
    },
    "google/flan-t5-large": {
        "code_bleu": [
            {
                "codebleu": 0.08238881783806709
            },
            {
                "ngram_match_score": 0.024863758121219954
            },
            {
                "weighted_ngram_match_score": 0.04828819432772953
            },
            {
                "syntax_match_score": 0.07386363636363637
            },
            {
                "dataflow_match_score": 0.18253968253968253
            }
        ],
        "cosine_similarity": 0.9689297676086426,
        "meteor": 0.08895499334982768
    },
    "facebook/bart-large-cnn": {
        "code_bleu": [
            {
                "codebleu": 0.18435686227647208
            },
            {
                "ngram_match_score": 0.0014251516931308161
            },
            {
                "weighted_ngram_match_score": 0.002668964079424122
            },
            {
                "syntax_match_score": 0.23333333333333334
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9330694675445557,
        "meteor": 0.04643206256109481
    },
    "Qwen/Qwen-VL-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.14800351748240936
            },
            {
                "ngram_match_score": 0.0022651955653338013
            },
            {
                "weighted_ngram_match_score": 0.002985804286439252
            },
            {
                "syntax_match_score": 0.3448275862068966
            },
            {
                "dataflow_match_score": 0.24193548387096775
            }
        ],
        "cosine_similarity": 0.9823503494262695,
        "meteor": 0.07541478129713425
    },
    "bert-base-cased": {
        "code_bleu": [
            {
                "codebleu": 0.198187012180992
            },
            {
                "ngram_match_score": 0.0005441787778913656
            },
            {
                "weighted_ngram_match_score": 0.0008039954953546632
            },
            {
                "syntax_match_score": 0.4185185185185185
            },
            {
                "dataflow_match_score": 0.3728813559322034
            }
        ],
        "cosine_similarity": 0.9343000650405884,
        "meteor": 0.02361673414304993
    },
    "facebook/opt-125m": {
        "code_bleu": [
            {
                "codebleu": 0.21038494808382424
            },
            {
                "ngram_match_score": 0.0023072496703118615
            },
            {
                "weighted_ngram_match_score": 0.005899209331651708
            },
            {
                "syntax_match_score": 0.18333333333333332
            },
            {
                "dataflow_match_score": 0.65
            }
        ],
        "cosine_similarity": 0.9497703909873962,
        "meteor": 0.06250000000000001
    },
    "google/vit-base-patch16-224": {
        "code_bleu": [
            {
                "codebleu": 0.22407170146837518
            },
            {
                "ngram_match_score": 0.002442703760042221
            },
            {
                "weighted_ngram_match_score": 0.006398214667571038
            },
            {
                "syntax_match_score": 0.36363636363636365
            },
            {
                "dataflow_match_score": 0.5238095238095238
            }
        ],
        "cosine_similarity": 0.9433656334877014,
        "meteor": 0.10380622837370242
    },
    "albert-base-v2": {
        "code_bleu": [
            {
                "codebleu": 0.20466776370404838
            },
            {
                "ngram_match_score": 0.0019887488777665654
            },
            {
                "weighted_ngram_match_score": 0.002495237514070338
            },
            {
                "syntax_match_score": 0.4074074074074074
            },
            {
                "dataflow_match_score": 0.4067796610169492
            }
        ],
        "cosine_similarity": 0.9725468754768372,
        "meteor": 0.028691660290742157
    },
    "stabilityai/stable-diffusion-xl-base-1.0": {
        "code_bleu": [
            {
                "codebleu": 0.11237428666802399
            },
            {
                "ngram_match_score": 4.2173732278121615e-05
            },
            {
                "weighted_ngram_match_score": 0.0012842412325007472
            },
            {
                "syntax_match_score": 0.3231707317073171
            },
            {
                "dataflow_match_score": 0.125
            }
        ],
        "cosine_similarity": 0.9454053640365601,
        "meteor": 0.014220705346985213
    },
    "nomic-ai/nomic-embed-text-v1": {
        "code_bleu": [
            {
                "codebleu": 0.25786525486960804
            },
            {
                "ngram_match_score": 0.0963022734853009
            },
            {
                "weighted_ngram_match_score": 0.13900168667817736
            },
            {
                "syntax_match_score": 0.5263157894736842
            },
            {
                "dataflow_match_score": 0.2698412698412698
            }
        ],
        "cosine_similarity": 0.9909868836402893,
        "meteor": 0.18867924528301888
    },
    "bert-base-multilingual-cased": {
        "code_bleu": [
            {
                "codebleu": 0.20832538423635535
            },
            {
                "ngram_match_score": 0.004969308711888149
            },
            {
                "weighted_ngram_match_score": 0.005048646143981066
            },
            {
                "syntax_match_score": 0.34328358208955223
            },
            {
                "dataflow_match_score": 0.48
            }
        ],
        "cosine_similarity": 0.9753566980361938,
        "meteor": 0.058780308596620125
    },
    "almanach/camembert-base": {
        "code_bleu": [
            {
                "codebleu": 0.18538729435214607
            },
            {
                "ngram_match_score": 0.0012762992265687063
            },
            {
                "weighted_ngram_match_score": 0.0025383904475279177
            },
            {
                "syntax_match_score": 0.3409090909090909
            },
            {
                "dataflow_match_score": 0.3968253968253968
            }
        ],
        "cosine_similarity": 0.9749549031257629,
        "meteor": 0.059706584783350385
    },
    "stabilityai/stable-diffusion-3-medium": {
        "code_bleu": [
            {
                "codebleu": 0.38955710002970634
            },
            {
                "ngram_match_score": 0.04629437958350973
            },
            {
                "weighted_ngram_match_score": 0.22068402053531555
            },
            {
                "syntax_match_score": 0.53125
            },
            {
                "dataflow_match_score": 0.76
            }
        ],
        "cosine_similarity": 0.9514503479003906,
        "meteor": 0.17587939698492464
    },
    "SamLowe/roberta-base-go/emotions": {
        "code_bleu": [
            {
                "codebleu": 0.292817513422443
            },
            {
                "ngram_match_score": 0.0024065919238633405
            },
            {
                "weighted_ngram_match_score": 0.008863461765908675
            },
            {
                "syntax_match_score": 0.36
            },
            {
                "dataflow_match_score": 0.8
            }
        ],
        "cosine_similarity": 0.9025645852088928,
        "meteor": 0.08557457212713937
    },
    "distilroberta-base": {
        "code_bleu": [
            {
                "codebleu": 0.1875471065538708
            },
            {
                "ngram_match_score": 0.006539824675375322
            },
            {
                "weighted_ngram_match_score": 0.00933022964158545
            },
            {
                "syntax_match_score": 0.02843601895734597
            },
            {
                "dataflow_match_score": 0.7058823529411765
            }
        ],
        "cosine_similarity": 0.9647246599197388,
        "meteor": 0.03743180864456567
    },
    "distilbert-base-multilingual-cased": {
        "code_bleu": [
            {
                "codebleu": 0.38117948119279554
            },
            {
                "ngram_match_score": 0.001949174943170598
            },
            {
                "weighted_ngram_match_score": 0.0027687498280114424
            },
            {
                "syntax_match_score": 0.52
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9521727561950684,
        "meteor": 0.012594458438287152
    },
    "papluca/xlm-roberta-base-language-detection": {
        "code_bleu": [
            {
                "codebleu": 0.22768557751688362
            },
            {
                "ngram_match_score": 0.00490886536817194
            },
            {
                "weighted_ngram_match_score": 0.006792348808951515
            },
            {
                "syntax_match_score": 0.31
            },
            {
                "dataflow_match_score": 0.589041095890411
            }
        ],
        "cosine_similarity": 0.9677421450614929,
        "meteor": 0.09737098344693282
    },
    "meta-llama/Meta-Llama-3-8B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.14253769562767
            },
            {
                "ngram_match_score": 0.0021236845110698334
            },
            {
                "weighted_ngram_match_score": 0.0027724736561526827
            },
            {
                "syntax_match_score": 0.2980132450331126
            },
            {
                "dataflow_match_score": 0.2672413793103448
            }
        ],
        "cosine_similarity": 0.9648098945617676,
        "meteor": 0.04942965779467681
    },
    "sentence-transformers/bert-base-nli-mean-tokens": {
        "code_bleu": [
            {
                "codebleu": 0.21959183437072166
            },
            {
                "ngram_match_score": 0.0490677239032778
            },
            {
                "weighted_ngram_match_score": 0.1108731125444121
            },
            {
                "syntax_match_score": 0.42857142857142855
            },
            {
                "dataflow_match_score": 0.2898550724637681
            }
        ],
        "cosine_similarity": 0.9853165149688721,
        "meteor": 0.16105971797794488
    },
    "sentence-transformers/multi-qa-MiniLM-L6-cos-v1": {
        "code_bleu": [
            {
                "codebleu": 0.32211743150258687
            },
            {
                "ngram_match_score": 0.00040240423182579194
            },
            {
                "weighted_ngram_match_score": 0.0011108000393911142
            },
            {
                "syntax_match_score": 0.28695652173913044
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9574333429336548,
        "meteor": 0.04404208465867384
    },
    "lxyuan/distilbert-base-multilingual-cased-sentiments-student": {
        "code_bleu": [
            {
                "codebleu": 0.27170845086007067
            },
            {
                "ngram_match_score": 0.0022632070962058746
            },
            {
                "weighted_ngram_match_score": 0.003276890050370586
            },
            {
                "syntax_match_score": 0.4659090909090909
            },
            {
                "dataflow_match_score": 0.6153846153846154
            }
        ],
        "cosine_similarity": 0.9782140851020813,
        "meteor": 0.03614457831325301
    },
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": {
        "code_bleu": [
            {
                "codebleu": 0.09792828555136057
            },
            {
                "ngram_match_score": 0.0018039375443779484
            },
            {
                "weighted_ngram_match_score": 0.00274564358445979
            },
            {
                "syntax_match_score": 0.2857142857142857
            },
            {
                "dataflow_match_score": 0.10144927536231885
            }
        ],
        "cosine_similarity": 0.983095645904541,
        "meteor": 0.04493464052287582
    },
    "intfloat/multilingual-e5-small": {
        "code_bleu": [
            {
                "codebleu": 0.1877426682047145
            },
            {
                "ngram_match_score": 0.0012574714786708198
            },
            {
                "weighted_ngram_match_score": 0.0024013733831978953
            },
            {
                "syntax_match_score": 0.3064516129032258
            },
            {
                "dataflow_match_score": 0.44086021505376344
            }
        ],
        "cosine_similarity": 0.9798136949539185,
        "meteor": 0.05130927105449398
    },
    "sentence-transformers/paraphrase-MiniLM-L6-v2": {
        "code_bleu": [
            {
                "codebleu": 0.21635414132416445
            },
            {
                "ngram_match_score": 0.005783525193327854
            },
            {
                "weighted_ngram_match_score": 0.00637217053811254
            },
            {
                "syntax_match_score": 0.375
            },
            {
                "dataflow_match_score": 0.4782608695652174
            }
        ],
        "cosine_similarity": 0.991810142993927,
        "meteor": 0.07861613647651533
    },
    "nlpconnect/vit-gpt2-image-captioning": {
        "code_bleu": [
            {
                "codebleu": 0.1892480369634419
            },
            {
                "ngram_match_score": 0.005820055318418327
            },
            {
                "weighted_ngram_match_score": 0.0059476788065093945
            },
            {
                "syntax_match_score": 0.39416058394160586
            },
            {
                "dataflow_match_score": 0.35106382978723405
            }
        ],
        "cosine_similarity": 0.9900451302528381,
        "meteor": 0.10975609756097561
    },
    "petals-team/StableBeluga2": {
        "code_bleu": [
            {
                "codebleu": 0.10620571369724556
            },
            {
                "ngram_match_score": 0.0019348418054402619
            },
            {
                "weighted_ngram_match_score": 0.0025666929661733325
            },
            {
                "syntax_match_score": 0.12244897959183673
            },
            {
                "dataflow_match_score": 0.2978723404255319
            }
        ],
        "cosine_similarity": 0.9581080675125122,
        "meteor": 0.03219871205151794
    },
    "camembert-base": {
        "code_bleu": [
            {
                "codebleu": 0.20744674266793706
            },
            {
                "ngram_match_score": 0.0009331896366596764
            },
            {
                "weighted_ngram_match_score": 0.0023747045560120563
            },
            {
                "syntax_match_score": 0.4772727272727273
            },
            {
                "dataflow_match_score": 0.3492063492063492
            }
        ],
        "cosine_similarity": 0.9736194014549255,
        "meteor": 0.04459691252144082
    },
    "Alibaba-NLP/gte-large-en-v1.5": {
        "code_bleu": [
            {
                "codebleu": 0.10642576831650688
            },
            {
                "ngram_match_score": 0.003760086781973205
            },
            {
                "weighted_ngram_match_score": 0.004160248388816271
            },
            {
                "syntax_match_score": 0.23809523809523808
            },
            {
                "dataflow_match_score": 0.1796875
            }
        ],
        "cosine_similarity": 0.9667729139328003,
        "meteor": 0.06729810568295114
    },
    "intfloat/e5-small-v2": {
        "code_bleu": [
            {
                "codebleu": 0.1416820593034891
            },
            {
                "ngram_match_score": 0.00048644328194024055
            },
            {
                "weighted_ngram_match_score": 0.0011880304911559322
            },
            {
                "syntax_match_score": 0.29838709677419356
            },
            {
                "dataflow_match_score": 0.26666666666666666
            }
        ],
        "cosine_similarity": 0.9779864549636841,
        "meteor": 0.02040175768989328
    },
    "CompVis/stable-diffusion-v1-4": {
        "code_bleu": [
            {
                "codebleu": 0.09131257057315388
            },
            {
                "ngram_match_score": 0.0003410273522277726
            },
            {
                "weighted_ngram_match_score": 0.0008693998679239708
            },
            {
                "syntax_match_score": 0.17708333333333334
            },
            {
                "dataflow_match_score": 0.18695652173913044
            }
        ],
        "cosine_similarity": 0.9822732210159302,
        "meteor": 0.008012820512820512
    },
    "meta-llama/Meta-Llama-3-8B": {
        "code_bleu": [
            {
                "codebleu": 0.2652970179862445
            },
            {
                "ngram_match_score": 0.004887945201591423
            },
            {
                "weighted_ngram_match_score": 0.01645192939993311
            },
            {
                "syntax_match_score": 0.45161290322580644
            },
            {
                "dataflow_match_score": 0.5882352941176471
            }
        ],
        "cosine_similarity": 0.9027884006500244,
        "meteor": 0.05952380952380953
    },
    "bert-large-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.18550392759775808
            },
            {
                "ngram_match_score": 0.0005590537828194233
            },
            {
                "weighted_ngram_match_score": 0.0009042397846096255
            },
            {
                "syntax_match_score": 0.4185185185185185
            },
            {
                "dataflow_match_score": 0.3220338983050847
            }
        ],
        "cosine_similarity": 0.9555124044418335,
        "meteor": 0.015228426395939085
    },
    "xlm-roberta-large": {
        "code_bleu": [
            {
                "codebleu": 0.18510249501329054
            },
            {
                "ngram_match_score": 0.0025263279615826747
            },
            {
                "weighted_ngram_match_score": 0.002579875408492441
            },
            {
                "syntax_match_score": 0.3904761904761905
            },
            {
                "dataflow_match_score": 0.3448275862068966
            }
        ],
        "cosine_similarity": 0.9263100624084473,
        "meteor": 0.03646308113035551
    },
    "emilyalsentzer/Bio/ClinicalBERT": {
        "code_bleu": [
            {
                "codebleu": 0.2392381214172617
            },
            {
                "ngram_match_score": 0.002104351645526129
            },
            {
                "weighted_ngram_match_score": 0.019554016376461803
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.9168192148208618,
        "meteor": 0.04975124378109453
    },
    "nomic-ai/nomic-embed-text-v1.5": {
        "code_bleu": [
            {
                "codebleu": 0.05182469066700596
            },
            {
                "ngram_match_score": 0.0008027950428884308
            },
            {
                "weighted_ngram_match_score": 0.00208970301550473
            },
            {
                "syntax_match_score": 0.1681159420289855
            },
            {
                "dataflow_match_score": 0.036290322580645164
            }
        ],
        "cosine_similarity": 0.9890091419219971,
        "meteor": 0.045507511193889454
    },
    "cardiffnlp/twitter-roberta-base-sentiment": {
        "code_bleu": [
            {
                "codebleu": 0.1504219904707095
            },
            {
                "ngram_match_score": 0.0028246184231331883
            },
            {
                "weighted_ngram_match_score": 0.0041957908204756375
            },
            {
                "syntax_match_score": 0.34673366834170855
            },
            {
                "dataflow_match_score": 0.24793388429752067
            }
        ],
        "cosine_similarity": 0.9622344970703125,
        "meteor": 0.051686615886833515
    },
    "sentence-transformers/distiluse-base-multilingual-cased-v2": {
        "code_bleu": [
            {
                "codebleu": 0.18360326949624173
            },
            {
                "ngram_match_score": 0.001240344865209797
            },
            {
                "weighted_ngram_match_score": 0.013475763422787423
            },
            {
                "syntax_match_score": 0.13636363636363635
            },
            {
                "dataflow_match_score": 0.5833333333333334
            }
        ],
        "cosine_similarity": 0.8939663171768188,
        "meteor": 0.02702702702702703
    },
    "Falconsai/nsfw/image/detection": {
        "code_bleu": [
            {
                "codebleu": 0.1856723218526241
            },
            {
                "ngram_match_score": 0.004993602968834348
            },
            {
                "weighted_ngram_match_score": 0.009615773231895186
            },
            {
                "syntax_match_score": 0.29411764705882354
            },
            {
                "dataflow_match_score": 0.4339622641509434
            }
        ],
        "cosine_similarity": 0.9637243747711182,
        "meteor": 0.1271746031746032
    },
    "google/byt5-small": {
        "code_bleu": [
            {
                "codebleu": 0.1423464769882905
            },
            {
                "ngram_match_score": 0.0022439633938682152
            },
            {
                "weighted_ngram_match_score": 0.003336185101360438
            },
            {
                "syntax_match_score": 0.2391304347826087
            },
            {
                "dataflow_match_score": 0.3246753246753247
            }
        ],
        "cosine_similarity": 0.967146635055542,
        "meteor": 0.058195926285160036
    },
    "Salesforce/blip-image-captioning-large": {
        "code_bleu": [
            {
                "codebleu": 0.1457826899732661
            },
            {
                "ngram_match_score": 0.0033769729870397557
            },
            {
                "weighted_ngram_match_score": 0.0035401036820322494
            },
            {
                "syntax_match_score": 0.3236363636363636
            },
            {
                "dataflow_match_score": 0.25257731958762886
            }
        ],
        "cosine_similarity": 0.9866257905960083,
        "meteor": 0.07470953912111471
    },
    "Salesforce/blip-image-captioning-base": {
        "code_bleu": [
            {
                "codebleu": 0.0888740767727619
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.19494584837545126
            },
            {
                "dataflow_match_score": 0.16055045871559634
            }
        ],
        "cosine_similarity": 0.9693621397018433,
        "meteor": 0.0
    },
    "bert-base-chinese": {
        "code_bleu": [
            {
                "codebleu": 0.19119088007953566
            },
            {
                "ngram_match_score": 0.0018158475126260065
            },
            {
                "weighted_ngram_match_score": 0.027653555158457886
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9346880912780762,
        "meteor": 0.08368200836820085
    },
    "facebook/m2m100/418M": {
        "code_bleu": [
            {
                "codebleu": 0.3096041258220754
            },
            {
                "ngram_match_score": 0.015006287643713579
            },
            {
                "weighted_ngram_match_score": 0.0367435489779215
            },
            {
                "syntax_match_score": 0.18666666666666668
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9617995023727417,
        "meteor": 0.044065804935370156
    },
    "tiiuae/falcon-7b-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.17629577385464978
            },
            {
                "ngram_match_score": 0.0025935678100118
            },
            {
                "weighted_ngram_match_score": 0.0029579365425039987
            },
            {
                "syntax_match_score": 0.45544554455445546
            },
            {
                "dataflow_match_score": 0.2441860465116279
            }
        ],
        "cosine_similarity": 0.9771565198898315,
        "meteor": 0.0691085003455425
    },
    "BAAI/bge-reranker-v2-m3": {
        "code_bleu": [
            {
                "codebleu": 0.11457788939936256
            },
            {
                "ngram_match_score": 0.0004285912112786984
            },
            {
                "weighted_ngram_match_score": 0.004210387899921875
            },
            {
                "syntax_match_score": 0.3219761499148211
            },
            {
                "dataflow_match_score": 0.13169642857142858
            }
        ],
        "cosine_similarity": 0.9626196622848511,
        "meteor": 0.03313696612665685
    },
    "thenlper/gte-base": {
        "code_bleu": [
            {
                "codebleu": 0.1917350498943268
            },
            {
                "ngram_match_score": 0.0028756347944308746
            },
            {
                "weighted_ngram_match_score": 0.0028984124359540658
            },
            {
                "syntax_match_score": 0.2824427480916031
            },
            {
                "dataflow_match_score": 0.4787234042553192
            }
        ],
        "cosine_similarity": 0.9859018325805664,
        "meteor": 0.06234413965087282
    },
    "microsoft/resnet-50": {
        "code_bleu": [
            {
                "codebleu": 0.13513513513513514
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.5405405405405406
            }
        ],
        "cosine_similarity": 0.9454914927482605,
        "meteor": 0.0
    },
    "nlpaueb/legal-bert-base-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.2980834604513519
            },
            {
                "ngram_match_score": 0.00219159013482822
            },
            {
                "weighted_ngram_match_score": 0.019554016376461803
            },
            {
                "syntax_match_score": 0.47058823529411764
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.9251527190208435,
        "meteor": 0.05076142131979696
    },
    "cross-encoder/ms-marco-MiniLM-L-6-v2": {
        "code_bleu": [
            {
                "codebleu": 0.277375068218184
            },
            {
                "ngram_match_score": 0.02637127606374109
            },
            {
                "weighted_ngram_match_score": 0.05080576448576261
            },
            {
                "syntax_match_score": 0.45454545454545453
            },
            {
                "dataflow_match_score": 0.5777777777777777
            }
        ],
        "cosine_similarity": 0.9602804780006409,
        "meteor": 0.09988249118683899
    },
    "stabilityai/stable-diffusion-2-1-base": {
        "code_bleu": [
            {
                "codebleu": 0.20287772059863352
            },
            {
                "ngram_match_score": 0.005689975342041942
            },
            {
                "weighted_ngram_match_score": 0.038459795941380964
            },
            {
                "syntax_match_score": 0.1111111111111111
            },
            {
                "dataflow_match_score": 0.65625
            }
        ],
        "cosine_similarity": 0.9472635984420776,
        "meteor": 0.05576208178438662
    },
    "thenlper/gte-small": {
        "code_bleu": [
            {
                "codebleu": 0.22352373209835266
            },
            {
                "ngram_match_score": 0.006995599301468484
            },
            {
                "weighted_ngram_match_score": 0.008668275007160681
            },
            {
                "syntax_match_score": 0.33587786259541985
            },
            {
                "dataflow_match_score": 0.5425531914893617
            }
        ],
        "cosine_similarity": 0.9791420698165894,
        "meteor": 0.09740259740259741
    },
    "Rostlab/prot/t5/xl/uniref50": {
        "code_bleu": [
            {
                "codebleu": 0.13390780910526132
            },
            {
                "ngram_match_score": 0.0008325603217605397
            },
            {
                "weighted_ngram_match_score": 0.0014084478082669516
            },
            {
                "syntax_match_score": 0.09302325581395349
            },
            {
                "dataflow_match_score": 0.44036697247706424
            }
        ],
        "cosine_similarity": 0.9645088911056519,
        "meteor": 0.042872454448017155
    },
    "intfloat/e5-large-v2": {
        "code_bleu": [
            {
                "codebleu": 0.23301757869072953
            },
            {
                "ngram_match_score": 0.004267325356575547
            },
            {
                "weighted_ngram_match_score": 0.0073728818794609125
            },
            {
                "syntax_match_score": 0.3870967741935484
            },
            {
                "dataflow_match_score": 0.5333333333333333
            }
        ],
        "cosine_similarity": 0.9912947416305542,
        "meteor": 0.09323225399563746
    },
    "sentence-transformers/distilbert-base-nli-stsb-mean-tokens": {
        "code_bleu": [
            {
                "codebleu": 0.24189103074642912
            },
            {
                "ngram_match_score": 0.002616403084584688
            },
            {
                "weighted_ngram_match_score": 0.003508796505686676
            },
            {
                "syntax_match_score": 0.4107142857142857
            },
            {
                "dataflow_match_score": 0.5507246376811594
            }
        ],
        "cosine_similarity": 0.9695475101470947,
        "meteor": 0.08852963818321785
    },
    "sentence-transformers/multi-qa-mpnet-base-dot-v1": {
        "code_bleu": [
            {
                "codebleu": 0.04402621244910169
            },
            {
                "ngram_match_score": 0.0003021132799494534
            },
            {
                "weighted_ngram_match_score": 0.0012250625960055334
            },
            {
                "syntax_match_score": 0.011976047904191617
            },
            {
                "dataflow_match_score": 0.16260162601626016
            }
        ],
        "cosine_similarity": 0.93829345703125,
        "meteor": 0.004574565416285452
    },
    "j-hartmann/emotion-english-distilroberta-base": {
        "code_bleu": [
            {
                "codebleu": 0.26537949765105573
            },
            {
                "ngram_match_score": 0.002023792894760146
            },
            {
                "weighted_ngram_match_score": 0.017827531042796257
            },
            {
                "syntax_match_score": 0.375
            },
            {
                "dataflow_match_score": 0.6666666666666666
            }
        ],
        "cosine_similarity": 0.905810534954071,
        "meteor": 0.04672897196261683
    },
    "sentence-transformers/all-roberta-large-v1": {
        "code_bleu": [
            {
                "codebleu": 0.19695081891471328
            },
            {
                "ngram_match_score": 0.001866332993966641
            },
            {
                "weighted_ngram_match_score": 0.003098179436000703
            },
            {
                "syntax_match_score": 0.3524590163934426
            },
            {
                "dataflow_match_score": 0.43037974683544306
            }
        ],
        "cosine_similarity": 0.9771571159362793,
        "meteor": 0.07757404795486601
    },
    "TheBloke/Llama-2-7B-Chat-GGUF": {
        "code_bleu": [
            {
                "codebleu": 0.24550382629657894
            },
            {
                "ngram_match_score": 0.005045059020298931
            },
            {
                "weighted_ngram_match_score": 0.012684531880302536
            },
            {
                "syntax_match_score": 0.38095238095238093
            },
            {
                "dataflow_match_score": 0.5833333333333334
            }
        ],
        "cosine_similarity": 0.9404398202896118,
        "meteor": 0.059989567031820554
    },
    "distilbert-base-cased": {
        "code_bleu": [
            {
                "codebleu": 0.12548298717286593
            },
            {
                "ngram_match_score": 0.002896320572321833
            },
            {
                "weighted_ngram_match_score": 0.00493644418944955
            },
            {
                "syntax_match_score": 0.07037037037037037
            },
            {
                "dataflow_match_score": 0.423728813559322
            }
        ],
        "cosine_similarity": 0.9561164379119873,
        "meteor": 0.014446227929373997
    },
    "bert-base-multilingual-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.1981835290033947
            },
            {
                "ngram_match_score": 0.0005078163687741312
            },
            {
                "weighted_ngram_match_score": 0.0008264251940828196
            },
            {
                "syntax_match_score": 0.4185185185185185
            },
            {
                "dataflow_match_score": 0.3728813559322034
            }
        ],
        "cosine_similarity": 0.9637813568115234,
        "meteor": 0.009963467286615742
    },
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": {
        "code_bleu": [
            {
                "codebleu": 0.24057147212070487
            },
            {
                "ngram_match_score": 0.022506276357275496
            },
            {
                "weighted_ngram_match_score": 0.031912117301527425
            },
            {
                "syntax_match_score": 0.35714285714285715
            },
            {
                "dataflow_match_score": 0.5507246376811594
            }
        ],
        "cosine_similarity": 0.9724218845367432,
        "meteor": 0.10695187165775401
    },
    "distilbert/distilbert-base-cased": {
        "code_bleu": [
            {
                "codebleu": 0.17487380534505212
            },
            {
                "ngram_match_score": 1.596204323483482e-06
            },
            {
                "weighted_ngram_match_score": 0.000811892595847359
            },
            {
                "syntax_match_score": 0.4444444444444444
            },
            {
                "dataflow_match_score": 0.2542372881355932
            }
        ],
        "cosine_similarity": 0.9549072980880737,
        "meteor": 0.010107816711590297
    },
    "mistralai/Mixtral-8x7B-v0.1": {
        "code_bleu": [
            {
                "codebleu": 0.06420373102793875
            },
            {
                "ngram_match_score": 0.0016942747446650152
            },
            {
                "weighted_ngram_match_score": 0.0017192888228722938
            },
            {
                "syntax_match_score": 0.08673469387755102
            },
            {
                "dataflow_match_score": 0.16666666666666666
            }
        ],
        "cosine_similarity": 0.9260190725326538,
        "meteor": 0.014981273408239702
    },
    "google/flan-t5-xxl": {
        "code_bleu": [
            {
                "codebleu": 0.1573533925319574
            },
            {
                "ngram_match_score": 0.00258860630438429
            },
            {
                "weighted_ngram_match_score": 0.002636652135133681
            },
            {
                "syntax_match_score": 0.26704545454545453
            },
            {
                "dataflow_match_score": 0.35714285714285715
            }
        ],
        "cosine_similarity": 0.9880281686782837,
        "meteor": 0.07593014426727411
    },
    "cardiffnlp/twitter-xlm-roberta-base-sentiment": {
        "code_bleu": [
            {
                "codebleu": 0.07941065884391074
            },
            {
                "ngram_match_score": 0.0011244823299320682
            },
            {
                "weighted_ngram_match_score": 0.0019510256183226519
            },
            {
                "syntax_match_score": 0.15568862275449102
            },
            {
                "dataflow_match_score": 0.1588785046728972
            }
        ],
        "cosine_similarity": 0.9851593375205994,
        "meteor": 0.04169339320076972
    },
    "microsoft/Phi-3-medium-128k-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.07257233459119235
            },
            {
                "ngram_match_score": 0.002044764987220838
            },
            {
                "weighted_ngram_match_score": 0.0018809370139121707
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.03636363636363636
            }
        ],
        "cosine_similarity": 0.9687622785568237,
        "meteor": 0.024429967426710098
    },
    "BAAI/bge-reranker-base": {
        "code_bleu": [
            {
                "codebleu": 0.337743028652896
            },
            {
                "ngram_match_score": 2.234468607750901e-05
            },
            {
                "weighted_ngram_match_score": 0.0006312985879269248
            },
            {
                "syntax_match_score": 0.3503184713375796
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9769479036331177,
        "meteor": 0.016536964980544747
    },
    "meta-llama/Meta-Llama-3-70B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.22695326972187466
            },
            {
                "ngram_match_score": 0.008618214628141519
            },
            {
                "weighted_ngram_match_score": 0.02814392937863503
            },
            {
                "syntax_match_score": 0.42424242424242425
            },
            {
                "dataflow_match_score": 0.44680851063829785
            }
        ],
        "cosine_similarity": 0.9520250558853149,
        "meteor": 0.1459996793330127
    },
    "yiyanghkust/finbert-tone": {
        "code_bleu": [
            {
                "codebleu": 0.22372491121725935
            },
            {
                "ngram_match_score": 0.0022441065945409903
            },
            {
                "weighted_ngram_match_score": 0.004022665770656179
            },
            {
                "syntax_match_score": 0.40476190476190477
            },
            {
                "dataflow_match_score": 0.4838709677419355
            }
        ],
        "cosine_similarity": 0.9661694765090942,
        "meteor": 0.03736920777279522
    },
    "mistralai/Mistral-7B-Instruct-v0.1": {
        "code_bleu": [
            {
                "codebleu": 0.17760030628925122
            },
            {
                "ngram_match_score": 0.0090358788224262
            },
            {
                "weighted_ngram_match_score": 0.010943890395881369
            },
            {
                "syntax_match_score": 0.3793103448275862
            },
            {
                "dataflow_match_score": 0.3111111111111111
            }
        ],
        "cosine_similarity": 0.9755600690841675,
        "meteor": 0.08687837028160575
    },
    "briaai/RMBG-1.4": {
        "code_bleu": [
            {
                "codebleu": 0.161165344703085
            },
            {
                "ngram_match_score": 0.023599333453388313
            },
            {
                "weighted_ngram_match_score": 0.02725066299625687
            },
            {
                "syntax_match_score": 0.3926940639269406
            },
            {
                "dataflow_match_score": 0.2011173184357542
            }
        ],
        "cosine_similarity": 0.9734842777252197,
        "meteor": 0.11585885046385898
    },
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0": {
        "code_bleu": [
            {
                "codebleu": 0.19093205676500105
            },
            {
                "ngram_match_score": 0.0018133550594169583
            },
            {
                "weighted_ngram_match_score": 0.0019467699272220007
            },
            {
                "syntax_match_score": 0.3508771929824561
            },
            {
                "dataflow_match_score": 0.4090909090909091
            }
        ],
        "cosine_similarity": 0.9629048109054565,
        "meteor": 0.02
    },
    "diffusers/stable-diffusion-xl-1.0-inpainting-0.1": {
        "code_bleu": [
            {
                "codebleu": 0.23016393062351653
            },
            {
                "ngram_match_score": 0.03841244477576216
            },
            {
                "weighted_ngram_match_score": 0.07799405355979362
            },
            {
                "syntax_match_score": 0.29577464788732394
            },
            {
                "dataflow_match_score": 0.5084745762711864
            }
        ],
        "cosine_similarity": 0.9800050854682922,
        "meteor": 0.13210039630118892
    },
    "unsloth/llama-3-8b-Instruct-bnb-4bit": {
        "code_bleu": [
            {
                "codebleu": 0.254231486382567
            },
            {
                "ngram_match_score": 0.002701950025131376
            },
            {
                "weighted_ngram_match_score": 0.005452065680575268
            },
            {
                "syntax_match_score": 0.39473684210526316
            },
            {
                "dataflow_match_score": 0.6140350877192983
            }
        ],
        "cosine_similarity": 0.9763221144676208,
        "meteor": 0.08571428571428572
    },
    "cross-encoder/ms-marco-TinyBERT-L-2-v2": {
        "code_bleu": [
            {
                "codebleu": 0.20643625046049818
            },
            {
                "ngram_match_score": 0.005134078651265367
            },
            {
                "weighted_ngram_match_score": 0.01079851337831768
            },
            {
                "syntax_match_score": 0.2987012987012987
            },
            {
                "dataflow_match_score": 0.5111111111111111
            }
        ],
        "cosine_similarity": 0.9408208131790161,
        "meteor": 0.09216589861751152
    },
    "intfloat/e5-base-v2": {
        "code_bleu": [
            {
                "codebleu": 0.033974066889895196
            },
            {
                "ngram_match_score": 1.1735640948552568e-05
            },
            {
                "weighted_ngram_match_score": 0.0007590838899583952
            },
            {
                "syntax_match_score": 0.11290322580645161
            },
            {
                "dataflow_match_score": 0.022222222222222223
            }
        ],
        "cosine_similarity": 0.9748364090919495,
        "meteor": 0.008041170794467674
    },
    "facebook/detr-resnet-50": {
        "code_bleu": [
            {
                "codebleu": 0.11293269516677598
            },
            {
                "ngram_match_score": 0.006448605224171729
            },
            {
                "weighted_ngram_match_score": 0.00577123593199267
            },
            {
                "syntax_match_score": 0.22522522522522523
            },
            {
                "dataflow_match_score": 0.21428571428571427
            }
        ],
        "cosine_similarity": 0.9730030298233032,
        "meteor": 0.04798464491362763
    },
    "mistralai/Mistral-7B-Instruct-v0.3": {
        "code_bleu": [
            {
                "codebleu": 0.09127649489572288
            },
            {
                "ngram_match_score": 0.0023020149658362376
            },
            {
                "weighted_ngram_match_score": 0.0025707721815904326
            },
            {
                "syntax_match_score": 0.26479750778816197
            },
            {
                "dataflow_match_score": 0.0954356846473029
            }
        ],
        "cosine_similarity": 0.9649347066879272,
        "meteor": 0.06842456608811749
    },
    "neuralmind/bert-base-portuguese-cased": {
        "code_bleu": [
            {
                "codebleu": 0.17674685721753444
            },
            {
                "ngram_match_score": 0.0013411024028485842
            },
            {
                "weighted_ngram_match_score": 0.0016949586861342216
            },
            {
                "syntax_match_score": 0.2571428571428571
            },
            {
                "dataflow_match_score": 0.44680851063829785
            }
        ],
        "cosine_similarity": 0.964958667755127,
        "meteor": 0.0450837269214255
    },
    "stabilityai/stable-diffusion-2-1": {
        "code_bleu": [
            {
                "codebleu": 0.15668851424543018
            },
            {
                "ngram_match_score": 0.003940399074299291
            },
            {
                "weighted_ngram_match_score": 0.009910432100969785
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.6129032258064516
            }
        ],
        "cosine_similarity": 0.9508762359619141,
        "meteor": 0.03913894324853229
    },
    "cross-encoder/ms-marco-MiniLM-L-12-v2": {
        "code_bleu": [
            {
                "codebleu": 0.25871207342314023
            },
            {
                "ngram_match_score": 0.0028620753592041048
            },
            {
                "weighted_ngram_match_score": 0.0034147897619283387
            },
            {
                "syntax_match_score": 0.42857142857142855
            },
            {
                "dataflow_match_score": 0.6
            }
        ],
        "cosine_similarity": 0.964737057685852,
        "meteor": 0.04842615012106538
    },
    "google/flan-t5-base": {
        "code_bleu": [
            {
                "codebleu": 0.13446636785616511
            },
            {
                "ngram_match_score": 0.005371285396203062
            },
            {
                "weighted_ngram_match_score": 0.006790722824994303
            },
            {
                "syntax_match_score": 0.3352272727272727
            },
            {
                "dataflow_match_score": 0.19047619047619047
            }
        ],
        "cosine_similarity": 0.967448890209198,
        "meteor": 0.06442376521116679
    },
    "microsoft/phi-2": {
        "code_bleu": [
            {
                "codebleu": 0.1615305910257466
            },
            {
                "ngram_match_score": 0.007247767614443553
            },
            {
                "weighted_ngram_match_score": 0.008105365719312039
            },
            {
                "syntax_match_score": 0.3076923076923077
            },
            {
                "dataflow_match_score": 0.3230769230769231
            }
        ],
        "cosine_similarity": 0.9856393337249756,
        "meteor": 0.06666666666666667
    },
    "EleutherAI/gpt-j-6b": {
        "code_bleu": [
            {
                "codebleu": 0.2494712770993729
            },
            {
                "ngram_match_score": 0.003592221924269515
            },
            {
                "weighted_ngram_match_score": 0.04135171000263379
            },
            {
                "syntax_match_score": 0.35294117647058826
            },
            {
                "dataflow_match_score": 0.6
            }
        ],
        "cosine_similarity": 0.928851306438446,
        "meteor": 0.08888888888888889
    },
    "google/vit-base-patch16-384": {
        "code_bleu": [
            {
                "codebleu": 0.19330817691936908
            },
            {
                "ngram_match_score": 0.004786100293485711
            },
            {
                "weighted_ngram_match_score": 0.010870849808233019
            },
            {
                "syntax_match_score": 0.25757575757575757
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.941828727722168,
        "meteor": 0.08928571428571429
    },
    "thenlper/gte-large": {
        "code_bleu": [
            {
                "codebleu": 0.143401561133079
            },
            {
                "ngram_match_score": 0.005158590621182685
            },
            {
                "weighted_ngram_match_score": 0.005267533722729872
            },
            {
                "syntax_match_score": 0.19083969465648856
            },
            {
                "dataflow_match_score": 0.3723404255319149
            }
        ],
        "cosine_similarity": 0.9797288179397583,
        "meteor": 0.06751054852320676
    },
    "vennify/t5-base-grammar-correction": {
        "code_bleu": [
            {
                "codebleu": 0.34581431525218415
            },
            {
                "ngram_match_score": 0.057910519767160516
            },
            {
                "weighted_ngram_match_score": 0.2276023803393205
            },
            {
                "syntax_match_score": 0.5714285714285714
            },
            {
                "dataflow_match_score": 0.5263157894736842
            }
        ],
        "cosine_similarity": 0.9452042579650879,
        "meteor": 0.2783578759482374
    },
    "jhgan/ko-sroberta-multitask": {
        "code_bleu": [
            {
                "codebleu": 0.18432351995793006
            },
            {
                "ngram_match_score": 0.00481655162729119
            },
            {
                "weighted_ngram_match_score": 0.00498758856660204
            },
            {
                "syntax_match_score": 0.2767857142857143
            },
            {
                "dataflow_match_score": 0.4507042253521127
            }
        ],
        "cosine_similarity": 0.959092378616333,
        "meteor": 0.055910543130990406
    },
    "gpt2-large": {
        "code_bleu": [
            {
                "codebleu": 0.11246200607902734
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.02127659574468085
            },
            {
                "dataflow_match_score": 0.42857142857142855
            }
        ],
        "cosine_similarity": 0.926167368888855,
        "meteor": 0.0
    },
    "deepset/bert-large-uncased-whole-word-masking-squad2": {
        "code_bleu": [
            {
                "codebleu": 0.17358390566643342
            },
            {
                "ngram_match_score": 0.0033837223311965637
            },
            {
                "weighted_ngram_match_score": 0.005251801029671117
            },
            {
                "syntax_match_score": 0.2894736842105263
            },
            {
                "dataflow_match_score": 0.39622641509433965
            }
        ],
        "cosine_similarity": 0.9423591494560242,
        "meteor": 0.07105459985041138
    },
    "sentence-transformers/distiluse-base-multilingual-cased-v1": {
        "code_bleu": [
            {
                "codebleu": 0.13825757575757575
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.13636363636363635
            },
            {
                "dataflow_match_score": 0.4166666666666667
            }
        ],
        "cosine_similarity": 0.8944416046142578,
        "meteor": 0.0
    },
    "ByteDance/SDXL-Lightning": {
        "code_bleu": [
            {
                "codebleu": 0.31799468161846056
            },
            {
                "ngram_match_score": 0.10497329049331666
            },
            {
                "weighted_ngram_match_score": 0.19733135870877935
            },
            {
                "syntax_match_score": 0.6769911504424779
            },
            {
                "dataflow_match_score": 0.2926829268292683
            }
        ],
        "cosine_similarity": 0.9904760122299194,
        "meteor": 0.24916851441241686
    },
    "sentence-transformers/msmarco-distilbert-base-v4": {
        "code_bleu": [
            {
                "codebleu": 0.19030188177884888
            },
            {
                "ngram_match_score": 0.004433380274939642
            },
            {
                "weighted_ngram_match_score": 0.006127148910849317
            },
            {
                "syntax_match_score": 0.33035714285714285
            },
            {
                "dataflow_match_score": 0.42028985507246375
            }
        ],
        "cosine_similarity": 0.9735069274902344,
        "meteor": 0.1252898366759753
    },
    "bigscience/bloomz-560m": {
        "code_bleu": [
            {
                "codebleu": 0.08099674361144948
            },
            {
                "ngram_match_score": 0.01061289778651537
            },
            {
                "weighted_ngram_match_score": 0.015182183979200925
            },
            {
                "syntax_match_score": 0.09448818897637795
            },
            {
                "dataflow_match_score": 0.2037037037037037
            }
        ],
        "cosine_similarity": 0.9727209806442261,
        "meteor": 0.0348605577689243
    },
    "EleutherAI/gpt-neo-125m": {
        "code_bleu": [
            {
                "codebleu": 0.13093018166591364
            },
            {
                "ngram_match_score": 0.002265553897317694
            },
            {
                "weighted_ngram_match_score": 0.004971656282820334
            },
            {
                "syntax_match_score": 0.23076923076923078
            },
            {
                "dataflow_match_score": 0.2857142857142857
            }
        ],
        "cosine_similarity": 0.9343899488449097,
        "meteor": 0.01295336787564767
    },
    "google/flan-t5-small": {
        "code_bleu": [
            {
                "codebleu": 0.0689484126984127
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.125
            },
            {
                "dataflow_match_score": 0.15079365079365079
            }
        ],
        "cosine_similarity": 0.982149600982666,
        "meteor": 0.0
    },
    "gpt2-medium": {
        "code_bleu": [
            {
                "codebleu": 0.17703901876146405
            },
            {
                "ngram_match_score": 0.001028310871555318
            },
            {
                "weighted_ngram_match_score": 0.002568493657583528
            },
            {
                "syntax_match_score": 0.3617021276595745
            },
            {
                "dataflow_match_score": 0.34285714285714286
            }
        ],
        "cosine_similarity": 0.9777084589004517,
        "meteor": 0.03994673768308921
    },
    "google/gemma-2b": {
        "code_bleu": [
            {
                "codebleu": 0.11079121920434196
            },
            {
                "ngram_match_score": 0.002891843111159009
            },
            {
                "weighted_ngram_match_score": 0.003263272089117312
            },
            {
                "syntax_match_score": 0.31135531135531136
            },
            {
                "dataflow_match_score": 0.1256544502617801
            }
        ],
        "cosine_similarity": 0.9650318622589111,
        "meteor": 0.03054989816700611
    },
    "snunlp/KR-SBERT-V40K-klueNLI-augSTS": {
        "code_bleu": [
            {
                "codebleu": 0.24336863807228026
            },
            {
                "ngram_match_score": 0.03585768979383552
            },
            {
                "weighted_ngram_match_score": 0.04087773206050299
            },
            {
                "syntax_match_score": 0.375
            },
            {
                "dataflow_match_score": 0.5217391304347826
            }
        ],
        "cosine_similarity": 0.9821703433990479,
        "meteor": 0.0846774193548387
    },
    "Salesforce/instructblip-vicuna-7b": {
        "code_bleu": [
            {
                "codebleu": 0.17285538483801743
            },
            {
                "ngram_match_score": 0.002117072937374405
            },
            {
                "weighted_ngram_match_score": 0.005093940098905907
            },
            {
                "syntax_match_score": 0.2631578947368421
            },
            {
                "dataflow_match_score": 0.42105263157894735
            }
        ],
        "cosine_similarity": 0.9905663728713989,
        "meteor": 0.08713136729222519
    },
    "facebook/roberta-hate-speech-dynabench-r4-target": {
        "code_bleu": [
            {
                "codebleu": 0.2519223837231248
            },
            {
                "ngram_match_score": 0.0014758140748286537
            },
            {
                "weighted_ngram_match_score": 0.006213720817670483
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.8981660604476929,
        "meteor": 0.027397260273972608
    },
    "stabilityai/stable-diffusion-2": {
        "code_bleu": [
            {
                "codebleu": 0.262014885599003
            },
            {
                "ngram_match_score": 0.0017404443880078115
            },
            {
                "weighted_ngram_match_score": 0.005808970476121208
            },
            {
                "syntax_match_score": 0.3953488372093023
            },
            {
                "dataflow_match_score": 0.6451612903225806
            }
        ],
        "cosine_similarity": 0.9531701803207397,
        "meteor": 0.0375234521575985
    },
    "google/gemma-7b-it": {
        "code_bleu": [
            {
                "codebleu": 0.1254674570917068
            },
            {
                "ngram_match_score": 0.0014833200312088107
            },
            {
                "weighted_ngram_match_score": 0.0030774770843679067
            },
            {
                "syntax_match_score": 0.35924932975871315
            },
            {
                "dataflow_match_score": 0.13805970149253732
            }
        ],
        "cosine_similarity": 0.9718268513679504,
        "meteor": 0.03906451242178183
    },
    "philschmid/bart-large-cnn-samsum": {
        "code_bleu": [
            {
                "codebleu": 0.27951467677196845
            },
            {
                "ngram_match_score": 0.008340053190442972
            },
            {
                "weighted_ngram_match_score": 0.021483359779783703
            },
            {
                "syntax_match_score": 0.5882352941176471
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9441051483154297,
        "meteor": 0.06480074299223236
    },
    "google/flan-t5-xl": {
        "code_bleu": [
            {
                "codebleu": 0.14716713637301046
            },
            {
                "ngram_match_score": 0.00213476463420218
            },
            {
                "weighted_ngram_match_score": 0.002028008852067664
            },
            {
                "syntax_match_score": 0.26704545454545453
            },
            {
                "dataflow_match_score": 0.31746031746031744
            }
        ],
        "cosine_similarity": 0.9608235359191895,
        "meteor": 0.025716385011021307
    },
    "sentence-transformers/paraphrase-MiniLM-L3-v2": {
        "code_bleu": [
            {
                "codebleu": 0.011569799275445347
            },
            {
                "ngram_match_score": 0.0008913821233303135
            },
            {
                "weighted_ngram_match_score": 0.0019095541088858586
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.043478260869565216
            }
        ],
        "cosine_similarity": 0.9491876363754272,
        "meteor": 0.003695491500369549
    },
    "latent-consistency/lcm-lora-sdv1-5": {
        "code_bleu": [
            {
                "codebleu": 0.12043886723389116
            },
            {
                "ngram_match_score": 0.0006968902291029347
            },
            {
                "weighted_ngram_match_score": 0.0012173302804858624
            },
            {
                "syntax_match_score": 0.2996941896024465
            },
            {
                "dataflow_match_score": 0.1801470588235294
            }
        ],
        "cosine_similarity": 0.9643221497535706,
        "meteor": 0.04435346298191743
    },
    "sentence-transformers/paraphrase-mpnet-base-v2": {
        "code_bleu": [
            {
                "codebleu": 0.17591283555791482
            },
            {
                "ngram_match_score": 0.004229263288638682
            },
            {
                "weighted_ngram_match_score": 0.005710898818796884
            },
            {
                "syntax_match_score": 0.25892857142857145
            },
            {
                "dataflow_match_score": 0.43478260869565216
            }
        ],
        "cosine_similarity": 0.9729751348495483,
        "meteor": 0.12091637901736174
    },
    "mrm8488/t5-base-finetuned-question-generation-ap": {
        "code_bleu": [
            {
                "codebleu": 0.18795228720353557
            },
            {
                "ngram_match_score": 0.0036256899560289796
            },
            {
                "weighted_ngram_match_score": 0.005117203079993138
            },
            {
                "syntax_match_score": 0.22033898305084745
            },
            {
                "dataflow_match_score": 0.5227272727272727
            }
        ],
        "cosine_similarity": 0.9629031419754028,
        "meteor": 0.06828528072837632
    },
    "sentence-transformers/multi-qa-distilbert-cos-v1": {
        "code_bleu": [
            {
                "codebleu": 0.09889952657032115
            },
            {
                "ngram_match_score": 0.0006613997633077974
            },
            {
                "weighted_ngram_match_score": 0.0017010951213145625
            },
            {
                "syntax_match_score": 0.22330097087378642
            },
            {
                "dataflow_match_score": 0.16993464052287582
            }
        ],
        "cosine_similarity": 0.9860588312149048,
        "meteor": 0.0517104216388226
    },
    "llava-hf/llava-1.5-7b-hf": {
        "code_bleu": [
            {
                "codebleu": 0.19132830374420898
            },
            {
                "ngram_match_score": 0.0018664638962887805
            },
            {
                "weighted_ngram_match_score": 0.0020181796519757444
            },
            {
                "syntax_match_score": 0.3314285714285714
            },
            {
                "dataflow_match_score": 0.43
            }
        ],
        "cosine_similarity": 0.988440752029419,
        "meteor": 0.04174120453190221
    },
    "HuggingFaceH4/zephyr-7b-beta": {
        "code_bleu": [
            {
                "codebleu": 0.2694650772857098
            },
            {
                "ngram_match_score": 0.08434434781914522
            },
            {
                "weighted_ngram_match_score": 0.09319698205734625
            },
            {
                "syntax_match_score": 0.49122807017543857
            },
            {
                "dataflow_match_score": 0.4090909090909091
            }
        ],
        "cosine_similarity": 0.9394848942756653,
        "meteor": 0.1769381499111229
    },
    "sentence-transformers/multi-qa-mpnet-base-cos-v1": {
        "code_bleu": [
            {
                "codebleu": 0.09175176347852702
            },
            {
                "ngram_match_score": 0.002641505006667471
            },
            {
                "weighted_ngram_match_score": 0.0069824662245292855
            },
            {
                "syntax_match_score": 0.21359223300970873
            },
            {
                "dataflow_match_score": 0.1437908496732026
            }
        ],
        "cosine_similarity": 0.9646008014678955,
        "meteor": 0.038244963056993125
    },
    "cointegrated/rubert-tiny2": {
        "code_bleu": [
            {
                "codebleu": 0.2124041397241998
            },
            {
                "ngram_match_score": 0.008368567120004066
            },
            {
                "weighted_ngram_match_score": 0.012909733721356806
            },
            {
                "syntax_match_score": 0.46601941747572817
            },
            {
                "dataflow_match_score": 0.36231884057971014
            }
        ],
        "cosine_similarity": 0.9728484749794006,
        "meteor": 0.1740752565564424
    },
    "stabilityai/stable-diffusion-2-base": {
        "code_bleu": [
            {
                "codebleu": 0.17017917674160288
            },
            {
                "ngram_match_score": 0.002423966433280439
            },
            {
                "weighted_ngram_match_score": 0.01370940719979775
            },
            {
                "syntax_match_score": 0.13333333333333333
            },
            {
                "dataflow_match_score": 0.53125
            }
        ],
        "cosine_similarity": 0.9508922696113586,
        "meteor": 0.050761421319796954
    },
    "Salesforce/blip2-opt-2.7b": {
        "code_bleu": [
            {
                "codebleu": 0.21795112182808749
            },
            {
                "ngram_match_score": 0.03631522197439823
            },
            {
                "weighted_ngram_match_score": 0.04836999800984517
            },
            {
                "syntax_match_score": 0.4964788732394366
            },
            {
                "dataflow_match_score": 0.29064039408866993
            }
        ],
        "cosine_similarity": 0.96865314245224,
        "meteor": 0.13078305586266534
    },
    "unitary/toxic-bert": {
        "code_bleu": [
            {
                "codebleu": 0.20923089860212157
            },
            {
                "ngram_match_score": 0.002397139398529023
            },
            {
                "weighted_ngram_match_score": 0.003004715879522541
            },
            {
                "syntax_match_score": 0.375
            },
            {
                "dataflow_match_score": 0.45652173913043476
            }
        ],
        "cosine_similarity": 0.9773114323616028,
        "meteor": 0.05862068965517241
    },
    "gpt2-xl": {
        "code_bleu": [
            {
                "codebleu": 0.1996976610890699
            },
            {
                "ngram_match_score": 0.003050623647421529
            },
            {
                "weighted_ngram_match_score": 0.0032105318490938685
            },
            {
                "syntax_match_score": 0.3211009174311927
            },
            {
                "dataflow_match_score": 0.4714285714285714
            }
        ],
        "cosine_similarity": 0.9724767208099365,
        "meteor": 0.06160164271047228
    },
    "sentence-transformers/LaBSE": {
        "code_bleu": [
            {
                "codebleu": 0.27208607751153857
            },
            {
                "ngram_match_score": 0.001869854090646045
            },
            {
                "weighted_ngram_match_score": 0.010716880197932516
            },
            {
                "syntax_match_score": 0.4090909090909091
            },
            {
                "dataflow_match_score": 0.6666666666666666
            }
        ],
        "cosine_similarity": 0.9041364192962646,
        "meteor": 0.059880239520958084
    },
    "apple/mobilevit-small": {
        "code_bleu": [
            {
                "codebleu": 0.20509222748997635
            },
            {
                "ngram_match_score": 0.001991875128164886
            },
            {
                "weighted_ngram_match_score": 0.004524220978926712
            },
            {
                "syntax_match_score": 0.24242424242424243
            },
            {
                "dataflow_match_score": 0.5714285714285714
            }
        ],
        "cosine_similarity": 0.9483434557914734,
        "meteor": 0.026785714285714284
    },
    "facebook/convnextv2-tiny-1k-224": {
        "code_bleu": [
            {
                "codebleu": 0.2304059532793032
            },
            {
                "ngram_match_score": 0.03316398369863373
            },
            {
                "weighted_ngram_match_score": 0.10030402655689381
            },
            {
                "syntax_match_score": 0.22058823529411764
            },
            {
                "dataflow_match_score": 0.5675675675675675
            }
        ],
        "cosine_similarity": 0.9593735933303833,
        "meteor": 0.17077798861480076
    },
    "xlnet-base-cased": {
        "code_bleu": [
            {
                "codebleu": 0.30790379563747655
            },
            {
                "ngram_match_score": 0.002594445031287883
            },
            {
                "weighted_ngram_match_score": 0.010270737518618322
            },
            {
                "syntax_match_score": 0.21875
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9150853157043457,
        "meteor": 0.07598784194528875
    },
    "hkunlp/instructor-large": {
        "code_bleu": [
            {
                "codebleu": 0.17409891579988995
            },
            {
                "ngram_match_score": 0.0006326498055480029
            },
            {
                "weighted_ngram_match_score": 0.0021157237202320666
            },
            {
                "syntax_match_score": 0.3973509933774834
            },
            {
                "dataflow_match_score": 0.2962962962962963
            }
        ],
        "cosine_similarity": 0.9737410545349121,
        "meteor": 0.042432814710042434
    },
    "Qwen/Qwen2-7B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.10713135979462482
            },
            {
                "ngram_match_score": 0.0025866737458825335
            },
            {
                "weighted_ngram_match_score": 0.004515227730009299
            },
            {
                "syntax_match_score": 0.20930232558139536
            },
            {
                "dataflow_match_score": 0.21212121212121213
            }
        ],
        "cosine_similarity": 0.96932053565979,
        "meteor": 0.08849557522123894
    },
    "distilbert-base-cased-distilled-squad": {
        "code_bleu": [
            {
                "codebleu": 0.09891251028417034
            },
            {
                "ngram_match_score": 0.026468799452195324
            },
            {
                "weighted_ngram_match_score": 0.03573240823565255
            },
            {
                "syntax_match_score": 0.0811965811965812
            },
            {
                "dataflow_match_score": 0.25225225225225223
            }
        ],
        "cosine_similarity": 0.9833640456199646,
        "meteor": 0.072736827668055
    },
    "distilbert/distilbert-base-cased-distilled-squad": {
        "code_bleu": [
            {
                "codebleu": 0.07237924298068138
            },
            {
                "ngram_match_score": 0.015190765142211718
            },
            {
                "weighted_ngram_match_score": 0.0210344534887605
            },
            {
                "syntax_match_score": 0.0641025641025641
            },
            {
                "dataflow_match_score": 0.1891891891891892
            }
        ],
        "cosine_similarity": 0.9783759713172913,
        "meteor": 0.03223207091055601
    },
    "segmind/SSD-1B": {
        "code_bleu": [
            {
                "codebleu": 0.17307692307692307
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.6923076923076923
            }
        ],
        "cosine_similarity": 0.9336274862289429,
        "meteor": 0.0
    },
    "google/gemma-7b": {
        "code_bleu": [
            {
                "codebleu": 0.1072365878351978
            },
            {
                "ngram_match_score": 0.0016277562547423181
            },
            {
                "weighted_ngram_match_score": 0.0018156512584977154
            },
            {
                "syntax_match_score": 0.2893772893772894
            },
            {
                "dataflow_match_score": 0.13612565445026178
            }
        ],
        "cosine_similarity": 0.934311032295227,
        "meteor": 0.0584944048830112
    },
    "LanguageBind/Video-LLaVA-7B": {
        "code_bleu": [
            {
                "codebleu": 0.04237090894737453
            },
            {
                "ngram_match_score": 9.480608844509231e-05
            },
            {
                "weighted_ngram_match_score": 0.0011898636538265471
            },
            {
                "syntax_match_score": 0.12114014251781473
            },
            {
                "dataflow_match_score": 0.047058823529411764
            }
        ],
        "cosine_similarity": 0.9848283529281616,
        "meteor": 0.018223989396951624
    },
    "stabilityai/stable-diffusion-3-medium-diffusers": {
        "code_bleu": [
            {
                "codebleu": 0.19711198633054258
            },
            {
                "ngram_match_score": 0.004011085923355499
            },
            {
                "weighted_ngram_match_score": 0.014436859398814792
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.52
            }
        ],
        "cosine_similarity": 0.9447305798530579,
        "meteor": 0.06527415143603134
    },
    "shibing624/text2vec-base-chinese": {
        "code_bleu": [
            {
                "codebleu": 0.17587130383965097
            },
            {
                "ngram_match_score": 0.00732705457947659
            },
            {
                "weighted_ngram_match_score": 0.008587539310200691
            },
            {
                "syntax_match_score": 0.43333333333333335
            },
            {
                "dataflow_match_score": 0.2542372881355932
            }
        ],
        "cosine_similarity": 0.9796713590621948,
        "meteor": 0.08026929052304504
    },
    "sentence-transformers/stsb-xlm-r-multilingual": {
        "code_bleu": [
            {
                "codebleu": 0.28164338902263597
            },
            {
                "ngram_match_score": 0.15071475934495082
            },
            {
                "weighted_ngram_match_score": 0.14938364146608982
            },
            {
                "syntax_match_score": 0.3482142857142857
            },
            {
                "dataflow_match_score": 0.4782608695652174
            }
        ],
        "cosine_similarity": 0.9778435230255127,
        "meteor": 0.19976265822784808
    },
    "Lykon/dreamshaper-8": {
        "code_bleu": [
            {
                "codebleu": 0.08974358974358974
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.358974358974359
            }
        ],
        "cosine_similarity": 0.9676674008369446,
        "meteor": 0.0
    },
    "sentence-transformers/nli-mpnet-base-v2": {
        "code_bleu": [
            {
                "codebleu": 0.1811600655834224
            },
            {
                "ngram_match_score": 0.005219347016908046
            },
            {
                "weighted_ngram_match_score": 0.005523399788831213
            },
            {
                "syntax_match_score": 0.36607142857142855
            },
            {
                "dataflow_match_score": 0.34782608695652173
            }
        ],
        "cosine_similarity": 0.9822846055030823,
        "meteor": 0.0822239624119029
    },
    "facebook/opt-350m": {
        "code_bleu": [
            {
                "codebleu": 0.14775084519827875
            },
            {
                "ngram_match_score": 0.000527743417053464
            },
            {
                "weighted_ngram_match_score": 0.0007957256762821587
            },
            {
                "syntax_match_score": 0.2980132450331126
            },
            {
                "dataflow_match_score": 0.2916666666666667
            }
        ],
        "cosine_similarity": 0.948339581489563,
        "meteor": 0.007399186089530153
    },
    "mistralai/Mixtral-8x7B-Instruct-v0.1": {
        "code_bleu": [
            {
                "codebleu": 0.15222496305055094
            },
            {
                "ngram_match_score": 0.0018367096302036924
            },
            {
                "weighted_ngram_match_score": 0.005602343607664994
            },
            {
                "syntax_match_score": 0.4665127020785219
            },
            {
                "dataflow_match_score": 0.13494809688581316
            }
        ],
        "cosine_similarity": 0.9733102321624756,
        "meteor": 0.04929634415558718
    },
    "siebert/sentiment-roberta-large-english": {
        "code_bleu": [
            {
                "codebleu": 0.24246422581669264
            },
            {
                "ngram_match_score": 0.00205598610184581
            },
            {
                "weighted_ngram_match_score": 0.01822108523215156
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.7142857142857143
            }
        ],
        "cosine_similarity": 0.9167758822441101,
        "meteor": 0.028089887640449444
    },
    "hustvl/yolos-tiny": {
        "code_bleu": [
            {
                "codebleu": 0.3007158108801497
            },
            {
                "ngram_match_score": 0.10283381029212163
            },
            {
                "weighted_ngram_match_score": 0.143539757712253
            },
            {
                "syntax_match_score": 0.5398230088495575
            },
            {
                "dataflow_match_score": 0.4166666666666667
            }
        ],
        "cosine_similarity": 0.9819168448448181,
        "meteor": 0.24378212187958886
    },
    "microsoft/Phi-3-vision-128k-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.32651730784819066
            },
            {
                "ngram_match_score": 0.0017109308627166837
            },
            {
                "weighted_ngram_match_score": 0.002032719134697203
            },
            {
                "syntax_match_score": 0.3023255813953488
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9718166589736938,
        "meteor": 0.04550321199143468
    },
    "google/gemma-2b-it": {
        "code_bleu": [
            {
                "codebleu": 0.09577636946901572
            },
            {
                "ngram_match_score": 0.0010222903008132672
            },
            {
                "weighted_ngram_match_score": 0.0025920040239450033
            },
            {
                "syntax_match_score": 0.22629969418960244
            },
            {
                "dataflow_match_score": 0.15319148936170213
            }
        ],
        "cosine_similarity": 0.9575753211975098,
        "meteor": 0.028351559335763464
    },
    "microsoft/Phi-3-medium-4k-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.08306277056277056
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.05952380952380952
            },
            {
                "dataflow_match_score": 0.2727272727272727
            }
        ],
        "cosine_similarity": 0.8884453773498535,
        "meteor": 0.0
    },
    "openai-gpt": {
        "code_bleu": [
            {
                "codebleu": 0.3281303416736977
            },
            {
                "ngram_match_score": 0.0004245737757188827
            },
            {
                "weighted_ngram_match_score": 0.0012159639035278019
            },
            {
                "syntax_match_score": 0.31088082901554404
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9442895650863647,
        "meteor": 0.023980815347721823
    },
    "microsoft/Phi-3-small-8k-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.23731921571471284
            },
            {
                "ngram_match_score": 0.009810090592725999
            },
            {
                "weighted_ngram_match_score": 0.011774464573817714
            },
            {
                "syntax_match_score": 0.42
            },
            {
                "dataflow_match_score": 0.5076923076923077
            }
        ],
        "cosine_similarity": 0.9783137440681458,
        "meteor": 0.07378777231201686
    },
    "latent-consistency/lcm-lora-sdxl": {
        "code_bleu": [
            {
                "codebleu": 0.11720597464831033
            },
            {
                "ngram_match_score": 0.000296097876711694
            },
            {
                "weighted_ngram_match_score": 0.0009314903529918867
            },
            {
                "syntax_match_score": 0.32842105263157895
            },
            {
                "dataflow_match_score": 0.13917525773195877
            }
        ],
        "cosine_similarity": 0.968365490436554,
        "meteor": 0.022641509433962263
    },
    "cross-encoder/stsb-distilroberta-base": {
        "code_bleu": [
            {
                "codebleu": 0.3632253385223815
            },
            {
                "ngram_match_score": 0.028320166332386716
            },
            {
                "weighted_ngram_match_score": 0.17782794100389226
            },
            {
                "syntax_match_score": 0.42857142857142855
            },
            {
                "dataflow_match_score": 0.8181818181818182
            }
        ],
        "cosine_similarity": 0.9129566550254822,
        "meteor": 0.12448132780082988
    },
    "sentence-transformers/paraphrase-albert-small-v2": {
        "code_bleu": [
            {
                "codebleu": 0.11089801474717637
            },
            {
                "ngram_match_score": 0.001726268646845543
            },
            {
                "weighted_ngram_match_score": 0.0024247965530400147
            },
            {
                "syntax_match_score": 0.17857142857142858
            },
            {
                "dataflow_match_score": 0.2608695652173913
            }
        ],
        "cosine_similarity": 0.966667890548706,
        "meteor": 0.03709810387469085
    },
    "meta-llama/Meta-Llama-3-70B": {
        "code_bleu": [
            {
                "codebleu": 0.3493819272017126
            },
            {
                "ngram_match_score": 0.0018213509029180624
            },
            {
                "weighted_ngram_match_score": 0.008609583710384035
            },
            {
                "syntax_match_score": 0.3870967741935484
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9262290000915527,
        "meteor": 0.05361930294906166
    },
    "databricks/dbrx-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.12801630168378259
            },
            {
                "ngram_match_score": 0.0031318351219776033
            },
            {
                "weighted_ngram_match_score": 0.006344276273524365
            },
            {
                "syntax_match_score": 0.29850746268656714
            },
            {
                "dataflow_match_score": 0.20408163265306123
            }
        ],
        "cosine_similarity": 0.9632026553153992,
        "meteor": 0.09784735812133072
    },
    "Rostlab/prot/bert": {
        "code_bleu": [
            {
                "codebleu": 0.1480682900295247
            },
            {
                "ngram_match_score": 0.00015979596328154333
            },
            {
                "weighted_ngram_match_score": 0.003798977160562259
            },
            {
                "syntax_match_score": 0.26732673267326734
            },
            {
                "dataflow_match_score": 0.32098765432098764
            }
        ],
        "cosine_similarity": 0.9663999676704407,
        "meteor": 0.020340953118946143
    },
    "microsoft/trocr-base-printed": {
        "code_bleu": [
            {
                "codebleu": 0.1893696693532591
            },
            {
                "ngram_match_score": 0.054788093707598115
            },
            {
                "weighted_ngram_match_score": 0.20131100152175668
            },
            {
                "syntax_match_score": 0.15254237288135594
            },
            {
                "dataflow_match_score": 0.3488372093023256
            }
        ],
        "cosine_similarity": 0.9522771239280701,
        "meteor": 0.15067051302042728
    },
    "Qwen/Qwen2-0.5B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.21225239066584445
            },
            {
                "ngram_match_score": 0.0025694108478283495
            },
            {
                "weighted_ngram_match_score": 0.004650158862765737
            },
            {
                "syntax_match_score": 0.37209302325581395
            },
            {
                "dataflow_match_score": 0.4696969696969697
            }
        ],
        "cosine_similarity": 0.9793306589126587,
        "meteor": 0.10976948408342481
    },
    "google/mobilenet/v1/0.75/192": {
        "code_bleu": [
            {
                "codebleu": 0.4377658493009569
            },
            {
                "ngram_match_score": 0.11384253251957405
            },
            {
                "weighted_ngram_match_score": 0.40778363524702416
            },
            {
                "syntax_match_score": 0.6818181818181818
            },
            {
                "dataflow_match_score": 0.5476190476190477
            }
        ],
        "cosine_similarity": 0.9758700132369995,
        "meteor": 0.5227581404200838
    },
    "microsoft/resnet-18": {
        "code_bleu": [
            {
                "codebleu": 0.18555659321620796
            },
            {
                "ngram_match_score": 0.00412559071039474
            },
            {
                "weighted_ngram_match_score": 0.004700715421037012
            },
            {
                "syntax_match_score": 0.24691358024691357
            },
            {
                "dataflow_match_score": 0.4864864864864865
            }
        ],
        "cosine_similarity": 0.9430862665176392,
        "meteor": 0.06514657980456026
    },
    "sentence-transformers/msmarco-distilbert-base-tas-b": {
        "code_bleu": [
            {
                "codebleu": 0.24963978435098547
            },
            {
                "ngram_match_score": 0.022677472944426638
            },
            {
                "weighted_ngram_match_score": 0.04116573047382802
            },
            {
                "syntax_match_score": 0.5688622754491018
            },
            {
                "dataflow_match_score": 0.36585365853658536
            }
        ],
        "cosine_similarity": 0.9932505488395691,
        "meteor": 0.12490995795817952
    },
    "Qwen/Qwen2-72B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.15597667623625744
            },
            {
                "ngram_match_score": 0.002316422979091014
            },
            {
                "weighted_ngram_match_score": 0.004606490563542675
            },
            {
                "syntax_match_score": 0.313953488372093
            },
            {
                "dataflow_match_score": 0.30303030303030304
            }
        ],
        "cosine_similarity": 0.9564692974090576,
        "meteor": 0.0819672131147541
    },
    "neuralmind/bert-large-portuguese-cased": {
        "code_bleu": [
            {
                "codebleu": 0.3255692969379919
            },
            {
                "ngram_match_score": 0.000981037563638144
            },
            {
                "weighted_ngram_match_score": 0.0012961501883295705
            },
            {
                "syntax_match_score": 0.3
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9497359991073608,
        "meteor": 0.017182130584192438
    },
    "Rostlab/prot/bert/bfd": {
        "code_bleu": [
            {
                "codebleu": 0.3055106164437726
            },
            {
                "ngram_match_score": 0.0001875307674166791
            },
            {
                "weighted_ngram_match_score": 0.0007328227964525562
            },
            {
                "syntax_match_score": 0.22112211221122113
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9728031754493713,
        "meteor": 0.030633735401110473
    },
    "microsoft/swin-tiny-patch4-window7-224": {
        "code_bleu": [
            {
                "codebleu": 0.1202372223269074
            },
            {
                "ngram_match_score": 0.003206590597368341
            },
            {
                "weighted_ngram_match_score": 0.008045329013291514
            },
            {
                "syntax_match_score": 0.30303030303030304
            },
            {
                "dataflow_match_score": 0.16666666666666666
            }
        ],
        "cosine_similarity": 0.9579855799674988,
        "meteor": 0.03289473684210526
    },
    "cointegrated/rubert-tiny": {
        "code_bleu": [
            {
                "codebleu": 0.2149406440636556
            },
            {
                "ngram_match_score": 0.006363342599086976
            },
            {
                "weighted_ngram_match_score": 0.010261978753574666
            },
            {
                "syntax_match_score": 0.3333333333333333
            },
            {
                "dataflow_match_score": 0.5098039215686274
            }
        ],
        "cosine_similarity": 0.9673250913619995,
        "meteor": 0.08836524300441825
    },
    "teknium/OpenHermes-2.5-Mistral-7B": {
        "code_bleu": [
            {
                "codebleu": 0.25
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9215898513793945,
        "meteor": 0.0
    },
    "klue/bert-base": {
        "code_bleu": [
            {
                "codebleu": 0.2480723341798906
            },
            {
                "ngram_match_score": 0.0021470850489829487
            },
            {
                "weighted_ngram_match_score": 0.019554016376461803
            },
            {
                "syntax_match_score": 0.47058823529411764
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9110687971115112,
        "meteor": 0.050251256281407045
    },
    "timm/ViT-SO400M-14-SigLIP-384": {
        "code_bleu": [
            {
                "codebleu": 0.0928814884490712
            },
            {
                "ngram_match_score": 0.003262409385731237
            },
            {
                "weighted_ngram_match_score": 0.005828215511809986
            },
            {
                "syntax_match_score": 0.10227272727272728
            },
            {
                "dataflow_match_score": 0.2601626016260163
            }
        ],
        "cosine_similarity": 0.970674455165863,
        "meteor": 0.05486968449931413
    },
    "microsoft/dit-base-finetuned-rvlcdip": {
        "code_bleu": [
            {
                "codebleu": 0.38291021971351613
            },
            {
                "ngram_match_score": 0.12936019045208985
            },
            {
                "weighted_ngram_match_score": 0.21180449792578407
            },
            {
                "syntax_match_score": 0.6349206349206349
            },
            {
                "dataflow_match_score": 0.5555555555555556
            }
        ],
        "cosine_similarity": 0.9578551054000854,
        "meteor": 0.23904288702928875
    },
    "Qwen/Qwen2-1.5B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.253495356185419
            },
            {
                "ngram_match_score": 0.005353341162078799
            },
            {
                "weighted_ngram_match_score": 0.008628083579597339
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9764575958251953,
        "meteor": 0.09371554575523705
    },
    "google/flan-ul2": {
        "code_bleu": [
            {
                "codebleu": 0.17409012937164003
            },
            {
                "ngram_match_score": 0.0032944286400630484
            },
            {
                "weighted_ngram_match_score": 0.0036615599116354733
            },
            {
                "syntax_match_score": 0.3469387755102041
            },
            {
                "dataflow_match_score": 0.3424657534246575
            }
        ],
        "cosine_similarity": 0.9705212116241455,
        "meteor": 0.05154639175257731
    },
    "deepseek-ai/deepseek-coder-6.7b-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.15842083547070746
            },
            {
                "ngram_match_score": 0.0020017952901834014
            },
            {
                "weighted_ngram_match_score": 0.007007576848254214
            },
            {
                "syntax_match_score": 0.30985915492957744
            },
            {
                "dataflow_match_score": 0.3148148148148148
            }
        ],
        "cosine_similarity": 0.9661009311676025,
        "meteor": 0.06611570247933886
    },
    "google/gemma-1.1-7b-it": {
        "code_bleu": [
            {
                "codebleu": 0.3154459226614066
            },
            {
                "ngram_match_score": 0.0003305134681869926
            },
            {
                "weighted_ngram_match_score": 0.000988060898369697
            },
            {
                "syntax_match_score": 0.26046511627906976
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9514153599739075,
        "meteor": 0.028954587016153607
    },
    "EleutherAI/gpt-neo-2.7B": {
        "code_bleu": [
            {
                "codebleu": 0.3031019123749372
            },
            {
                "ngram_match_score": 0.009062200691722947
            },
            {
                "weighted_ngram_match_score": 0.038510283972861256
            },
            {
                "syntax_match_score": 0.3076923076923077
            },
            {
                "dataflow_match_score": 0.8571428571428571
            }
        ],
        "cosine_similarity": 0.9226574897766113,
        "meteor": 0.07092198581560284
    },
    "roberta-large-mnli": {
        "code_bleu": [
            {
                "codebleu": 0.26002229622850076
            },
            {
                "ngram_match_score": 0.0015807708760779049
            },
            {
                "weighted_ngram_match_score": 0.023356898886410012
            },
            {
                "syntax_match_score": 0.18181818181818182
            },
            {
                "dataflow_match_score": 0.8333333333333334
            }
        ],
        "cosine_similarity": 0.866801381111145,
        "meteor": 0.026881720430107527
    },
    "EleutherAI/pythia-1b": {
        "code_bleu": [
            {
                "codebleu": 0.22181799226519872
            },
            {
                "ngram_match_score": 0.005802641816721098
            },
            {
                "weighted_ngram_match_score": 0.01619266325048916
            },
            {
                "syntax_match_score": 0.27906976744186046
            },
            {
                "dataflow_match_score": 0.5862068965517241
            }
        ],
        "cosine_similarity": 0.9482393264770508,
        "meteor": 0.07267441860465117
    },
    "monologg/kobigbird-bert-base": {
        "code_bleu": [
            {
                "codebleu": 0.1185072136956983
            },
            {
                "ngram_match_score": 0.003305992883254157
            },
            {
                "weighted_ngram_match_score": 0.004515102199914528
            },
            {
                "syntax_match_score": 0.3191489361702128
            },
            {
                "dataflow_match_score": 0.14705882352941177
            }
        ],
        "cosine_similarity": 0.9519588947296143,
        "meteor": 0.05409582689335394
    },
    "PixArt-alpha/PixArt-XL-2-1024-MS": {
        "code_bleu": [
            {
                "codebleu": 0.2387124404446055
            },
            {
                "ngram_match_score": 0.0012877418519909906
            },
            {
                "weighted_ngram_match_score": 0.01070487706928804
            },
            {
                "syntax_match_score": 0.13333333333333333
            },
            {
                "dataflow_match_score": 0.8095238095238095
            }
        ],
        "cosine_similarity": 0.9292263984680176,
        "meteor": 0.07056451612903226
    },
    "Alibaba-NLP/gte-base-en-v1.5": {
        "code_bleu": [
            {
                "codebleu": 0.11896262353132618
            },
            {
                "ngram_match_score": 0.003386008530337198
            },
            {
                "weighted_ngram_match_score": 0.00334246178544369
            },
            {
                "syntax_match_score": 0.27380952380952384
            },
            {
                "dataflow_match_score": 0.1953125
            }
        ],
        "cosine_similarity": 0.961464524269104,
        "meteor": 0.08382642998027613
    },
    "facebook/opt-2.7b": {
        "code_bleu": [
            {
                "codebleu": 0.13198062293058524
            },
            {
                "ngram_match_score": 0.0003169720030290251
            },
            {
                "weighted_ngram_match_score": 0.001090368204160458
            },
            {
                "syntax_match_score": 0.3181818181818182
            },
            {
                "dataflow_match_score": 0.20833333333333334
            }
        ],
        "cosine_similarity": 0.9770642518997192,
        "meteor": 0.011320754716981131
    },
    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
        "code_bleu": [
            {
                "codebleu": 0.014705882352941176
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.058823529411764705
            }
        ],
        "cosine_similarity": 0.9520319104194641,
        "meteor": 0.0
    },
    "codellama/CodeLlama-7b-hf": {
        "code_bleu": [
            {
                "codebleu": 0.22379914910598298
            },
            {
                "ngram_match_score": 0.0031937854049374466
            },
            {
                "weighted_ngram_match_score": 0.012955191971375388
            },
            {
                "syntax_match_score": 0.26
            },
            {
                "dataflow_match_score": 0.6190476190476191
            }
        ],
        "cosine_similarity": 0.9719783663749695,
        "meteor": 0.08928571428571427
    },
    "mistralai/Mistral-7B-Instruct-v0.2": {
        "code_bleu": [
            {
                "codebleu": 0.18700988207564143
            },
            {
                "ngram_match_score": 0.0012279656994805565
            },
            {
                "weighted_ngram_match_score": 0.002239404237823333
            },
            {
                "syntax_match_score": 0.4482758620689655
            },
            {
                "dataflow_match_score": 0.2962962962962963
            }
        ],
        "cosine_similarity": 0.9863187670707703,
        "meteor": 0.03492063492063492
    },
    "microsoft/phi-1/5": {
        "code_bleu": [
            {
                "codebleu": 0.23594346614215833
            },
            {
                "ngram_match_score": 0.011784541906557342
            },
            {
                "weighted_ngram_match_score": 0.019298219328128603
            },
            {
                "syntax_match_score": 0.4044943820224719
            },
            {
                "dataflow_match_score": 0.5081967213114754
            }
        ],
        "cosine_similarity": 0.9877502918243408,
        "meteor": 0.13223290161174575
    },
    "roberta-base-openai-detector": {
        "code_bleu": [
            {
                "codebleu": 0.22553458984790528
            },
            {
                "ngram_match_score": 0.00205598610184581
            },
            {
                "weighted_ngram_match_score": 0.009326070768766905
            },
            {
                "syntax_match_score": 0.17647058823529413
            },
            {
                "dataflow_match_score": 0.7142857142857143
            }
        ],
        "cosine_similarity": 0.9082303047180176,
        "meteor": 0.019305019305019305
    },
    "bhadresh-savani/distilbert-base-uncased-emotion": {
        "code_bleu": [
            {
                "codebleu": 0.1375
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.05
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9624227285385132,
        "meteor": 0.0
    },
    "uer/gpt2-chinese-cluecorpussmall": {
        "code_bleu": [
            {
                "codebleu": 0.20078219081702164
            },
            {
                "ngram_match_score": 0.004592167446468838
            },
            {
                "weighted_ngram_match_score": 0.0048303021153239855
            },
            {
                "syntax_match_score": 0.38461538461538464
            },
            {
                "dataflow_match_score": 0.4090909090909091
            }
        ],
        "cosine_similarity": 0.9678006172180176,
        "meteor": 0.03306666666666667
    },
    "THUDM/cogvlm2-llama3-chat-19B": {
        "code_bleu": [
            {
                "codebleu": 0.1294740574929822
            },
            {
                "ngram_match_score": 0.013520304042614997
            },
            {
                "weighted_ngram_match_score": 0.030182576302087833
            },
            {
                "syntax_match_score": 0.29739776951672864
            },
            {
                "dataflow_match_score": 0.17679558011049723
            }
        ],
        "cosine_similarity": 0.9874677062034607,
        "meteor": 0.09416265963497233
    },
    "NousResearch/Meta-Llama-3-8B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.19419264083357057
            },
            {
                "ngram_match_score": 0.040225772947222085
            },
            {
                "weighted_ngram_match_score": 0.05745576347194624
            },
            {
                "syntax_match_score": 0.4409937888198758
            },
            {
                "dataflow_match_score": 0.23809523809523808
            }
        ],
        "cosine_similarity": 0.9848634004592896,
        "meteor": 0.1387566548252138
    },
    "HuggingFaceH4/zephyr-7b-alpha": {
        "code_bleu": [
            {
                "codebleu": 0.22497665143255688
            },
            {
                "ngram_match_score": 0.0013726109161076585
            },
            {
                "weighted_ngram_match_score": 0.0022022563771183375
            },
            {
                "syntax_match_score": 0.3508771929824561
            },
            {
                "dataflow_match_score": 0.5454545454545454
            }
        ],
        "cosine_similarity": 0.9478381276130676,
        "meteor": 0.032175032175032175
    },
    "tiiuae/falcon-40b-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.09829035229104306
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.07920792079207921
            },
            {
                "dataflow_match_score": 0.313953488372093
            }
        ],
        "cosine_similarity": 0.9694222211837769,
        "meteor": 0.0
    },
    "stabilityai/sd-turbo": {
        "code_bleu": [
            {
                "codebleu": 0.16959505983517087
            },
            {
                "ngram_match_score": 0.002617787947591995
            },
            {
                "weighted_ngram_match_score": 0.0036012792319193646
            },
            {
                "syntax_match_score": 0.24358974358974358
            },
            {
                "dataflow_match_score": 0.42857142857142855
            }
        ],
        "cosine_similarity": 0.9732143878936768,
        "meteor": 0.05082592121982212
    },
    "EleutherAI/gpt-neo-1.3B": {
        "code_bleu": [
            {
                "codebleu": 0.3005482766347213
            },
            {
                "ngram_match_score": 0.002104351645526129
            },
            {
                "weighted_ngram_match_score": 0.007781062585666588
            },
            {
                "syntax_match_score": 0.19230769230769232
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9098894596099854,
        "meteor": 0.024509803921568627
    },
    "TheBloke/Mistral-7B-Instruct-v0.2-GGUF": {
        "code_bleu": [
            {
                "codebleu": 0.21318196511410076
            },
            {
                "ngram_match_score": 0.0028695702336182464
            },
            {
                "weighted_ngram_match_score": 0.003704444068938591
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 0.34615384615384615
            }
        ],
        "cosine_similarity": 0.9813823699951172,
        "meteor": 0.087890625
    },
    "bert-large-cased": {
        "code_bleu": [
            {
                "codebleu": 0.18669791237709854
            },
            {
                "ngram_match_score": 0.001980551964781254
            },
            {
                "weighted_ngram_match_score": 0.0021243429924515597
            },
            {
                "syntax_match_score": 0.40370370370370373
            },
            {
                "dataflow_match_score": 0.3389830508474576
            }
        ],
        "cosine_similarity": 0.9787908792495728,
        "meteor": 0.03763440860215054
    },
    "openchat/openchat-3.6-8b-20240522": {
        "code_bleu": [
            {
                "codebleu": 0.030073597051329375
            },
            {
                "ngram_match_score": 0.0014881650601031944
            },
            {
                "weighted_ngram_match_score": 0.003421607760598922
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.11538461538461539
            }
        ],
        "cosine_similarity": 0.9644324779510498,
        "meteor": 0.008756567425569179
    },
    "EleutherAI/pythia-160m": {
        "code_bleu": [
            {
                "codebleu": 0.11551353669319715
            },
            {
                "ngram_match_score": 0.004837632096507674
            },
            {
                "weighted_ngram_match_score": 0.042621486608919235
            },
            {
                "syntax_match_score": 0.06976744186046512
            },
            {
                "dataflow_match_score": 0.3448275862068966
            }
        ],
        "cosine_similarity": 0.9358259439468384,
        "meteor": 0.08187772925764193
    },
    "EleutherAI/pythia-70m-deduped": {
        "code_bleu": [
            {
                "codebleu": 0.2713943713011191
            },
            {
                "ngram_match_score": 0.0018926919188554955
            },
            {
                "weighted_ngram_match_score": 0.009105803710641137
            },
            {
                "syntax_match_score": 0.4883720930232558
            },
            {
                "dataflow_match_score": 0.5862068965517241
            }
        ],
        "cosine_similarity": 0.966922402381897,
        "meteor": 0.062034739454094316
    },
    "Salesforce/codet5p-770m": {
        "code_bleu": [
            {
                "codebleu": 0.16391516564855452
            },
            {
                "ngram_match_score": 0.004644469948654294
            },
            {
                "weighted_ngram_match_score": 0.027291702849645345
            },
            {
                "syntax_match_score": 0.061224489795918366
            },
            {
                "dataflow_match_score": 0.5625
            }
        ],
        "cosine_similarity": 0.9320654273033142,
        "meteor": 0.026362038664323375
    },
    "EleutherAI/pythia-410m": {
        "code_bleu": [
            {
                "codebleu": 0.13278802606222587
            },
            {
                "ngram_match_score": 0.005887199662988413
            },
            {
                "weighted_ngram_match_score": 0.04170435927717409
            },
            {
                "syntax_match_score": 0.06976744186046512
            },
            {
                "dataflow_match_score": 0.41379310344827586
            }
        ],
        "cosine_similarity": 0.9489525556564331,
        "meteor": 0.03640776699029127
    },
    "facebook/opt-13b": {
        "code_bleu": [
            {
                "codebleu": 0.09909887569665879
            },
            {
                "ngram_match_score": 0.0005042610278096326
            },
            {
                "weighted_ngram_match_score": 0.0011614041224135452
            },
            {
                "syntax_match_score": 0.17301038062283736
            },
            {
                "dataflow_match_score": 0.22171945701357465
            }
        ],
        "cosine_similarity": 0.9790289402008057,
        "meteor": 0.03126028298782494
    },
    "google/gemma-1.1-2b-it": {
        "code_bleu": [
            {
                "codebleu": 0.3912365749431824
            },
            {
                "ngram_match_score": 0.026754714893689108
            },
            {
                "weighted_ngram_match_score": 0.047537379271563956
            },
            {
                "syntax_match_score": 0.49065420560747663
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9815770387649536,
        "meteor": 0.07512296821552522
    },
    "Salesforce/blip2-flan-t5-xl": {
        "code_bleu": [
            {
                "codebleu": 0.10023817265762158
            },
            {
                "ngram_match_score": 0.0012402394206442282
            },
            {
                "weighted_ngram_match_score": 0.0012864979139508364
            },
            {
                "syntax_match_score": 0.2426470588235294
            },
            {
                "dataflow_match_score": 0.15577889447236182
            }
        ],
        "cosine_similarity": 0.9598329067230225,
        "meteor": 0.015932023366967602
    },
    "tiiuae/falcon-7b": {
        "code_bleu": [
            {
                "codebleu": 0.1271828194107543
            },
            {
                "ngram_match_score": 0.028313786355336205
            },
            {
                "weighted_ngram_match_score": 0.03602421475072494
            },
            {
                "syntax_match_score": 0.1188118811881188
            },
            {
                "dataflow_match_score": 0.32558139534883723
            }
        ],
        "cosine_similarity": 0.9752835035324097,
        "meteor": 0.053987501468152226
    },
    "nvidia/segformer-b0-finetuned-ade-512-512": {
        "code_bleu": [
            {
                "codebleu": 0.17121988130921706
            },
            {
                "ngram_match_score": 0.001257297921162267
            },
            {
                "weighted_ngram_match_score": 0.009336513029991762
            },
            {
                "syntax_match_score": 0.16
            },
            {
                "dataflow_match_score": 0.5142857142857142
            }
        ],
        "cosine_similarity": 0.9391132593154907,
        "meteor": 0.052356020942408384
    },
    "llava-hf/llava-1.5-13b-hf": {
        "code_bleu": [
            {
                "codebleu": 0.22451464474370286
            },
            {
                "ngram_match_score": 0.031589762030522335
            },
            {
                "weighted_ngram_match_score": 0.050754531230003445
            },
            {
                "syntax_match_score": 0.4857142857142857
            },
            {
                "dataflow_match_score": 0.33
            }
        ],
        "cosine_similarity": 0.9889422655105591,
        "meteor": 0.09724920970969518
    },
    "stabilityai/stable-cascade": {
        "code_bleu": [
            {
                "codebleu": 0.04123182723050009
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.12773722627737227
            },
            {
                "dataflow_match_score": 0.0371900826446281
            }
        ],
        "cosine_similarity": 0.9806461334228516,
        "meteor": 0.0
    },
    "hustvl/yolos-small": {
        "code_bleu": [
            {
                "codebleu": 0.20379617441888154
            },
            {
                "ngram_match_score": 0.005888794423372789
            },
            {
                "weighted_ngram_match_score": 0.011381305436860433
            },
            {
                "syntax_match_score": 0.24528301886792453
            },
            {
                "dataflow_match_score": 0.5526315789473685
            }
        ],
        "cosine_similarity": 0.9507702589035034,
        "meteor": 0.1076320939334638
    },
    "sentence-transformers/sentence-t5-base": {
        "code_bleu": [
            {
                "codebleu": 0.2540924487554178
            },
            {
                "ngram_match_score": 0.004887945201591423
            },
            {
                "weighted_ngram_match_score": 0.019057607395837256
            },
            {
                "syntax_match_score": 0.4090909090909091
            },
            {
                "dataflow_match_score": 0.5833333333333334
            }
        ],
        "cosine_similarity": 0.9046802520751953,
        "meteor": 0.0859106529209622
    },
    "EleutherAI/pythia-1.4b": {
        "code_bleu": [
            {
                "codebleu": 0.1616082760411785
            },
            {
                "ngram_match_score": 0.003004926757200381
            },
            {
                "weighted_ngram_match_score": 0.009105803710641137
            },
            {
                "syntax_match_score": 0.18604651162790697
            },
            {
                "dataflow_match_score": 0.4482758620689655
            }
        ],
        "cosine_similarity": 0.9534762501716614,
        "meteor": 0.07122507122507123
    },
    "stabilityai/stablelm-2-zephyr-1/6b": {
        "code_bleu": [
            {
                "codebleu": 0.14703947368421053
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.2631578947368421
            },
            {
                "dataflow_match_score": 0.325
            }
        ],
        "cosine_similarity": 0.96677166223526,
        "meteor": 0.0
    },
    "sentence-transformers/clip-ViT-B-32-multilingual-v1": {
        "code_bleu": [
            {
                "codebleu": 0.1058831616812432
            },
            {
                "ngram_match_score": 0.007054752105204712
            },
            {
                "weighted_ngram_match_score": 0.009270687412560907
            },
            {
                "syntax_match_score": 0.2072072072072072
            },
            {
                "dataflow_match_score": 0.2
            }
        ],
        "cosine_similarity": 0.9526685476303101,
        "meteor": 0.02564102564102564
    },
    "Qwen/Qwen1.5-0.5B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.48631890435789654
            },
            {
                "ngram_match_score": 0.39264231291070095
            },
            {
                "weighted_ngram_match_score": 0.4095748126251841
            },
            {
                "syntax_match_score": 0.627906976744186
            },
            {
                "dataflow_match_score": 0.5151515151515151
            }
        ],
        "cosine_similarity": 0.9858726263046265,
        "meteor": 0.5564779916406453
    },
    "SimianLuo/LCM/Dreamshaper/v7": {
        "code_bleu": [
            {
                "codebleu": 0.25355721908541945
            },
            {
                "ngram_match_score": 0.002311798500092198
            },
            {
                "weighted_ngram_match_score": 0.005338130473164603
            },
            {
                "syntax_match_score": 0.3815789473684211
            },
            {
                "dataflow_match_score": 0.625
            }
        ],
        "cosine_similarity": 0.9529244899749756,
        "meteor": 0.03496503496503496
    },
    "kykim/bert-kor-base": {
        "code_bleu": [
            {
                "codebleu": 0.33180526040112746
            },
            {
                "ngram_match_score": 0.001784672286871559
            },
            {
                "weighted_ngram_match_score": 0.019554016376461803
            },
            {
                "syntax_match_score": 0.7058823529411765
            },
            {
                "dataflow_match_score": 0.6
            }
        ],
        "cosine_similarity": 0.9158796072006226,
        "meteor": 0.045662100456621016
    },
    "aubmindlab/bert-base-arabert": {
        "code_bleu": [
            {
                "codebleu": 0.2285533498727363
            },
            {
                "ngram_match_score": 0.0009590792344966317
            },
            {
                "weighted_ngram_match_score": 0.007659914662042851
            },
            {
                "syntax_match_score": 0.13636363636363635
            },
            {
                "dataflow_match_score": 0.7692307692307693
            }
        ],
        "cosine_similarity": 0.9277443885803223,
        "meteor": 0.01692047377326565
    },
    "google/mobilenet/v2/1.0/224": {
        "code_bleu": [
            {
                "codebleu": 0.31854087184705665
            },
            {
                "ngram_match_score": 0.0806966849027814
            },
            {
                "weighted_ngram_match_score": 0.14584775486639753
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 0.5476190476190477
            }
        ],
        "cosine_similarity": 0.9614718556404114,
        "meteor": 0.2944263059701493
    },
    "nvidia/mit-b0": {
        "code_bleu": [
            {
                "codebleu": 0.17696635837507552
            },
            {
                "ngram_match_score": 0.0053733870784451725
            },
            {
                "weighted_ngram_match_score": 0.007686851616662047
            },
            {
                "syntax_match_score": 0.24242424242424243
            },
            {
                "dataflow_match_score": 0.4523809523809524
            }
        ],
        "cosine_similarity": 0.9603442549705505,
        "meteor": 0.0508130081300813
    },
    "facebook/m2m100/1.2B": {
        "code_bleu": [
            {
                "codebleu": 0.3315030544141325
            },
            {
                "ngram_match_score": 0.003060136510355402
            },
            {
                "weighted_ngram_match_score": 0.0029520811461746835
            },
            {
                "syntax_match_score": 0.32
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.8824472427368164,
        "meteor": 0.025673940949935817
    },
    "deepseek-ai/deepseek-coder-33b-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.1959783042925882
            },
            {
                "ngram_match_score": 0.004655158407050898
            },
            {
                "weighted_ngram_match_score": 0.01086995234702647
            },
            {
                "syntax_match_score": 0.323943661971831
            },
            {
                "dataflow_match_score": 0.4444444444444444
            }
        ],
        "cosine_similarity": 0.9771217107772827,
        "meteor": 0.07005253940455342
    },
    "cross-encoder/ms-marco-TinyBERT-L-2": {
        "code_bleu": [
            {
                "codebleu": 0.05
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.2
            }
        ],
        "cosine_similarity": 0.940532386302948,
        "meteor": 0.0
    },
    "nvidia/Llama3-ChatQA-1.5-8B": {
        "code_bleu": [
            {
                "codebleu": 0.02996977094740775
            },
            {
                "ngram_match_score": 0.0001254193638024359
            },
            {
                "weighted_ngram_match_score": 0.0016384902373027344
            },
            {
                "syntax_match_score": 0.11048158640226628
            },
            {
                "dataflow_match_score": 0.007633587786259542
            }
        ],
        "cosine_similarity": 0.9608855247497559,
        "meteor": 0.03396066690253117
    },
    "mosaicml/mpt-7b": {
        "code_bleu": [
            {
                "codebleu": 0.27293577981651373
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.09174311926605505
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9397162199020386,
        "meteor": 0.0
    },
    "Qwen/Qwen-7B": {
        "code_bleu": [
            {
                "codebleu": 0.19037253506593024
            },
            {
                "ngram_match_score": 0.005068525327948969
            },
            {
                "weighted_ngram_match_score": 0.012319981541942584
            },
            {
                "syntax_match_score": 0.08620689655172414
            },
            {
                "dataflow_match_score": 0.6578947368421053
            }
        ],
        "cosine_similarity": 0.9682161808013916,
        "meteor": 0.02561912894961571
    },
    "Falconsai/text/summarization": {
        "code_bleu": [
            {
                "codebleu": 0.3184190736818136
            },
            {
                "ngram_match_score": 0.0005073652884048407
            },
            {
                "weighted_ngram_match_score": 0.006502262772182844
            },
            {
                "syntax_match_score": 0.4666666666666667
            },
            {
                "dataflow_match_score": 0.8
            }
        ],
        "cosine_similarity": 0.960995078086853,
        "meteor": 0.02386257435015278
    },
    "facebook/opt-6.7b": {
        "code_bleu": [
            {
                "codebleu": 0.1152349735003661
            },
            {
                "ngram_match_score": 0.0017245446375075786
            },
            {
                "weighted_ngram_match_score": 0.003160207437479345
            },
            {
                "syntax_match_score": 0.21498371335504887
            },
            {
                "dataflow_match_score": 0.24107142857142858
            }
        ],
        "cosine_similarity": 0.9844748973846436,
        "meteor": 0.05378096479791396
    },
    "NeuML/pubmedbert-base-embeddings": {
        "code_bleu": [
            {
                "codebleu": 0.1669700971529679
            },
            {
                "ngram_match_score": 0.013717103535833945
            },
            {
                "weighted_ngram_match_score": 0.014018619614916522
            },
            {
                "syntax_match_score": 0.2857142857142857
            },
            {
                "dataflow_match_score": 0.35443037974683544
            }
        ],
        "cosine_similarity": 0.9782580137252808,
        "meteor": 0.11243851018973998
    },
    "sentence-transformers/paraphrase-xlm-r-multilingual-v1": {
        "code_bleu": [
            {
                "codebleu": 0.20282509564107606
            },
            {
                "ngram_match_score": 0.0023812487026605355
            },
            {
                "weighted_ngram_match_score": 0.0026303139858673843
            },
            {
                "syntax_match_score": 0.24107142857142858
            },
            {
                "dataflow_match_score": 0.5652173913043478
            }
        ],
        "cosine_similarity": 0.954025387763977,
        "meteor": 0.048154093097913325
    },
    "TheBloke/Llama-2-7B-GPTQ": {
        "code_bleu": [
            {
                "codebleu": 0.3460112621719474
            },
            {
                "ngram_match_score": 0.01191078176976675
            },
            {
                "weighted_ngram_match_score": 0.016578711362467257
            },
            {
                "syntax_match_score": 0.35555555555555557
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9892019033432007,
        "meteor": 0.135612389477269
    },
    "tuner007/pegasus/paraphrase": {
        "code_bleu": [
            {
                "codebleu": 0.34807302154487757
            },
            {
                "ngram_match_score": 0.10665984800079377
            },
            {
                "weighted_ngram_match_score": 0.2964547490012273
            },
            {
                "syntax_match_score": 0.5606060606060606
            },
            {
                "dataflow_match_score": 0.42857142857142855
            }
        ],
        "cosine_similarity": 0.9543650150299072,
        "meteor": 0.32693674484719265
    },
    "indolem/indobert-base-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.3824115601979837
            },
            {
                "ngram_match_score": 0.006349366327757819
            },
            {
                "weighted_ngram_match_score": 0.11741452152300066
            },
            {
                "syntax_match_score": 0.7058823529411765
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.9198687672615051,
        "meteor": 0.11627906976744187
    },
    "huggingface/CodeBERTa-small-v1": {
        "code_bleu": [
            {
                "codebleu": 0.18944834364007232
            },
            {
                "ngram_match_score": 0.0021470850489829487
            },
            {
                "weighted_ngram_match_score": 0.005646289511306304
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.75
            }
        ],
        "cosine_similarity": 0.9432394504547119,
        "meteor": 0.025597269624573378
    },
    "setu4993/LaBSE": {
        "code_bleu": [
            {
                "codebleu": 0.1835650232502986
            },
            {
                "ngram_match_score": 0.011926526830536665
            },
            {
                "weighted_ngram_match_score": 0.021283649058850797
            },
            {
                "syntax_match_score": 0.25842696629213485
            },
            {
                "dataflow_match_score": 0.4426229508196721
            }
        ],
        "cosine_similarity": 0.972974956035614,
        "meteor": 0.12418300653594773
    },
    "NousResearch/Meta-Llama-3-8B": {
        "code_bleu": [
            {
                "codebleu": 0.3413282964936957
            },
            {
                "ngram_match_score": 0.0018648925869794491
            },
            {
                "weighted_ngram_match_score": 0.008609583710384035
            },
            {
                "syntax_match_score": 0.3548387096774194
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9287553429603577,
        "meteor": 0.05405405405405406
    },
    "microsoft/BiomedCLIP-PubMedBERT/256-vit/base/patch16/224": {
        "code_bleu": [
            {
                "codebleu": 0.044024912080864134
            },
            {
                "ngram_match_score": 0.0025425489723849936
            },
            {
                "weighted_ngram_match_score": 0.006081861833384058
            },
            {
                "syntax_match_score": 0.04982817869415808
            },
            {
                "dataflow_match_score": 0.11764705882352941
            }
        ],
        "cosine_similarity": 0.9699900150299072,
        "meteor": 0.02989074417645846
    },
    "nerijs/pixel-art-xl": {
        "code_bleu": [
            {
                "codebleu": 0.09150152851861992
            },
            {
                "ngram_match_score": 0.002296801792830711
            },
            {
                "weighted_ngram_match_score": 0.004999351772653838
            },
            {
                "syntax_match_score": 0.1511627906976744
            },
            {
                "dataflow_match_score": 0.20754716981132076
            }
        ],
        "cosine_similarity": 0.97720867395401,
        "meteor": 0.03205128205128205
    },
    "microsoft/biogpt": {
        "code_bleu": [
            {
                "codebleu": 0.2923355920436833
            },
            {
                "ngram_match_score": 0.08073835414301818
            },
            {
                "weighted_ngram_match_score": 0.11615695328443358
            },
            {
                "syntax_match_score": 0.5033112582781457
            },
            {
                "dataflow_match_score": 0.4691358024691358
            }
        ],
        "cosine_similarity": 0.9799877405166626,
        "meteor": 0.20118923687692988
    },
    "databricks/dolly-v2-3b": {
        "code_bleu": [
            {
                "codebleu": 0.12719178193631508
            },
            {
                "ngram_match_score": 0.0019329315323428134
            },
            {
                "weighted_ngram_match_score": 0.0029912555278716283
            },
            {
                "syntax_match_score": 0.23015873015873015
            },
            {
                "dataflow_match_score": 0.2736842105263158
            }
        ],
        "cosine_similarity": 0.9690815210342407,
        "meteor": 0.06756756756756757
    },
    "timm/ViT-B-16-SigLIP": {
        "code_bleu": [
            {
                "codebleu": 0.3422979736783095
            },
            {
                "ngram_match_score": 0.005453797693239049
            },
            {
                "weighted_ngram_match_score": 0.005783551565453522
            },
            {
                "syntax_match_score": 0.35795454545454547
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9891067743301392,
        "meteor": 0.0727802037845706
    },
    "dreamlike-art/dreamlike-photoreal-2.0": {
        "code_bleu": [
            {
                "codebleu": 0.1867236692997539
            },
            {
                "ngram_match_score": 0.00599838612391244
            },
            {
                "weighted_ngram_match_score": 0.0367900108818665
            },
            {
                "syntax_match_score": 0.1388888888888889
            },
            {
                "dataflow_match_score": 0.5652173913043478
            }
        ],
        "cosine_similarity": 0.9165657758712769,
        "meteor": 0.06517690875232776
    },
    "Salesforce/codet5-small": {
        "code_bleu": [
            {
                "codebleu": 0.21887361252902002
            },
            {
                "ngram_match_score": 0.004212303041767081
            },
            {
                "weighted_ngram_match_score": 0.012933178969247342
            },
            {
                "syntax_match_score": 0.5121951219512195
            },
            {
                "dataflow_match_score": 0.34615384615384615
            }
        ],
        "cosine_similarity": 0.9638145565986633,
        "meteor": 0.06507592190889372
    },
    "Qwen/Qwen1.5-7B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.20166710432477686
            },
            {
                "ngram_match_score": 0.007412472280612276
            },
            {
                "weighted_ngram_match_score": 0.009967713869798944
            },
            {
                "syntax_match_score": 0.3953488372093023
            },
            {
                "dataflow_match_score": 0.3939393939393939
            }
        ],
        "cosine_similarity": 0.9753190279006958,
        "meteor": 0.0855188141391106
    },
    "EleutherAI/pythia-410m-deduped": {
        "code_bleu": [
            {
                "codebleu": 0.216053973256926
            },
            {
                "ngram_match_score": 0.005350346902093604
            },
            {
                "weighted_ngram_match_score": 0.04170435927717409
            },
            {
                "syntax_match_score": 0.09302325581395349
            },
            {
                "dataflow_match_score": 0.7241379310344828
            }
        ],
        "cosine_similarity": 0.9558764696121216,
        "meteor": 0.035128805620608904
    },
    "tiiuae/falcon-40b": {
        "code_bleu": [
            {
                "codebleu": 0.09338439570811263
            },
            {
                "ngram_match_score": 0.004068881381359766
            },
            {
                "weighted_ngram_match_score": 0.005204368041005609
            },
            {
                "syntax_match_score": 0.1782178217821782
            },
            {
                "dataflow_match_score": 0.18604651162790697
            }
        ],
        "cosine_similarity": 0.9694427251815796,
        "meteor": 0.09389671361502347
    },
    "THUDM/cogvlm-chat-hf": {
        "code_bleu": [
            {
                "codebleu": 0.11198700708035582
            },
            {
                "ngram_match_score": 0.0002191682863378874
            },
            {
                "weighted_ngram_match_score": 0.0010410197447042779
            },
            {
                "syntax_match_score": 0.28879310344827586
            },
            {
                "dataflow_match_score": 0.15789473684210525
            }
        ],
        "cosine_similarity": 0.9165114164352417,
        "meteor": 0.024984384759525295
    },
    "Salesforce/codegen-350M-mono": {
        "code_bleu": [
            {
                "codebleu": 0.22574086330212736
            },
            {
                "ngram_match_score": 0.0033983588614059147
            },
            {
                "weighted_ngram_match_score": 0.023392486467178574
            },
            {
                "syntax_match_score": 0.4146341463414634
            },
            {
                "dataflow_match_score": 0.46153846153846156
            }
        ],
        "cosine_similarity": 0.9420348405838013,
        "meteor": 0.10443864229765014
    },
    "ckiplab/bert-base-chinese": {
        "code_bleu": [
            {
                "codebleu": 0.28405158436977046
            },
            {
                "ngram_match_score": 0.00445061034936547
            },
            {
                "weighted_ngram_match_score": 0.09646160948265745
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.8
            }
        ],
        "cosine_similarity": 0.9181678295135498,
        "meteor": 0.06042296072507554
    },
    "diffusers/controlnet-depth-sdxl-1.0": {
        "code_bleu": [
            {
                "codebleu": 0.12726787531337974
            },
            {
                "ngram_match_score": 0.002510297983656472
            },
            {
                "weighted_ngram_match_score": 0.002457075690125219
            },
            {
                "syntax_match_score": 0.30288461538461536
            },
            {
                "dataflow_match_score": 0.20121951219512196
            }
        ],
        "cosine_similarity": 0.987156093120575,
        "meteor": 0.06574394463667821
    },
    "thibaud/controlnet-openpose-sdxl-1.0": {
        "code_bleu": [
            {
                "codebleu": 0.026990582223669554
            },
            {
                "ngram_match_score": 0.0017400953025671476
            },
            {
                "weighted_ngram_match_score": 0.0027739577300421084
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.10344827586206896
            }
        ],
        "cosine_similarity": 0.9484061002731323,
        "meteor": 0.01770956316410862
    },
    "albert-base-v1": {
        "code_bleu": [
            {
                "codebleu": 0.21964611313984714
            },
            {
                "ngram_match_score": 0.0009428600593117587
            },
            {
                "weighted_ngram_match_score": 0.0010565328641697815
            },
            {
                "syntax_match_score": 0.5037037037037037
            },
            {
                "dataflow_match_score": 0.3728813559322034
            }
        ],
        "cosine_similarity": 0.9749736189842224,
        "meteor": 0.018982536066818528
    },
    "dreamlike-art/dreamlike-diffusion-1.0": {
        "code_bleu": [
            {
                "codebleu": 0.23281529851426297
            },
            {
                "ngram_match_score": 0.0021762540846192785
            },
            {
                "weighted_ngram_match_score": 0.007587355431369793
            },
            {
                "syntax_match_score": 0.1388888888888889
            },
            {
                "dataflow_match_score": 0.782608695652174
            }
        ],
        "cosine_similarity": 0.9583350419998169,
        "meteor": 0.033003300330033
    },
    "facebook/convnext-tiny-224": {
        "code_bleu": [
            {
                "codebleu": 0.23821165332425165
            },
            {
                "ngram_match_score": 0.005730717317104476
            },
            {
                "weighted_ngram_match_score": 0.011901269588805136
            },
            {
                "syntax_match_score": 0.36764705882352944
            },
            {
                "dataflow_match_score": 0.5675675675675675
            }
        ],
        "cosine_similarity": 0.9606497287750244,
        "meteor": 0.1183431952662722
    },
    "klue/roberta-base": {
        "code_bleu": [
            {
                "codebleu": 0.16423812141726168
            },
            {
                "ngram_match_score": 0.002104351645526129
            },
            {
                "weighted_ngram_match_score": 0.019554016376461803
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.4
            }
        ],
        "cosine_similarity": 0.9239853620529175,
        "meteor": 0.04975124378109453
    },
    "berkeley-nest/Starling-LM-7B-alpha": {
        "code_bleu": [
            {
                "codebleu": 0.1375148669674401
            },
            {
                "ngram_match_score": 0.0005904264808197796
            },
            {
                "weighted_ngram_match_score": 0.0017935195673088298
            },
            {
                "syntax_match_score": 0.25735294117647056
            },
            {
                "dataflow_match_score": 0.2903225806451613
            }
        ],
        "cosine_similarity": 0.9813913106918335,
        "meteor": 0.04320145520691222
    },
    "prompthero/openjourney": {
        "code_bleu": [
            {
                "codebleu": 0.25092358234997714
            },
            {
                "ngram_match_score": 0.005034706048148382
            },
            {
                "weighted_ngram_match_score": 0.0167755653807457
            },
            {
                "syntax_match_score": 0.4166666666666667
            },
            {
                "dataflow_match_score": 0.5652173913043478
            }
        ],
        "cosine_similarity": 0.9269558787345886,
        "meteor": 0.08158508158508158
    },
    "dbmdz/bert-base-german-cased": {
        "code_bleu": [
            {
                "codebleu": 0.2396356819120402
            },
            {
                "ngram_match_score": 0.0016085336197478118
            },
            {
                "weighted_ngram_match_score": 0.02164007638135428
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.905164897441864,
        "meteor": 0.06097560975609756
    },
    "google/byt5-base": {
        "code_bleu": [
            {
                "codebleu": 0.1293810929978012
            },
            {
                "ngram_match_score": 0.007060145441373793
            },
            {
                "weighted_ngram_match_score": 0.013428653427301322
            },
            {
                "syntax_match_score": 0.31521739130434784
            },
            {
                "dataflow_match_score": 0.18181818181818182
            }
        ],
        "cosine_similarity": 0.9497606754302979,
        "meteor": 0.07464984869178651
    },
    "kandinsky-community/kandinsky-2-2-decoder": {
        "code_bleu": [
            {
                "codebleu": 0.20100041941681576
            },
            {
                "ngram_match_score": 0.030643267654075202
            },
            {
                "weighted_ngram_match_score": 0.04125440089685101
            },
            {
                "syntax_match_score": 0.5076142131979695
            },
            {
                "dataflow_match_score": 0.22448979591836735
            }
        ],
        "cosine_similarity": 0.9573488831520081,
        "meteor": 0.10979729729729729
    },
    "microsoft/resnet-101": {
        "code_bleu": [
            {
                "codebleu": 0.08512963019850431
            },
            {
                "ngram_match_score": 0.004669770271800337
            },
            {
                "weighted_ngram_match_score": 0.004370213161326582
            },
            {
                "syntax_match_score": 0.08823529411764706
            },
            {
                "dataflow_match_score": 0.24324324324324326
            }
        ],
        "cosine_similarity": 0.9759947061538696,
        "meteor": 0.0228310502283105
    },
    "EleutherAI/gpt-neox-20b": {
        "code_bleu": [
            {
                "codebleu": 0.3030966594110273
            },
            {
                "ngram_match_score": 0.0033163000893597764
            },
            {
                "weighted_ngram_match_score": 0.03848210226063191
            },
            {
                "syntax_match_score": 0.47058823529411764
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.9281355142593384,
        "meteor": 0.06637168141592921
    },
    "sentence-transformers/distiluse-base-multilingual-cased": {
        "code_bleu": [
            {
                "codebleu": 0.2570682399282096
            },
            {
                "ngram_match_score": 0.004309287083540006
            },
            {
                "weighted_ngram_match_score": 0.02396367262929851
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9088122844696045,
        "meteor": 0.09090909090909091
    },
    "Qwen/Qwen1.5-32B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.3186139500733059
            },
            {
                "ngram_match_score": 0.0034805831802288212
            },
            {
                "weighted_ngram_match_score": 0.0035333566478784324
            },
            {
                "syntax_match_score": 0.26744186046511625
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9487712979316711,
        "meteor": 0.05263157894736842
    },
    "emilianJR/epiCRealism": {
        "code_bleu": [
            {
                "codebleu": 0.21660683606509162
            },
            {
                "ngram_match_score": 0.003787047284389884
            },
            {
                "weighted_ngram_match_score": 0.011191021613657745
            },
            {
                "syntax_match_score": 0.4166666666666667
            },
            {
                "dataflow_match_score": 0.43478260869565216
            }
        ],
        "cosine_similarity": 0.9359903335571289,
        "meteor": 0.10309278350515463
    },
    "blanchefort/rubert-base-cased-sentiment": {
        "code_bleu": [
            {
                "codebleu": 0.5839524036715837
            },
            {
                "ngram_match_score": 0.21847222437885042
            },
            {
                "weighted_ngram_match_score": 0.6917421522122466
            },
            {
                "syntax_match_score": 0.9047619047619048
            },
            {
                "dataflow_match_score": 0.5208333333333334
            }
        ],
        "cosine_similarity": 0.9756518602371216,
        "meteor": 0.5711797551354952
    },
    "xlnet-large-cased": {
        "code_bleu": [
            {
                "codebleu": 0.17621375809646353
            },
            {
                "ngram_match_score": 0.0026200091529501165
            },
            {
                "weighted_ngram_match_score": 0.010270737518618322
            },
            {
                "syntax_match_score": 0.40625
            },
            {
                "dataflow_match_score": 0.2857142857142857
            }
        ],
        "cosine_similarity": 0.9321010112762451,
        "meteor": 0.07621951219512196
    },
    "stabilityai/stable-cascade-prior": {
        "code_bleu": [
            {
                "codebleu": 0.038793409229678555
            },
            {
                "ngram_match_score": 0.0011374414038221846
            },
            {
                "weighted_ngram_match_score": 0.0035876102836981147
            },
            {
                "syntax_match_score": 0.08695652173913043
            },
            {
                "dataflow_match_score": 0.06349206349206349
            }
        ],
        "cosine_similarity": 0.9519380331039429,
        "meteor": 0.03504672897196262
    },
    "deepset/bert-base-cased-squad2": {
        "code_bleu": [
            {
                "codebleu": 0.22568946969364123
            },
            {
                "ngram_match_score": 0.002427755902198282
            },
            {
                "weighted_ngram_match_score": 0.0031106591186427775
            },
            {
                "syntax_match_score": 0.23684210526315788
            },
            {
                "dataflow_match_score": 0.660377358490566
            }
        ],
        "cosine_similarity": 0.9298802614212036,
        "meteor": 0.07745933384972889
    },
    "bert-large-uncased-whole-word-masking": {
        "code_bleu": [
            {
                "codebleu": 0.27097525138861617
            },
            {
                "ngram_match_score": 0.002348924419623707
            },
            {
                "weighted_ngram_match_score": 0.006536387475079385
            },
            {
                "syntax_match_score": 0.6851851851851852
            },
            {
                "dataflow_match_score": 0.3898305084745763
            }
        ],
        "cosine_similarity": 0.9796826243400574,
        "meteor": 0.034530386740331494
    },
    "OpenAssistant/reward-model-deberta-v3-large-v2": {
        "code_bleu": [
            {
                "codebleu": 0.1606633847108024
            },
            {
                "ngram_match_score": 0.004639565187994983
            },
            {
                "weighted_ngram_match_score": 0.004899938567495381
            },
            {
                "syntax_match_score": 0.24561403508771928
            },
            {
                "dataflow_match_score": 0.3875
            }
        ],
        "cosine_similarity": 0.9521746039390564,
        "meteor": 0.052666227781435156
    },
    "bigcode/starcoder2-15b": {
        "code_bleu": [
            {
                "codebleu": 0.3464789982679039
            },
            {
                "ngram_match_score": 0.0026706589443602985
            },
            {
                "weighted_ngram_match_score": 0.0029636439864102553
            },
            {
                "syntax_match_score": 0.38028169014084506
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9686323404312134,
        "meteor": 0.06463878326996197
    },
    "openlm-research/open/llama/7b": {
        "code_bleu": [
            {
                "codebleu": 0.22489238007820442
            },
            {
                "ngram_match_score": 0.01764975002280802
            },
            {
                "weighted_ngram_match_score": 0.041103443759397465
            },
            {
                "syntax_match_score": 0.32653061224489793
            },
            {
                "dataflow_match_score": 0.5142857142857142
            }
        ],
        "cosine_similarity": 0.9642651677131653,
        "meteor": 0.23251874279123413
    },
    "Qwen/Qwen1.5-72B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.19865182854558547
            },
            {
                "ngram_match_score": 0.00855658538057247
            },
            {
                "weighted_ngram_match_score": 0.00944748708224865
            },
            {
                "syntax_match_score": 0.3372093023255814
            },
            {
                "dataflow_match_score": 0.4393939393939394
            }
        ],
        "cosine_similarity": 0.9738301038742065,
        "meteor": 0.10436368373062394
    },
    "GroNLP/bert-base-dutch-cased": {
        "code_bleu": [
            {
                "codebleu": 0.21630636716418872
            },
            {
                "ngram_match_score": 0.0020364128919134737
            },
            {
                "weighted_ngram_match_score": 0.013189055764841435
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.6
            }
        ],
        "cosine_similarity": 0.9241012930870056,
        "meteor": 0.06756756756756757
    },
    "kandinsky-community/kandinsky-2-2-prior": {
        "code_bleu": [
            {
                "codebleu": 0.12288208370310003
            },
            {
                "ngram_match_score": 5.493065088643057e-05
            },
            {
                "weighted_ngram_match_score": 0.0029387941365408873
            },
            {
                "syntax_match_score": 0.34390651085141904
            },
            {
                "dataflow_match_score": 0.1446280991735537
            }
        ],
        "cosine_similarity": 0.9512709975242615,
        "meteor": 0.02707581227436823
    },
    "facebook/convnext-base-224-22k-1k": {
        "code_bleu": [
            {
                "codebleu": 0.17853591622669507
            },
            {
                "ngram_match_score": 0.0044935253117907295
            },
            {
                "weighted_ngram_match_score": 0.014896562488471246
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.4594594594594595
            }
        ],
        "cosine_similarity": 0.9607112407684326,
        "meteor": 0.1633435270132518
    },
    "deepset/minilm-uncased-squad2": {
        "code_bleu": [
            {
                "codebleu": 0.0330188679245283
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.1320754716981132
            }
        ],
        "cosine_similarity": 0.9653741121292114,
        "meteor": 0.008503401360544217
    },
    "Qwen/Qwen1.5-14B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.23169702837283368
            },
            {
                "ngram_match_score": 0.020134127533816396
            },
            {
                "weighted_ngram_match_score": 0.03244679779684167
            },
            {
                "syntax_match_score": 0.46511627906976744
            },
            {
                "dataflow_match_score": 0.4090909090909091
            }
        ],
        "cosine_similarity": 0.9775958061218262,
        "meteor": 0.20150923497890774
    },
    "allenai/OLMo-1.7-7B-hf": {
        "code_bleu": [
            {
                "codebleu": 0.1487871434007159
            },
            {
                "ngram_match_score": 0.003291854088347366
            },
            {
                "weighted_ngram_match_score": 0.0038776254378610933
            },
            {
                "syntax_match_score": 0.1951219512195122
            },
            {
                "dataflow_match_score": 0.39285714285714285
            }
        ],
        "cosine_similarity": 0.9583984613418579,
        "meteor": 0.07085020242914979
    },
    "Salesforce/codet5-base-multi-sum": {
        "code_bleu": [
            {
                "codebleu": 0.1952150992907336
            },
            {
                "ngram_match_score": 0.004413285863940807
            },
            {
                "weighted_ngram_match_score": 0.009780444632326895
            },
            {
                "syntax_match_score": 0.26666666666666666
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9896329641342163,
        "meteor": 0.10572139303482588
    },
    "stabilityai/stablelm-3b-4e1t": {
        "code_bleu": [
            {
                "codebleu": 0.09812783950184652
            },
            {
                "ngram_match_score": 0.009997720050135661
            },
            {
                "weighted_ngram_match_score": 0.01944743586665812
            },
            {
                "syntax_match_score": 0.3142857142857143
            },
            {
                "dataflow_match_score": 0.04878048780487805
            }
        ],
        "cosine_similarity": 0.9827437996864319,
        "meteor": 0.12131365699484327
    },
    "Qwen/Qwen-7B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.3848359078077272
            },
            {
                "ngram_match_score": 0.35994315258199405
            },
            {
                "weighted_ngram_match_score": 0.3673315131316734
            },
            {
                "syntax_match_score": 0.45
            },
            {
                "dataflow_match_score": 0.3620689655172414
            }
        ],
        "cosine_similarity": 0.9822636842727661,
        "meteor": 0.43999166603761175
    },
    "nvidia/mit-b5": {
        "code_bleu": [
            {
                "codebleu": 0.14541328378878887
            },
            {
                "ngram_match_score": 0.01871787376436019
            },
            {
                "weighted_ngram_match_score": 0.11704781550334942
            },
            {
                "syntax_match_score": 0.13636363636363635
            },
            {
                "dataflow_match_score": 0.30952380952380953
            }
        ],
        "cosine_similarity": 0.9489787817001343,
        "meteor": 0.07835511782032402
    },
    "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens": {
        "code_bleu": [
            {
                "codebleu": 0.2017754754845632
            },
            {
                "ngram_match_score": 0.012213530326357856
            },
            {
                "weighted_ngram_match_score": 0.012926673889327733
            },
            {
                "syntax_match_score": 0.41964285714285715
            },
            {
                "dataflow_match_score": 0.36231884057971014
            }
        ],
        "cosine_similarity": 0.984796941280365,
        "meteor": 0.09349298758165754
    },
    "lordtt13/emo-mobilebert": {
        "code_bleu": [
            {
                "codebleu": 0.15060256942513484
            },
            {
                "ngram_match_score": 0.00362623784035086
            },
            {
                "weighted_ngram_match_score": 0.008307849383998017
            },
            {
                "syntax_match_score": 0.4
            },
            {
                "dataflow_match_score": 0.19047619047619047
            }
        ],
        "cosine_similarity": 0.9602832794189453,
        "meteor": 0.02873563218390805
    },
    "TheBloke/Llama-2-13B-chat-GPTQ": {
        "code_bleu": [
            {
                "codebleu": 0.12310361193488384
            },
            {
                "ngram_match_score": 0.00186997007453765
            },
            {
                "weighted_ngram_match_score": 0.001955889076409111
            },
            {
                "syntax_match_score": 0.17777777777777778
            },
            {
                "dataflow_match_score": 0.3108108108108108
            }
        ],
        "cosine_similarity": 0.9763132333755493,
        "meteor": 0.041296060991105464
    },
    "kakaobrain/align-base": {
        "code_bleu": [
            {
                "codebleu": 0.11268500900194614
            },
            {
                "ngram_match_score": 0.003791162307401149
            },
            {
                "weighted_ngram_match_score": 0.004281727762209475
            },
            {
                "syntax_match_score": 0.1542056074766355
            },
            {
                "dataflow_match_score": 0.28846153846153844
            }
        ],
        "cosine_similarity": 0.9674603939056396,
        "meteor": 0.07119277814133061
    },
    "Babelscape/rebel-large": {
        "code_bleu": [
            {
                "codebleu": 0.2473945471425179
            },
            {
                "ngram_match_score": 0.0325384370750406
            },
            {
                "weighted_ngram_match_score": 0.1108238580922048
            },
            {
                "syntax_match_score": 0.7044534412955465
            },
            {
                "dataflow_match_score": 0.1417624521072797
            }
        ],
        "cosine_similarity": 0.9351580739021301,
        "meteor": 0.16842389817711004
    },
    "hkunlp/instructor-xl": {
        "code_bleu": [
            {
                "codebleu": 0.15299744284986833
            },
            {
                "ngram_match_score": 0.00023244259532494312
            },
            {
                "weighted_ngram_match_score": 0.0016273312569322373
            },
            {
                "syntax_match_score": 0.2582781456953642
            },
            {
                "dataflow_match_score": 0.35185185185185186
            }
        ],
        "cosine_similarity": 0.9748350977897644,
        "meteor": 0.02616555661274976
    },
    "EleutherAI/pythia-2.8b": {
        "code_bleu": [
            {
                "codebleu": 0.24064212256995388
            },
            {
                "ngram_match_score": 0.004874224382711144
            },
            {
                "weighted_ngram_match_score": 0.023452084662140504
            },
            {
                "syntax_match_score": 0.27906976744186046
            },
            {
                "dataflow_match_score": 0.6551724137931034
            }
        ],
        "cosine_similarity": 0.9368159770965576,
        "meteor": 0.10309278350515463
    },
    "intfloat/e5-small": {
        "code_bleu": [
            {
                "codebleu": 0.14165919804374436
            },
            {
                "ngram_match_score": 0.00040333581411277914
            },
            {
                "weighted_ngram_match_score": 0.00117969292000442
            },
            {
                "syntax_match_score": 0.29838709677419356
            },
            {
                "dataflow_match_score": 0.26666666666666666
            }
        ],
        "cosine_similarity": 0.9749543070793152,
        "meteor": 0.01889763779527559
    },
    "Lykon/dreamshaper-7": {
        "code_bleu": [
            {
                "codebleu": 0.08814102564102563
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.019230769230769232
            },
            {
                "dataflow_match_score": 0.3333333333333333
            }
        ],
        "cosine_similarity": 0.982525110244751,
        "meteor": 0.0
    },
    "deepseek-ai/deepseek-coder-6.7b-base": {
        "code_bleu": [
            {
                "codebleu": 0.16713979882645066
            },
            {
                "ngram_match_score": 0.002815246739369191
            },
            {
                "weighted_ngram_match_score": 0.0069269293829608
            },
            {
                "syntax_match_score": 0.3223684210526316
            },
            {
                "dataflow_match_score": 0.3364485981308411
            }
        ],
        "cosine_similarity": 0.9768367409706116,
        "meteor": 0.04938692098092643
    },
    "beomi/kcbert-base": {
        "code_bleu": [
            {
                "codebleu": 0.15820884892779533
            },
            {
                "ngram_match_score": 0.0028312232710596256
            },
            {
                "weighted_ngram_match_score": 0.005004172440121658
            },
            {
                "syntax_match_score": 0.28125
            },
            {
                "dataflow_match_score": 0.34375
            }
        ],
        "cosine_similarity": 0.9601514339447021,
        "meteor": 0.08771929824561404
    },
    "microsoft/BiomedVLP-CXR-BERT-specialized": {
        "code_bleu": [
            {
                "codebleu": 0.05851063829787234
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.23404255319148937
            }
        ],
        "cosine_similarity": 0.9434882998466492,
        "meteor": 0.0
    },
    "bigcode/tiny/starcoder/py": {
        "code_bleu": [
            {
                "codebleu": 0.37790020976978156
            },
            {
                "ngram_match_score": 0.11639908945333934
            },
            {
                "weighted_ngram_match_score": 0.22287298250249932
            },
            {
                "syntax_match_score": 0.7123287671232876
            },
            {
                "dataflow_match_score": 0.46
            }
        ],
        "cosine_similarity": 0.9749991297721863,
        "meteor": 0.2833456607495069
    },
    "Salesforce/instructblip-flan-t5-xl": {
        "code_bleu": [
            {
                "codebleu": 0.3335559746603669
            },
            {
                "ngram_match_score": 0.0030555190188022445
            },
            {
                "weighted_ngram_match_score": 0.004852590148981055
            },
            {
                "syntax_match_score": 0.3263157894736842
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9864517450332642,
        "meteor": 0.07246376811594203
    },
    "google/vit-large-patch16-224": {
        "code_bleu": [
            {
                "codebleu": 0.12058429270112768
            },
            {
                "ngram_match_score": 0.005093190413099485
            },
            {
                "weighted_ngram_match_score": 0.02919203233946316
            },
            {
                "syntax_match_score": 0.09090909090909091
            },
            {
                "dataflow_match_score": 0.35714285714285715
            }
        ],
        "cosine_similarity": 0.9464524388313293,
        "meteor": 0.0462962962962963
    },
    "hakurei/waifu-diffusion": {
        "code_bleu": [
            {
                "codebleu": 0.17803030303030304
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.29545454545454547
            },
            {
                "dataflow_match_score": 0.4166666666666667
            }
        ],
        "cosine_similarity": 0.9644125699996948,
        "meteor": 0.008944543828264758
    },
    "openlm-research/open/llama/3b": {
        "code_bleu": [
            {
                "codebleu": 0.11223663161098021
            },
            {
                "ngram_match_score": 0.005270421622728221
            },
            {
                "weighted_ngram_match_score": 0.011023043596702859
            },
            {
                "syntax_match_score": 0.20408163265306123
            },
            {
                "dataflow_match_score": 0.22857142857142856
            }
        ],
        "cosine_similarity": 0.9662222266197205,
        "meteor": 0.024752475247524754
    },
    "Qwen/Qwen1.5-1.8B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.1992702542678066
            },
            {
                "ngram_match_score": 0.008609359024370232
            },
            {
                "weighted_ngram_match_score": 0.008344808152564288
            },
            {
                "syntax_match_score": 0.32558139534883723
            },
            {
                "dataflow_match_score": 0.45454545454545453
            }
        ],
        "cosine_similarity": 0.9819024801254272,
        "meteor": 0.08225616921269095
    },
    "Open-Orca/Mistral-7B-OpenOrca": {
        "code_bleu": [
            {
                "codebleu": 0.25879171549651864
            },
            {
                "ngram_match_score": 0.0022027035575704645
            },
            {
                "weighted_ngram_match_score": 0.002194927659273304
            },
            {
                "syntax_match_score": 0.03076923076923077
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9567468166351318,
        "meteor": 0.027450980392156866
    },
    "albert-xxlarge-v2": {
        "code_bleu": [
            {
                "codebleu": 0.16928268732925333
            },
            {
                "ngram_match_score": 0.0006007293604436271
            },
            {
                "weighted_ngram_match_score": 0.001200453101579081
            },
            {
                "syntax_match_score": 0.4888888888888889
            },
            {
                "dataflow_match_score": 0.1864406779661017
            }
        ],
        "cosine_similarity": 0.9852809906005859,
        "meteor": 0.021334367726920093
    },
    "facebook/xglm-564M": {
        "code_bleu": [
            {
                "codebleu": 0.15877312853963815
            },
            {
                "ngram_match_score": 0.0023812648399184067
            },
            {
                "weighted_ngram_match_score": 0.0051817977657342
            },
            {
                "syntax_match_score": 0.39823008849557523
            },
            {
                "dataflow_match_score": 0.22929936305732485
            }
        ],
        "cosine_similarity": 0.9490507245063782,
        "meteor": 0.03772906934962271
    },
    "facebook/detr-resnet-50-panoptic": {
        "code_bleu": [
            {
                "codebleu": 0.014966745552899027
            },
            {
                "ngram_match_score": 0.00021542927158074157
            },
            {
                "weighted_ngram_match_score": 0.002508695797158223
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.05714285714285714
            }
        ],
        "cosine_similarity": 0.9483931064605713,
        "meteor": 0.005656108597285067
    },
    "SenseTime/deformable-detr": {
        "code_bleu": [
            {
                "codebleu": 0.1982961355635846
            },
            {
                "ngram_match_score": 0.0049205954192172476
            },
            {
                "weighted_ngram_match_score": 0.00789011505942027
            },
            {
                "syntax_match_score": 0.2803738317757009
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9318967461585999,
        "meteor": 0.09646302250803858
    },
    "deepseek-ai/deepseek-coder-1.3b-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.22026968474729058
            },
            {
                "ngram_match_score": 0.0031783691666762474
            },
            {
                "weighted_ngram_match_score": 0.007269175247629657
            },
            {
                "syntax_match_score": 0.352112676056338
            },
            {
                "dataflow_match_score": 0.5185185185185185
            }
        ],
        "cosine_similarity": 0.9507943987846375,
        "meteor": 0.08928571428571427
    },
    "EleutherAI/pythia-6.9b": {
        "code_bleu": [
            {
                "codebleu": 0.11619925265912039
            },
            {
                "ngram_match_score": 0.004701712638603976
            },
            {
                "weighted_ngram_match_score": 0.00861173745256884
            },
            {
                "syntax_match_score": 0.27906976744186046
            },
            {
                "dataflow_match_score": 0.1724137931034483
            }
        ],
        "cosine_similarity": 0.9559823274612427,
        "meteor": 0.06329113924050632
    },
    "togethercomputer/LLaMA-2-7B-32K": {
        "code_bleu": [
            {
                "codebleu": 0.22609655699536024
            },
            {
                "ngram_match_score": 0.004315621605804481
            },
            {
                "weighted_ngram_match_score": 0.008635800047213175
            },
            {
                "syntax_match_score": 0.40425531914893614
            },
            {
                "dataflow_match_score": 0.48717948717948717
            }
        ],
        "cosine_similarity": 0.9596553444862366,
        "meteor": 0.09887005649717515
    },
    "baichuan-inc/Baichuan2-7B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.19718080559049833
            },
            {
                "ngram_match_score": 0.0020377360500218643
            },
            {
                "weighted_ngram_match_score": 0.006396722972360764
            },
            {
                "syntax_match_score": 0.3728813559322034
            },
            {
                "dataflow_match_score": 0.4074074074074074
            }
        ],
        "cosine_similarity": 0.9708507657051086,
        "meteor": 0.05175983436853002
    },
    "facebook/deit-base-distilled-patch16-224": {
        "code_bleu": [
            {
                "codebleu": 0.17529430047596994
            },
            {
                "ngram_match_score": 0.0051432478421602965
            },
            {
                "weighted_ngram_match_score": 0.027202785230550666
            },
            {
                "syntax_match_score": 0.12121212121212122
            },
            {
                "dataflow_match_score": 0.5476190476190477
            }
        ],
        "cosine_similarity": 0.9571927785873413,
        "meteor": 0.04457652303120357
    },
    "tiiuae/falcon-rw-1b": {
        "code_bleu": [
            {
                "codebleu": 0.20693448746551046
            },
            {
                "ngram_match_score": 0.002516364277621553
            },
            {
                "weighted_ngram_match_score": 0.002728360652171015
            },
            {
                "syntax_match_score": 0.4444444444444444
            },
            {
                "dataflow_match_score": 0.3780487804878049
            }
        ],
        "cosine_similarity": 0.9592735767364502,
        "meteor": 0.07286606523247745
    },
    "codellama/CodeLlama-34b-hf": {
        "code_bleu": [
            {
                "codebleu": 0.23138845046077788
            },
            {
                "ngram_match_score": 0.0021024982854913206
            },
            {
                "weighted_ngram_match_score": 0.00821320831952497
            },
            {
                "syntax_match_score": 0.32
            },
            {
                "dataflow_match_score": 0.5952380952380952
            }
        ],
        "cosine_similarity": 0.9744329452514648,
        "meteor": 0.08411214953271029
    },
    "microsoft/swin-base-patch4-window7-224": {
        "code_bleu": [
            {
                "codebleu": 0.36149186169129577
            },
            {
                "ngram_match_score": 0.124348621474803
            },
            {
                "weighted_ngram_match_score": 0.3281123317838864
            },
            {
                "syntax_match_score": 0.4696969696969697
            },
            {
                "dataflow_match_score": 0.5238095238095238
            }
        ],
        "cosine_similarity": 0.9645538330078125,
        "meteor": 0.39723661485319517
    },
    "baichuan-inc/Baichuan-7B": {
        "code_bleu": [
            {
                "codebleu": 0.21624893919873556
            },
            {
                "ngram_match_score": 0.009352699941579197
            },
            {
                "weighted_ngram_match_score": 0.01440896159382017
            },
            {
                "syntax_match_score": 0.29906542056074764
            },
            {
                "dataflow_match_score": 0.5421686746987951
            }
        ],
        "cosine_similarity": 0.9742242693901062,
        "meteor": 0.11363636363636365
    },
    "stabilityai/stablelm-zephyr-3b": {
        "code_bleu": [
            {
                "codebleu": 0.3804647163700233
            },
            {
                "ngram_match_score": 0.01157194240761736
            },
            {
                "weighted_ngram_match_score": 0.03660271254616017
            },
            {
                "syntax_match_score": 0.47368421052631576
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9674742221832275,
        "meteor": 0.1348747591522158
    },
    "google/electra-small-generator": {
        "code_bleu": [
            {
                "codebleu": 0.24114784207654458
            },
            {
                "ngram_match_score": 0.005192512588207006
            },
            {
                "weighted_ngram_match_score": 0.04525744157655719
            },
            {
                "syntax_match_score": 0.13636363636363635
            },
            {
                "dataflow_match_score": 0.7777777777777778
            }
        ],
        "cosine_similarity": 0.912105917930603,
        "meteor": 0.037037037037037035
    },
    "rinna/japanese-gpt-neox-3.6b": {
        "code_bleu": [
            {
                "codebleu": 0.21975308641975308
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.345679012345679
            },
            {
                "dataflow_match_score": 0.5333333333333333
            }
        ],
        "cosine_similarity": 0.9468291997909546,
        "meteor": 0.0
    },
    "KB/bert-base-swedish-cased": {
        "code_bleu": [
            {
                "codebleu": 0.16685399846020066
            },
            {
                "ngram_match_score": 0.004687458508501782
            },
            {
                "weighted_ngram_match_score": 0.016478535332300856
            },
            {
                "syntax_match_score": 0.24
            },
            {
                "dataflow_match_score": 0.40625
            }
        ],
        "cosine_similarity": 0.9619957804679871,
        "meteor": 0.07990867579908675
    },
    "flaubert/flaubert/base/cased": {
        "code_bleu": [
            {
                "codebleu": 0.2037427592292011
            },
            {
                "ngram_match_score": 0.0027138499890625137
            },
            {
                "weighted_ngram_match_score": 0.0037337775639964188
            },
            {
                "syntax_match_score": 0.3673469387755102
            },
            {
                "dataflow_match_score": 0.4411764705882353
            }
        ],
        "cosine_similarity": 0.9574998617172241,
        "meteor": 0.04772004241781548
    },
    "CohereForAI/c4ai-command-r-v01-4bit": {
        "code_bleu": [
            {
                "codebleu": 0.08795685653453794
            },
            {
                "ngram_match_score": 5.033128754321686e-13
            },
            {
                "weighted_ngram_match_score": 0.002619562041446871
            },
            {
                "syntax_match_score": 0.3225806451612903
            },
            {
                "dataflow_match_score": 0.026627218934911243
            }
        ],
        "cosine_similarity": 0.9695971012115479,
        "meteor": 0.013823252444766386
    },
    "facebook/deit-base-patch16-224": {
        "code_bleu": [
            {
                "codebleu": 0.07251082251082251
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.07575757575757576
            },
            {
                "dataflow_match_score": 0.21428571428571427
            }
        ],
        "cosine_similarity": 0.9624702334403992,
        "meteor": 0.0
    },
    "Qwen/Qwen1.5-4B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.031016678570799528
            },
            {
                "ngram_match_score": 0.0008146086592813247
            },
            {
                "weighted_ngram_match_score": 0.002039984411795567
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.12121212121212122
            }
        ],
        "cosine_similarity": 0.9434391260147095,
        "meteor": 0.006313131313131313
    },
    "microsoft/kosmos-2-patch14-224": {
        "code_bleu": [
            {
                "codebleu": 0.05299164488693007
            },
            {
                "ngram_match_score": 0.0005112086524252125
            },
            {
                "weighted_ngram_match_score": 0.013833645755281936
            },
            {
                "syntax_match_score": 0.15950920245398773
            },
            {
                "dataflow_match_score": 0.038112522686025406
            }
        ],
        "cosine_similarity": 0.9657346606254578,
        "meteor": 0.021699566008679828
    },
    "bigscience/bloomz-7b1": {
        "code_bleu": [
            {
                "codebleu": 0.08648311019623743
            },
            {
                "ngram_match_score": 0.0030529807855792165
            },
            {
                "weighted_ngram_match_score": 0.003421892195346014
            },
            {
                "syntax_match_score": 0.2283464566929134
            },
            {
                "dataflow_match_score": 0.1111111111111111
            }
        ],
        "cosine_similarity": 0.9647468328475952,
        "meteor": 0.07858546168958742
    },
    "Salesforce/codet5-base": {
        "code_bleu": [
            {
                "codebleu": 0.29445526048712267
            },
            {
                "ngram_match_score": 0.007067093716617054
            },
            {
                "weighted_ngram_match_score": 0.018783966993599677
            },
            {
                "syntax_match_score": 0.5365853658536586
            },
            {
                "dataflow_match_score": 0.6153846153846154
            }
        ],
        "cosine_similarity": 0.9332690834999084,
        "meteor": 0.1254790683962264
    },
    "intfloat/e5-base": {
        "code_bleu": [
            {
                "codebleu": 0.12695576976036968
            },
            {
                "ngram_match_score": 0.0008957040381744562
            },
            {
                "weighted_ngram_match_score": 0.001371819447748722
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.25555555555555554
            }
        ],
        "cosine_similarity": 0.9825426340103149,
        "meteor": 0.0558139534883721
    },
    "microsoft/swinv2-tiny-patch4-window8-256": {
        "code_bleu": [
            {
                "codebleu": 0.1611050860770869
            },
            {
                "ngram_match_score": 0.0021024982854913206
            },
            {
                "weighted_ngram_match_score": 0.005954209659219934
            },
            {
                "syntax_match_score": 0.13636363636363635
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9593819379806519,
        "meteor": 0.11035653650254669
    },
    "csebuetnlp/mT5/multilingual/XLSum": {
        "code_bleu": [
            {
                "codebleu": 0.2547809115629756
            },
            {
                "ngram_match_score": 0.0027486838656864106
            },
            {
                "weighted_ngram_match_score": 0.0047269899962245725
            },
            {
                "syntax_match_score": 0.5526315789473685
            },
            {
                "dataflow_match_score": 0.45901639344262296
            }
        ],
        "cosine_similarity": 0.9209376573562622,
        "meteor": 0.05030621172353456
    },
    "facebook/incoder-6B": {
        "code_bleu": [
            {
                "codebleu": 0.275
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.1
            },
            {
                "dataflow_match_score": 1.0
            }
        ],
        "cosine_similarity": 0.9031415581703186,
        "meteor": 0.0
    },
    "hkunlp/instructor-base": {
        "code_bleu": [
            {
                "codebleu": 0.15489256211364283
            },
            {
                "ngram_match_score": 0.0006022902462297672
            },
            {
                "weighted_ngram_match_score": 0.0020314853116037276
            },
            {
                "syntax_match_score": 0.40397350993377484
            },
            {
                "dataflow_match_score": 0.21296296296296297
            }
        ],
        "cosine_similarity": 0.9694812297821045,
        "meteor": 0.03653936822253654
    },
    "Qwen/Qwen-VL": {
        "code_bleu": [
            {
                "codebleu": 0.35178322788204863
            },
            {
                "ngram_match_score": 0.0054310696892003974
            },
            {
                "weighted_ngram_match_score": 0.005868508505660913
            },
            {
                "syntax_match_score": 0.3958333333333333
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9815146923065186,
        "meteor": 0.08942457231726283
    },
    "TheBloke/Llama-2-13B-chat-GGUF": {
        "code_bleu": [
            {
                "codebleu": 0.23088484529813064
            },
            {
                "ngram_match_score": 0.0024874082113819154
            },
            {
                "weighted_ngram_match_score": 0.004385306314473909
            },
            {
                "syntax_match_score": 0.3333333333333333
            },
            {
                "dataflow_match_score": 0.5833333333333334
            }
        ],
        "cosine_similarity": 0.9288565516471863,
        "meteor": 0.012048192771084338
    },
    "togethercomputer/Llama-2-7B-32K-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.0643903264923864
            },
            {
                "ngram_match_score": 0.00021432440605536588
            },
            {
                "weighted_ngram_match_score": 0.0015028257193343698
            },
            {
                "syntax_match_score": 0.22857142857142856
            },
            {
                "dataflow_match_score": 0.02727272727272727
            }
        ],
        "cosine_similarity": 0.9608502388000488,
        "meteor": 0.02661934338952972
    },
    "aubmindlab/bert-base-arabertv02": {
        "code_bleu": [
            {
                "codebleu": 0.2283345695112713
            },
            {
                "ngram_match_score": 0.0014945643417360278
            },
            {
                "weighted_ngram_match_score": 0.005250307109942533
            },
            {
                "syntax_match_score": 0.21428571428571427
            },
            {
                "dataflow_match_score": 0.6923076923076923
            }
        ],
        "cosine_similarity": 0.9261535406112671,
        "meteor": 0.019880715705765408
    },
    "Salesforce/ctrl": {
        "code_bleu": [
            {
                "codebleu": 0.16158974904277812
            },
            {
                "ngram_match_score": 0.0038609584341964096
            },
            {
                "weighted_ngram_match_score": 0.0054923880194018984
            },
            {
                "syntax_match_score": 0.22033898305084745
            },
            {
                "dataflow_match_score": 0.4166666666666667
            }
        ],
        "cosine_similarity": 0.8888291716575623,
        "meteor": 0.03861003861003861
    },
    "EleutherAI/pythia-1.4b-deduped": {
        "code_bleu": [
            {
                "codebleu": 0.1616082760411785
            },
            {
                "ngram_match_score": 0.003004926757200381
            },
            {
                "weighted_ngram_match_score": 0.009105803710641137
            },
            {
                "syntax_match_score": 0.18604651162790697
            },
            {
                "dataflow_match_score": 0.4482758620689655
            }
        ],
        "cosine_similarity": 0.9534566402435303,
        "meteor": 0.07122507122507123
    },
    "openbmb/cpm-ant-10b": {
        "code_bleu": [
            {
                "codebleu": 0.3802325431217881
            },
            {
                "ngram_match_score": 0.024134368115305085
            },
            {
                "weighted_ngram_match_score": 0.10833426591030883
            },
            {
                "syntax_match_score": 0.85
            },
            {
                "dataflow_match_score": 0.5384615384615384
            }
        ],
        "cosine_similarity": 0.9402868747711182,
        "meteor": 0.34024390243902436
    },
    "openchat/openchat/3.5": {
        "code_bleu": [
            {
                "codebleu": 0.11985180326359257
            },
            {
                "ngram_match_score": 0.0015551193392810197
            },
            {
                "weighted_ngram_match_score": 0.001661617524613096
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.47619047619047616
            }
        ],
        "cosine_similarity": 0.9574427008628845,
        "meteor": 0.012427506213753105
    },
    "facebook/maskformer-swin-base-coco": {
        "code_bleu": [
            {
                "codebleu": 0.1459060791526959
            },
            {
                "ngram_match_score": 0.003671435047745441
            },
            {
                "weighted_ngram_match_score": 0.004010852577530914
            },
            {
                "syntax_match_score": 0.11594202898550725
            },
            {
                "dataflow_match_score": 0.46
            }
        ],
        "cosine_similarity": 0.961047351360321,
        "meteor": 0.09249183895538629
    },
    "diffusers/controlnet-depth-sdxl-1.0-small": {
        "code_bleu": [
            {
                "codebleu": 0.006557189806623937
            },
            {
                "ngram_match_score": 0.0009433479585830543
            },
            {
                "weighted_ngram_match_score": 0.001755999503206811
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.023529411764705882
            }
        ],
        "cosine_similarity": 0.9534287452697754,
        "meteor": 0.007204610951008645
    },
    "intfloat/e5-large": {
        "code_bleu": [
            {
                "codebleu": 0.2166259480578812
            },
            {
                "ngram_match_score": 0.0012307174200414295
            },
            {
                "weighted_ngram_match_score": 0.002369849005031842
            },
            {
                "syntax_match_score": 0.3629032258064516
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9130104184150696,
        "meteor": 0.051610885204879574
    },
    "google/bigbird-pegasus-large-arxiv": {
        "code_bleu": [
            {
                "codebleu": 0.25753951775105244
            },
            {
                "ngram_match_score": 0.004881230634811334
            },
            {
                "weighted_ngram_match_score": 0.008344087587589024
            },
            {
                "syntax_match_score": 0.4528301886792453
            },
            {
                "dataflow_match_score": 0.5641025641025641
            }
        ],
        "cosine_similarity": 0.9784749746322632,
        "meteor": 0.10600706713780919
    },
    "kandinsky-community/kandinsky-2-1": {
        "code_bleu": [
            {
                "codebleu": 0.08674099273054953
            },
            {
                "ngram_match_score": 0.011127214356302404
            },
            {
                "weighted_ngram_match_score": 0.014765778296964452
            },
            {
                "syntax_match_score": 0.13402061855670103
            },
            {
                "dataflow_match_score": 0.18705035971223022
            }
        ],
        "cosine_similarity": 0.9761735200881958,
        "meteor": 0.06666666666666667
    },
    "h2oai/h2ogpt-oig-oasst1-512-6/9b": {
        "code_bleu": [
            {
                "codebleu": 0.24171367436617808
            },
            {
                "ngram_match_score": 0.019364311624662435
            },
            {
                "weighted_ngram_match_score": 0.05005448840415247
            },
            {
                "syntax_match_score": 0.3974358974358974
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9780330061912537,
        "meteor": 0.18353392639106927
    },
    "google/switch-base-128": {
        "code_bleu": [
            {
                "codebleu": 0.16471505958809957
            },
            {
                "ngram_match_score": 0.0177175879609205
            },
            {
                "weighted_ngram_match_score": 0.01890606344900852
            },
            {
                "syntax_match_score": 0.3516483516483517
            },
            {
                "dataflow_match_score": 0.27058823529411763
            }
        ],
        "cosine_similarity": 0.973293125629425,
        "meteor": 0.09244556631305283
    },
    "klue/roberta-large": {
        "code_bleu": [
            {
                "codebleu": 0.27666083460391533
            },
            {
                "ngram_match_score": 0.004526414192411749
            },
            {
                "weighted_ngram_match_score": 0.04917574775266129
            },
            {
                "syntax_match_score": 0.35294117647058826
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.918237566947937,
        "meteor": 0.09174311926605506
    },
    "bigscience/T0pp": {
        "code_bleu": [
            {
                "codebleu": 0.17159753900062336
            },
            {
                "ngram_match_score": 0.002324796558859722
            },
            {
                "weighted_ngram_match_score": 0.004704180082454316
            },
            {
                "syntax_match_score": 0.2702702702702703
            },
            {
                "dataflow_match_score": 0.4090909090909091
            }
        ],
        "cosine_similarity": 0.9232611656188965,
        "meteor": 0.01243781094527363
    },
    "facebook/opt-66b": {
        "code_bleu": [
            {
                "codebleu": 0.3073042622677338
            },
            {
                "ngram_match_score": 0.0005359049923423926
            },
            {
                "weighted_ngram_match_score": 0.0010949371820409822
            },
            {
                "syntax_match_score": 0.22758620689655173
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9657789468765259,
        "meteor": 0.028938906752411574
    },
    "EleutherAI/pythia-2.8b-deduped": {
        "code_bleu": [
            {
                "codebleu": 0.2712634230258235
            },
            {
                "ngram_match_score": 0.0021708234367587642
            },
            {
                "weighted_ngram_match_score": 0.009105803710641137
            },
            {
                "syntax_match_score": 0.4186046511627907
            },
            {
                "dataflow_match_score": 0.6551724137931034
            }
        ],
        "cosine_similarity": 0.9641745686531067,
        "meteor": 0.06493506493506494
    },
    "facebook/opt-30b": {
        "code_bleu": [
            {
                "codebleu": 0.0985390803581445
            },
            {
                "ngram_match_score": 0.0017166804302936513
            },
            {
                "weighted_ngram_match_score": 0.004846988730289499
            },
            {
                "syntax_match_score": 0.15862068965517243
            },
            {
                "dataflow_match_score": 0.22897196261682243
            }
        ],
        "cosine_similarity": 0.9817907214164734,
        "meteor": 0.03399158303658141
    },
    "mosaicml/mpt-30b": {
        "code_bleu": [
            {
                "codebleu": 0.41939389049371567
            },
            {
                "ngram_match_score": 0.09118197163781364
            },
            {
                "weighted_ngram_match_score": 0.10363496964739377
            },
            {
                "syntax_match_score": 0.4827586206896552
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9621874094009399,
        "meteor": 0.14245035970572817
    },
    "01-ai/Yi-34B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.05742306910616751
            },
            {
                "ngram_match_score": 0.000733253721415416
            },
            {
                "weighted_ngram_match_score": 0.0013651396215666682
            },
            {
                "syntax_match_score": 0.10060975609756098
            },
            {
                "dataflow_match_score": 0.12698412698412698
            }
        ],
        "cosine_similarity": 0.9722813367843628,
        "meteor": 0.05032467532467532
    },
    "01-ai/Yi-34B": {
        "code_bleu": [
            {
                "codebleu": 0.05742306910616751
            },
            {
                "ngram_match_score": 0.000733253721415416
            },
            {
                "weighted_ngram_match_score": 0.0013651396215666682
            },
            {
                "syntax_match_score": 0.10060975609756098
            },
            {
                "dataflow_match_score": 0.12698412698412698
            }
        ],
        "cosine_similarity": 0.9729201793670654,
        "meteor": 0.05032467532467532
    },
    "stabilityai/stable-diffusion-2-1-unclip": {
        "code_bleu": [
            {
                "codebleu": 0.2744194427158531
            },
            {
                "ngram_match_score": 0.014774184273501242
            },
            {
                "weighted_ngram_match_score": 0.046878741869414334
            },
            {
                "syntax_match_score": 0.5142857142857142
            },
            {
                "dataflow_match_score": 0.5217391304347826
            }
        ],
        "cosine_similarity": 0.9348809123039246,
        "meteor": 0.1760387811634349
    },
    "state-spaces/mamba-2.8b-hf": {
        "code_bleu": [
            {
                "codebleu": 0.4312377047437758
            },
            {
                "ngram_match_score": 0.10722008673443299
            },
            {
                "weighted_ngram_match_score": 0.09804569287059156
            },
            {
                "syntax_match_score": 0.5196850393700787
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9886011481285095,
        "meteor": 0.20253386454183264
    },
    "microsoft/phi-1": {
        "code_bleu": [
            {
                "codebleu": 0.22666970762597005
            },
            {
                "ngram_match_score": 0.004942286852694065
            },
            {
                "weighted_ngram_match_score": 0.010263675434131845
            },
            {
                "syntax_match_score": 0.3333333333333333
            },
            {
                "dataflow_match_score": 0.5581395348837209
            }
        ],
        "cosine_similarity": 0.9764501452445984,
        "meteor": 0.11422324031653476
    },
    "flaubert/flaubert/small/cased": {
        "code_bleu": [
            {
                "codebleu": 0.045040044730455885
            },
            {
                "ngram_match_score": 0.0009399324811396073
            },
            {
                "weighted_ngram_match_score": 0.002749658205389795
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.17647058823529413
            }
        ],
        "cosine_similarity": 0.9535835981369019,
        "meteor": 0.029239766081871347
    },
    "Qwen/Qwen-1/8B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.16316427739776482
            },
            {
                "ngram_match_score": 0.001794335869687135
            },
            {
                "weighted_ngram_match_score": 0.002907681500040944
            },
            {
                "syntax_match_score": 0.37209302325581395
            },
            {
                "dataflow_match_score": 0.27586206896551724
            }
        ],
        "cosine_similarity": 0.968103289604187,
        "meteor": 0.029106029106029104
    },
    "Phind/Phind-CodeLlama-34B-v2": {
        "code_bleu": [
            {
                "codebleu": 0.05404933520075994
            },
            {
                "ngram_match_score": 0.00236875207461064
            },
            {
                "weighted_ngram_match_score": 0.00294635818327724
            },
            {
                "syntax_match_score": 0.06481481481481481
            },
            {
                "dataflow_match_score": 0.14606741573033707
            }
        ],
        "cosine_similarity": 0.9837179183959961,
        "meteor": 0.03496503496503497
    },
    "Snowflake/snowflake-arctic-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.20227762632737212
            },
            {
                "ngram_match_score": 0.00349032311582652
            },
            {
                "weighted_ngram_match_score": 0.003172629746109535
            },
            {
                "syntax_match_score": 0.3409090909090909
            },
            {
                "dataflow_match_score": 0.46153846153846156
            }
        ],
        "cosine_similarity": 0.9629626870155334,
        "meteor": 0.03293807641633728
    },
    "alibaba-damo/mgp-str-base": {
        "code_bleu": [
            {
                "codebleu": 0.3201590972071129
            },
            {
                "ngram_match_score": 0.14040321900707187
            },
            {
                "weighted_ngram_match_score": 0.43746343025957724
            },
            {
                "syntax_match_score": 0.288135593220339
            },
            {
                "dataflow_match_score": 0.4146341463414634
            }
        ],
        "cosine_similarity": 0.9714803695678711,
        "meteor": 0.6590870596205961
    },
    "nghuyong/ernie-3.0-base-zh": {
        "code_bleu": [
            {
                "codebleu": 0.35744053108209495
            },
            {
                "ngram_match_score": 0.0022396950058490295
            },
            {
                "weighted_ngram_match_score": 0.02164007638135428
            },
            {
                "syntax_match_score": 0.7058823529411765
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.9306151866912842,
        "meteor": 0.07317073170731708
    },
    "mosaicml/mpt-7b-storywriter": {
        "code_bleu": [
            {
                "codebleu": 0.4596107112078581
            },
            {
                "ngram_match_score": 0.14431989331184522
            },
            {
                "weighted_ngram_match_score": 0.1528385478498625
            },
            {
                "syntax_match_score": 0.5412844036697247
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9678501486778259,
        "meteor": 0.1868897442842174
    },
    "EleutherAI/pythia-6.9b-deduped": {
        "code_bleu": [
            {
                "codebleu": 0.2190556908964304
            },
            {
                "ngram_match_score": 0.0018402958814958586
            },
            {
                "weighted_ngram_match_score": 0.009105803710641137
            },
            {
                "syntax_match_score": 0.27906976744186046
            },
            {
                "dataflow_match_score": 0.5862068965517241
            }
        ],
        "cosine_similarity": 0.9607717394828796,
        "meteor": 0.07371007371007371
    },
    "Intel/neural-chat-7b-v3-1": {
        "code_bleu": [
            {
                "codebleu": 0.3130449984771181
            },
            {
                "ngram_match_score": 0.00015902599296772763
            },
            {
                "weighted_ngram_match_score": 0.002020967915504652
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.980810284614563,
        "meteor": 0.0257034632034632
    },
    "togethercomputer/GPT-JT-6B-v1": {
        "code_bleu": [
            {
                "codebleu": 0.5195002523805112
            },
            {
                "ngram_match_score": 0.05565046275430797
            },
            {
                "weighted_ngram_match_score": 0.3223505467677371
            },
            {
                "syntax_match_score": 0.7
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9400761723518372,
        "meteor": 0.2860803452519429
    },
    "01-ai/Yi-6B": {
        "code_bleu": [
            {
                "codebleu": 0.052625375081863296
            },
            {
                "ngram_match_score": 0.000323074662452047
            },
            {
                "weighted_ngram_match_score": 0.001328896693520967
            },
            {
                "syntax_match_score": 0.11890243902439024
            },
            {
                "dataflow_match_score": 0.08994708994708994
            }
        ],
        "cosine_similarity": 0.9733551740646362,
        "meteor": 0.03789126853377265
    },
    "facebook/mbart-large-50-one-to-many-mmt": {
        "code_bleu": [
            {
                "codebleu": 0.19031980009257093
            },
            {
                "ngram_match_score": 0.005944679908713342
            },
            {
                "weighted_ngram_match_score": 0.009945587021313715
            },
            {
                "syntax_match_score": 0.39655172413793105
            },
            {
                "dataflow_match_score": 0.3488372093023256
            }
        ],
        "cosine_similarity": 0.968352735042572,
        "meteor": 0.08960573476702509
    },
    "flaubert/flaubert/base/uncased": {
        "code_bleu": [
            {
                "codebleu": 0.14095063166130606
            },
            {
                "ngram_match_score": 0.0019880364317163474
            },
            {
                "weighted_ngram_match_score": 0.002990960801743131
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.5588235294117647
            }
        ],
        "cosine_similarity": 0.9753612875938416,
        "meteor": 0.0358974358974359
    },
    "Qwen/Qwen-14B": {
        "code_bleu": [
            {
                "codebleu": 0.23851417944705766
            },
            {
                "ngram_match_score": 0.005459117204158294
            },
            {
                "weighted_ngram_match_score": 0.006673825629444361
            },
            {
                "syntax_match_score": 0.3103448275862069
            },
            {
                "dataflow_match_score": 0.631578947368421
            }
        ],
        "cosine_similarity": 0.9647560715675354,
        "meteor": 0.06915156721857753
    },
    "Salesforce/codet5p-220m": {
        "code_bleu": [
            {
                "codebleu": 0.23974639292854
            },
            {
                "ngram_match_score": 0.0076852993748728095
            },
            {
                "weighted_ngram_match_score": 0.021453333563776985
            },
            {
                "syntax_match_score": 0.3673469387755102
            },
            {
                "dataflow_match_score": 0.5625
            }
        ],
        "cosine_similarity": 0.9388954639434814,
        "meteor": 0.13265306122448983
    },
    "facebook/mask2former-swin-small-coco-instance": {
        "code_bleu": [
            {
                "codebleu": 0.15993879253207854
            },
            {
                "ngram_match_score": 0.0021734508760155626
            },
            {
                "weighted_ngram_match_score": 0.0029663346369139423
            },
            {
                "syntax_match_score": 0.11538461538461539
            },
            {
                "dataflow_match_score": 0.5192307692307693
            }
        ],
        "cosine_similarity": 0.8560893535614014,
        "meteor": 0.04859086491739554
    },
    "google/long-t5-tglobal-large": {
        "code_bleu": [
            {
                "codebleu": 0.19712439951644214
            },
            {
                "ngram_match_score": 0.00247404012277847
            },
            {
                "weighted_ngram_match_score": 0.012214034133466296
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.5238095238095238
            }
        ],
        "cosine_similarity": 0.9159044027328491,
        "meteor": 0.08849557522123892
    },
    "facebook/xmod-base": {
        "code_bleu": [
            {
                "codebleu": 0.3239643600153841
            },
            {
                "ngram_match_score": 0.0014145830547518839
            },
            {
                "weighted_ngram_match_score": 0.02777619034011792
            },
            {
                "syntax_match_score": 0.6
            },
            {
                "dataflow_match_score": 0.6666666666666666
            }
        ],
        "cosine_similarity": 0.9075725078582764,
        "meteor": 0.044843049327354265
    },
    "HuggingFaceM4/idefics-9b-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.07949821148694673
            },
            {
                "ngram_match_score": 4.851953052152539e-05
            },
            {
                "weighted_ngram_match_score": 0.0010785039334653735
            },
            {
                "syntax_match_score": 0.23220973782771537
            },
            {
                "dataflow_match_score": 0.08465608465608465
            }
        ],
        "cosine_similarity": 0.972660481929779,
        "meteor": 0.00901875901875902
    },
    "arpanghoshal/EmoRoBERTa": {
        "code_bleu": [
            {
                "codebleu": 0.5306749234635322
            },
            {
                "ngram_match_score": 0.09859769058939087
            },
            {
                "weighted_ngram_match_score": 0.5559201850829196
            },
            {
                "syntax_match_score": 0.8181818181818182
            },
            {
                "dataflow_match_score": 0.65
            }
        ],
        "cosine_similarity": 0.9560155868530273,
        "meteor": 0.4060724431818182
    },
    "dandelin/vilt-b32-mlm": {
        "code_bleu": [
            {
                "codebleu": 0.13573690617227638
            },
            {
                "ngram_match_score": 0.002516369551854135
            },
            {
                "weighted_ngram_match_score": 0.0027653011817046717
            },
            {
                "syntax_match_score": 0.2603550295857988
            },
            {
                "dataflow_match_score": 0.2773109243697479
            }
        ],
        "cosine_similarity": 0.9632698893547058,
        "meteor": 0.0691085003455425
    },
    "mosaicml/mpt-30b-instruct": {
        "code_bleu": [
            {
                "codebleu": 0.2508466025674321
            },
            {
                "ngram_match_score": 0.0016796057576493797
            },
            {
                "weighted_ngram_match_score": 0.0017068045120790128
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9619231224060059,
        "meteor": 0.02816901408450704
    },
    "bigscience/bloomz-3b": {
        "code_bleu": [
            {
                "codebleu": 0.19077513912745
            },
            {
                "ngram_match_score": 0.008309480452965182
            },
            {
                "weighted_ngram_match_score": 0.011498570953306086
            },
            {
                "syntax_match_score": 0.5118110236220472
            },
            {
                "dataflow_match_score": 0.23148148148148148
            }
        ],
        "cosine_similarity": 0.9597756266593933,
        "meteor": 0.12548262548262548
    },
    "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3": {
        "code_bleu": [
            {
                "codebleu": 0.1508136700801363
            },
            {
                "ngram_match_score": 0.0013907567060033059
            },
            {
                "weighted_ngram_match_score": 0.001914233861062029
            },
            {
                "syntax_match_score": 0.31343283582089554
            },
            {
                "dataflow_match_score": 0.28651685393258425
            }
        ],
        "cosine_similarity": 0.9823533892631531,
        "meteor": 0.04587155963302752
    },
    "google/long-t5-local-base": {
        "code_bleu": [
            {
                "codebleu": 0.17682016696953076
            },
            {
                "ngram_match_score": 0.00206945416902857
            },
            {
                "weighted_ngram_match_score": 0.010270737518618322
            },
            {
                "syntax_match_score": 0.21875
            },
            {
                "dataflow_match_score": 0.47619047619047616
            }
        ],
        "cosine_similarity": 0.9286025166511536,
        "meteor": 0.07042253521126761
    },
    "THUDM/glm-4-9b": {
        "code_bleu": [
            {
                "codebleu": 0.25925336852720615
            },
            {
                "ngram_match_score": 0.0003654897150796161
            },
            {
                "weighted_ngram_match_score": 0.0009336986794592838
            },
            {
                "syntax_match_score": 0.03571428571428571
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9398323893547058,
        "meteor": 0.002224199288256228
    },
    "deepseek-ai/deepseek-llm-67b-chat": {
        "code_bleu": [
            {
                "codebleu": 0.15752791385644632
            },
            {
                "ngram_match_score": 0.006815998441261126
            },
            {
                "weighted_ngram_match_score": 0.011481310992962991
            },
            {
                "syntax_match_score": 0.27848101265822783
            },
            {
                "dataflow_match_score": 0.3333333333333333
            }
        ],
        "cosine_similarity": 0.9500446319580078,
        "meteor": 0.10436432637571157
    },
    "microsoft/prophetnet-large-uncased": {
        "code_bleu": [
            {
                "codebleu": 0.058502906976744186
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.046511627906976744
            },
            {
                "dataflow_match_score": 0.1875
            }
        ],
        "cosine_similarity": 0.9539860486984253,
        "meteor": 0.0
    },
    "deepset/roberta-large-squad2": {
        "code_bleu": [
            {
                "codebleu": 0.22716668591533565
            },
            {
                "ngram_match_score": 0.005168984783110524
            },
            {
                "weighted_ngram_match_score": 0.005036984300277732
            },
            {
                "syntax_match_score": 0.40789473684210525
            },
            {
                "dataflow_match_score": 0.49056603773584906
            }
        ],
        "cosine_similarity": 0.9656063914299011,
        "meteor": 0.09788566953797966
    },
    "kandinsky-community/kandinsky-2-2-decoder-inpaint": {
        "code_bleu": [
            {
                "codebleu": 0.21932569622264325
            },
            {
                "ngram_match_score": 0.004191556027391095
            },
            {
                "weighted_ngram_match_score": 0.0042113115761678345
            },
            {
                "syntax_match_score": 0.32051282051282054
            },
            {
                "dataflow_match_score": 0.5483870967741935
            }
        ],
        "cosine_similarity": 0.9639729261398315,
        "meteor": 0.09113001215066828
    },
    "baichuan-inc/Baichuan2-13B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.157261491287626
            },
            {
                "ngram_match_score": 0.0010142795834262397
            },
            {
                "weighted_ngram_match_score": 0.006236813772205947
            },
            {
                "syntax_match_score": 0.3384615384615385
            },
            {
                "dataflow_match_score": 0.2833333333333333
            }
        ],
        "cosine_similarity": 0.9566714763641357,
        "meteor": 0.025510204081632654
    },
    "codellama/CodeLlama-13b-hf": {
        "code_bleu": [
            {
                "codebleu": 0.09735376258303532
            },
            {
                "ngram_match_score": 0.00667617426787696
            },
            {
                "weighted_ngram_match_score": 0.0322626855880739
            },
            {
                "syntax_match_score": 0.16
            },
            {
                "dataflow_match_score": 0.19047619047619047
            }
        ],
        "cosine_similarity": 0.9814708232879639,
        "meteor": 0.0800711743772242
    },
    "RUCAIBox/mvp": {
        "code_bleu": [
            {
                "codebleu": 0.20954816951859007
            },
            {
                "ngram_match_score": 0.004797918622358589
            },
            {
                "weighted_ngram_match_score": 0.004945431002673187
            },
            {
                "syntax_match_score": 0.43956043956043955
            },
            {
                "dataflow_match_score": 0.3888888888888889
            }
        ],
        "cosine_similarity": 0.9674414396286011,
        "meteor": 0.053586150041220124
    },
    "shi-labs/oneformer/ade20k/swin/tiny": {
        "code_bleu": [
            {
                "codebleu": 0.14590567450979047
            },
            {
                "ngram_match_score": 0.0065101387022778266
            },
            {
                "weighted_ngram_match_score": 0.00946550051335474
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.3176470588235294
            }
        ],
        "cosine_similarity": 0.9463241100311279,
        "meteor": 0.12413108242303875
    },
    "togethercomputer/RedPajama-INCITE-Chat-3B-v1": {
        "code_bleu": [
            {
                "codebleu": 0.13130962268047036
            },
            {
                "ngram_match_score": 0.0007160254663147998
            },
            {
                "weighted_ngram_match_score": 0.0020299044293807075
            },
            {
                "syntax_match_score": 0.3448275862068966
            },
            {
                "dataflow_match_score": 0.17766497461928935
            }
        ],
        "cosine_similarity": 0.9818342924118042,
        "meteor": 0.02781413612565445
    },
    "facebook/deit-small-patch16-224": {
        "code_bleu": [
            {
                "codebleu": 0.2114168552610235
            },
            {
                "ngram_match_score": 0.0022236437886132207
            },
            {
                "weighted_ngram_match_score": 0.005781439593143088
            },
            {
                "syntax_match_score": 0.24242424242424243
            },
            {
                "dataflow_match_score": 0.5952380952380952
            }
        ],
        "cosine_similarity": 0.9697301387786865,
        "meteor": 0.08665511265164647
    },
    "codellama/CodeLlama-70b-Instruct-hf": {
        "code_bleu": [
            {
                "codebleu": 0.09463244730747311
            },
            {
                "ngram_match_score": 0.00527465268581667
            },
            {
                "weighted_ngram_match_score": 0.005756034855250808
            },
            {
                "syntax_match_score": 0.24793388429752067
            },
            {
                "dataflow_match_score": 0.11956521739130435
            }
        ],
        "cosine_similarity": 0.9830235242843628,
        "meteor": 0.08340283569641369
    },
    "Deci/DeciLM-7B": {
        "code_bleu": [
            {
                "codebleu": 0.09689078793876793
            },
            {
                "ngram_match_score": 0.0034993523675867677
            },
            {
                "weighted_ngram_match_score": 0.0043264635525880986
            },
            {
                "syntax_match_score": 0.1951219512195122
            },
            {
                "dataflow_match_score": 0.18461538461538463
            }
        ],
        "cosine_similarity": 0.9835524559020996,
        "meteor": 0.0909090909090909
    },
    "EleutherAI/pythia-12b-deduped": {
        "code_bleu": [
            {
                "codebleu": 0.207660206064036
            },
            {
                "ngram_match_score": 0.00700299622706327
            },
            {
                "weighted_ngram_match_score": 0.02652475665778972
            },
            {
                "syntax_match_score": 0.3488372093023256
            },
            {
                "dataflow_match_score": 0.4482758620689655
            }
        ],
        "cosine_similarity": 0.9676269292831421,
        "meteor": 0.13966480446927373
    },
    "rinna/japanese-gpt2-medium": {
        "code_bleu": [
            {
                "codebleu": 0.3168382154435986
            },
            {
                "ngram_match_score": 0.019572757580051156
            },
            {
                "weighted_ngram_match_score": 0.07635153276577188
            },
            {
                "syntax_match_score": 0.5714285714285714
            },
            {
                "dataflow_match_score": 0.6
            }
        ],
        "cosine_similarity": 0.9372105598449707,
        "meteor": 0.1470588235294118
    },
    "google/pix2struct-textcaps-base": {
        "code_bleu": [
            {
                "codebleu": 0.1255052044003135
            },
            {
                "ngram_match_score": 0.0032196467008354292
            },
            {
                "weighted_ngram_match_score": 0.0037462258454735455
            },
            {
                "syntax_match_score": 0.2642857142857143
            },
            {
                "dataflow_match_score": 0.23076923076923078
            }
        ],
        "cosine_similarity": 0.9550259113311768,
        "meteor": 0.06433350488934637
    },
    "keepitreal/vietnamese-sbert": {
        "code_bleu": [
            {
                "codebleu": 0.12006440796548437
            },
            {
                "ngram_match_score": 0.0022629658624549716
            },
            {
                "weighted_ngram_match_score": 0.004579192584009101
            },
            {
                "syntax_match_score": 0.21367521367521367
            },
            {
                "dataflow_match_score": 0.2597402597402597
            }
        ],
        "cosine_similarity": 0.9582350254058838,
        "meteor": 0.03777472527472527
    },
    "xlm-mlm-en-2048": {
        "code_bleu": [
            {
                "codebleu": 0.24761577499683235
            },
            {
                "ngram_match_score": 0.018072853646516975
            },
            {
                "weighted_ngram_match_score": 0.11104570852568636
            },
            {
                "syntax_match_score": 0.14705882352941177
            },
            {
                "dataflow_match_score": 0.7142857142857143
            }
        ],
        "cosine_similarity": 0.8915066719055176,
        "meteor": 0.13666072489601902
    },
    "google/long-t5-tglobal-base": {
        "code_bleu": [
            {
                "codebleu": 0.2223162155454842
            },
            {
                "ngram_match_score": 0.013853411725680142
            },
            {
                "weighted_ngram_match_score": 0.02273287902768527
            },
            {
                "syntax_match_score": 0.28125
            },
            {
                "dataflow_match_score": 0.5714285714285714
            }
        ],
        "cosine_similarity": 0.9706882238388062,
        "meteor": 0.11152416356877323
    },
    "TheBloke/Llama-2-7B-Chat-AWQ": {
        "code_bleu": [
            {
                "codebleu": 0.18127317312261643
            },
            {
                "ngram_match_score": 0.0025429678097875566
            },
            {
                "weighted_ngram_match_score": 0.003972649581863967
            },
            {
                "syntax_match_score": 0.391304347826087
            },
            {
                "dataflow_match_score": 0.32727272727272727
            }
        ],
        "cosine_similarity": 0.9888437986373901,
        "meteor": 0.07417709782104775
    },
    "Vamsi/T5/Paraphrase/Paws": {
        "code_bleu": [
            {
                "codebleu": 0.42100741342411196
            },
            {
                "ngram_match_score": 0.17877509358746316
            },
            {
                "weighted_ngram_match_score": 0.2694714503413671
            },
            {
                "syntax_match_score": 0.6835443037974683
            },
            {
                "dataflow_match_score": 0.5522388059701493
            }
        ],
        "cosine_similarity": 0.9866466522216797,
        "meteor": 0.4073355464683305
    },
    "abeja/gpt-neox-japanese-2.7b": {
        "code_bleu": [
            {
                "codebleu": 0.20329440439470875
            },
            {
                "ngram_match_score": 0.005888794423372789
            },
            {
                "weighted_ngram_match_score": 0.009771699867790947
            },
            {
                "syntax_match_score": 0.3287671232876712
            },
            {
                "dataflow_match_score": 0.46875
            }
        ],
        "cosine_similarity": 0.9341140985488892,
        "meteor": 0.0753012048192771
    },
    "microsoft/cvt-13": {
        "code_bleu": [
            {
                "codebleu": 0.12608225108225107
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.07575757575757576
            },
            {
                "dataflow_match_score": 0.42857142857142855
            }
        ],
        "cosine_similarity": 0.9624931812286377,
        "meteor": 0.0
    },
    "albert-large-v2": {
        "code_bleu": [
            {
                "codebleu": 0.18660692148921096
            },
            {
                "ngram_match_score": 0.0009515863624646957
            },
            {
                "weighted_ngram_match_score": 0.0012199790670722188
            },
            {
                "syntax_match_score": 0.4222222222222222
            },
            {
                "dataflow_match_score": 0.3220338983050847
            }
        ],
        "cosine_similarity": 0.95453280210495,
        "meteor": 0.03446955189582535
    },
    "CohereForAI/c4ai-command-r-v01": {
        "code_bleu": [
            {
                "codebleu": 0.0363349428078506
            },
            {
                "ngram_match_score": 3.013194857425203e-07
            },
            {
                "weighted_ngram_match_score": 0.0006240626231614473
            },
            {
                "syntax_match_score": 0.08635578583765112
            },
            {
                "dataflow_match_score": 0.0583596214511041
            }
        ],
        "cosine_similarity": 0.9799344539642334,
        "meteor": 0.011502497693705
    },
    "bigcode/starcoder": {
        "code_bleu": [
            {
                "codebleu": 0.18286566309482213
            },
            {
                "ngram_match_score": 0.002333863701022845
            },
            {
                "weighted_ngram_match_score": 0.006663035253608147
            },
            {
                "syntax_match_score": 0.3424657534246575
            },
            {
                "dataflow_match_score": 0.38
            }
        ],
        "cosine_similarity": 0.9636622071266174,
        "meteor": 0.08702531645569621
    },
    "microsoft/swin-base-patch4-window12-384": {
        "code_bleu": [
            {
                "codebleu": 0.22084834745308046
            },
            {
                "ngram_match_score": 0.0025363006858763397
            },
            {
                "weighted_ngram_match_score": 0.006398214667571038
            },
            {
                "syntax_match_score": 0.30303030303030304
            },
            {
                "dataflow_match_score": 0.5714285714285714
            }
        ],
        "cosine_similarity": 0.9434126615524292,
        "meteor": 0.10471204188481677
    },
    "teknium/OpenHermes-2-Mistral-7B": {
        "code_bleu": [
            {
                "codebleu": 0.23106060606060608
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.09090909090909091
            },
            {
                "dataflow_match_score": 0.8333333333333334
            }
        ],
        "cosine_similarity": 0.8623387813568115,
        "meteor": 0.0
    },
    "Salesforce/blip2-flan-t5-xxl": {
        "code_bleu": [
            {
                "codebleu": 0.3483468221889917
            },
            {
                "ngram_match_score": 0.0037049800010940095
            },
            {
                "weighted_ngram_match_score": 0.0036528969901669104
            },
            {
                "syntax_match_score": 0.3860294117647059
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9622859954833984,
        "meteor": 0.08093994778067885
    },
    "BAAI/AquilaChat2-7B": {
        "code_bleu": [
            {
                "codebleu": 0.19671372450174002
            },
            {
                "ngram_match_score": 0.03421387076778741
            },
            {
                "weighted_ngram_match_score": 0.13340414329640635
            },
            {
                "syntax_match_score": 0.14864864864864866
            },
            {
                "dataflow_match_score": 0.47058823529411764
            }
        ],
        "cosine_similarity": 0.9339308738708496,
        "meteor": 0.07604562737642588
    },
    "MBZUAI/LaMini-T5-738M": {
        "code_bleu": [
            {
                "codebleu": 0.15081084100188624
            },
            {
                "ngram_match_score": 0.0026416971682939393
            },
            {
                "weighted_ngram_match_score": 0.0033794446170288587
            },
            {
                "syntax_match_score": 0.1388888888888889
            },
            {
                "dataflow_match_score": 0.4583333333333333
            }
        ],
        "cosine_similarity": 0.9832521080970764,
        "meteor": 0.05035246727089627
    },
    "01-ai/Yi-6B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.2908787059199982
            },
            {
                "ngram_match_score": 0.0005307516530487704
            },
            {
                "weighted_ngram_match_score": 0.001398706173285587
            },
            {
                "syntax_match_score": 0.16158536585365854
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9353539347648621,
        "meteor": 0.05735824319895116
    },
    "openaccess-ai-collective/manticore-13b": {
        "code_bleu": [
            {
                "codebleu": 0.013304016695987731
            },
            {
                "ngram_match_score": 5.316615060268767e-09
            },
            {
                "weighted_ngram_match_score": 0.0009195436292670746
            },
            {
                "syntax_match_score": 0.026490066225165563
            },
            {
                "dataflow_match_score": 0.025806451612903226
            }
        ],
        "cosine_similarity": 0.9479968547821045,
        "meteor": 0.00887648998224702
    },
    "bigcode/santacoder": {
        "code_bleu": [
            {
                "codebleu": 0.20580067622947576
            },
            {
                "ngram_match_score": 0.004385021733192307
            },
            {
                "weighted_ngram_match_score": 0.011645126012153614
            },
            {
                "syntax_match_score": 0.36486486486486486
            },
            {
                "dataflow_match_score": 0.4423076923076923
            }
        ],
        "cosine_similarity": 0.9561229944229126,
        "meteor": 0.10852713178294575
    },
    "allenai/OLMo-1B": {
        "code_bleu": [
            {
                "codebleu": 0.08316192616001716
            },
            {
                "ngram_match_score": 0.002206860322586274
            },
            {
                "weighted_ngram_match_score": 0.005077018953657002
            },
            {
                "syntax_match_score": 0.0945945945945946
            },
            {
                "dataflow_match_score": 0.23076923076923078
            }
        ],
        "cosine_similarity": 0.9614018201828003,
        "meteor": 0.13312034078807242
    },
    "asafaya/bert-base-arabic": {
        "code_bleu": [
            {
                "codebleu": 0.2363421286874531
            },
            {
                "ngram_match_score": 0.004925628175455071
            },
            {
                "weighted_ngram_match_score": 0.03848210226063191
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.6666666666666666
            }
        ],
        "cosine_similarity": 0.916643500328064,
        "meteor": 0.13813813813813813
    },
    "aubmindlab/bert-base-arabertv2": {
        "code_bleu": [
            {
                "codebleu": 0.37674237960729695
            },
            {
                "ngram_match_score": 0.001847024761999881
            },
            {
                "weighted_ngram_match_score": 0.0051224936671879035
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9264801740646362,
        "meteor": 0.020618556701030927
    },
    "Intel/dynamic/tinybert": {
        "code_bleu": [
            {
                "codebleu": 0.5910561448134816
            },
            {
                "ngram_match_score": 0.4599132727281551
            },
            {
                "weighted_ngram_match_score": 0.7171318193462841
            },
            {
                "syntax_match_score": 0.6538461538461539
            },
            {
                "dataflow_match_score": 0.5333333333333333
            }
        ],
        "cosine_similarity": 0.9808641672134399,
        "meteor": 0.6699129076337932
    },
    "pszemraj/long-t5-tglobal-base-16384-book-summary": {
        "code_bleu": [
            {
                "codebleu": 0.23172255612514164
            },
            {
                "ngram_match_score": 0.0025386772918494123
            },
            {
                "weighted_ngram_match_score": 0.005594080064153323
            },
            {
                "syntax_match_score": 0.37037037037037035
            },
            {
                "dataflow_match_score": 0.5483870967741935
            }
        ],
        "cosine_similarity": 0.9561585783958435,
        "meteor": 0.10653409090909091
    },
    "deepseek-ai/DeepSeek-V2": {
        "code_bleu": [
            {
                "codebleu": 0.11627313495686328
            },
            {
                "ngram_match_score": 0.0020173036517219327
            },
            {
                "weighted_ngram_match_score": 0.0033467293884008294
            },
            {
                "syntax_match_score": 0.2923076923076923
            },
            {
                "dataflow_match_score": 0.167420814479638
            }
        ],
        "cosine_similarity": 0.962027370929718,
        "meteor": 0.05011565150346955
    },
    "bigscience/mt0-large": {
        "code_bleu": [
            {
                "codebleu": 0.12568374927329148
            },
            {
                "ngram_match_score": 0.03844414456242185
            },
            {
                "weighted_ngram_match_score": 0.06927772917116402
            },
            {
                "syntax_match_score": 0.2283464566929134
            },
            {
                "dataflow_match_score": 0.16666666666666666
            }
        ],
        "cosine_similarity": 0.9790645241737366,
        "meteor": 0.12689677106284752
    },
    "google/recurrentgemma-2b": {
        "code_bleu": [
            {
                "codebleu": 0.05377655259375879
            },
            {
                "ngram_match_score": 0.0016093850171181648
            },
            {
                "weighted_ngram_match_score": 0.00608941795050959
            },
            {
                "syntax_match_score": 0.13333333333333333
            },
            {
                "dataflow_match_score": 0.07407407407407407
            }
        ],
        "cosine_similarity": 0.9238280057907104,
        "meteor": 0.026809651474530835
    },
    "ynie/roberta-large-snli/mnli/fever/anli/R1/R2/R3-nli": {
        "code_bleu": [
            {
                "codebleu": 0.12799468303876627
            },
            {
                "ngram_match_score": 0.00427721430731466
            },
            {
                "weighted_ngram_match_score": 0.006075501587587737
            },
            {
                "syntax_match_score": 0.23333333333333334
            },
            {
                "dataflow_match_score": 0.2682926829268293
            }
        ],
        "cosine_similarity": 0.9885742664337158,
        "meteor": 0.05663430420711975
    },
    "EleutherAI/pythia-12b": {
        "code_bleu": [
            {
                "codebleu": 0.1446397565711312
            },
            {
                "ngram_match_score": 0.0035548168188028837
            },
            {
                "weighted_ngram_match_score": 0.007241579152971296
            },
            {
                "syntax_match_score": 0.3953488372093023
            },
            {
                "dataflow_match_score": 0.1724137931034483
            }
        ],
        "cosine_similarity": 0.9096596240997314,
        "meteor": 0.031055900621118016
    },
    "Qwen/Qwen-72B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.10436907240342355
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.10687022900763359
            },
            {
                "dataflow_match_score": 0.3106060606060606
            }
        ],
        "cosine_similarity": 0.9805806279182434,
        "meteor": 0.0
    },
    "internlm/internlm-20b": {
        "code_bleu": [
            {
                "codebleu": 0.18373461866943858
            },
            {
                "ngram_match_score": 0.008233646253459143
            },
            {
                "weighted_ngram_match_score": 0.010668151777588643
            },
            {
                "syntax_match_score": 0.41916167664670656
            },
            {
                "dataflow_match_score": 0.296875
            }
        ],
        "cosine_similarity": 0.9518523216247559,
        "meteor": 0.0818746470920384
    },
    "facebook/mask2former-swin-small-ade-semantic": {
        "code_bleu": [
            {
                "codebleu": 0.1712880671574386
            },
            {
                "ngram_match_score": 0.07549297516226847
            },
            {
                "weighted_ngram_match_score": 0.10055835579511968
            },
            {
                "syntax_match_score": 0.16216216216216217
            },
            {
                "dataflow_match_score": 0.3469387755102041
            }
        ],
        "cosine_similarity": 0.9876527786254883,
        "meteor": 0.09175309468049513
    },
    "allenai/OLMo-7B-Instruct": {
        "code_bleu": [
            {
                "codebleu": 0.18237314620408002
            },
            {
                "ngram_match_score": 0.006245702511669435
            },
            {
                "weighted_ngram_match_score": 0.009561408114974678
            },
            {
                "syntax_match_score": 0.2647058823529412
            },
            {
                "dataflow_match_score": 0.4489795918367347
            }
        ],
        "cosine_similarity": 0.9768400192260742,
        "meteor": 0.10597883597883598
    },
    "timm/ViT-B-16-SigLIP-256": {
        "code_bleu": [
            {
                "codebleu": 0.1997933436027989
            },
            {
                "ngram_match_score": 0.004621921613980584
            },
            {
                "weighted_ngram_match_score": 0.004824919168242391
            },
            {
                "syntax_match_score": 0.3181818181818182
            },
            {
                "dataflow_match_score": 0.4715447154471545
            }
        ],
        "cosine_similarity": 0.9910434484481812,
        "meteor": 0.07235890014471781
    },
    "microsoft/beit-base-patch16-224": {
        "code_bleu": [
            {
                "codebleu": 0.15125953025225475
            },
            {
                "ngram_match_score": 0.019129680717590008
            },
            {
                "weighted_ngram_match_score": 0.09240194678493545
            },
            {
                "syntax_match_score": 0.13636363636363635
            },
            {
                "dataflow_match_score": 0.35714285714285715
            }
        ],
        "cosine_similarity": 0.9585010409355164,
        "meteor": 0.0794912559618442
    },
    "timm/ViT-B-16-SigLIP-512": {
        "code_bleu": [
            {
                "codebleu": 0.09611111012042742
            },
            {
                "ngram_match_score": 0.001240344865209797
            },
            {
                "weighted_ngram_match_score": 0.0013674363408162126
            },
            {
                "syntax_match_score": 0.17045454545454544
            },
            {
                "dataflow_match_score": 0.21138211382113822
            }
        ],
        "cosine_similarity": 0.9782038331031799,
        "meteor": 0.006939625260235947
    },
    "databricks/dolly-v2-7b": {
        "code_bleu": [
            {
                "codebleu": 0.19976419715504684
            },
            {
                "ngram_match_score": 0.0024769271047582206
            },
            {
                "weighted_ngram_match_score": 0.002678441298219463
            },
            {
                "syntax_match_score": 0.3412698412698413
            },
            {
                "dataflow_match_score": 0.45263157894736844
            }
        ],
        "cosine_similarity": 0.9328813552856445,
        "meteor": 0.05716463414634147
    },
    "allenai/OLMo-7B": {
        "code_bleu": [
            {
                "codebleu": 0.20212786241778574
            },
            {
                "ngram_match_score": 0.005873459825706979
            },
            {
                "weighted_ngram_match_score": 0.005340692548138673
            },
            {
                "syntax_match_score": 0.2972972972972973
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9355177879333496,
        "meteor": 0.041766109785202864
    },
    "apple/mobilevit-x-small": {
        "code_bleu": [
            {
                "codebleu": 0.23265397782778371
            },
            {
                "ngram_match_score": 0.0027565496400696666
            },
            {
                "weighted_ngram_match_score": 0.005781439593143088
            },
            {
                "syntax_match_score": 0.30303030303030304
            },
            {
                "dataflow_match_score": 0.6190476190476191
            }
        ],
        "cosine_similarity": 0.9463896751403809,
        "meteor": 0.08166969147005446
    },
    "togethercomputer/RedPajama-INCITE-Base-3B-v1": {
        "code_bleu": [
            {
                "codebleu": 0.17226971456941662
            },
            {
                "ngram_match_score": 0.0018517175291136822
            },
            {
                "weighted_ngram_match_score": 0.005694101491575857
            },
            {
                "syntax_match_score": 0.36681222707423583
            },
            {
                "dataflow_match_score": 0.3147208121827411
            }
        ],
        "cosine_similarity": 0.9551864862442017,
        "meteor": 0.04082674151569278
    },
    "baichuan-inc/Baichuan-13B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.22684624889963048
            },
            {
                "ngram_match_score": 0.002756562467759728
            },
            {
                "weighted_ngram_match_score": 0.003696756112128682
            },
            {
                "syntax_match_score": 0.3652173913043478
            },
            {
                "dataflow_match_score": 0.5357142857142857
            }
        ],
        "cosine_similarity": 0.9747534990310669,
        "meteor": 0.0746268656716418
    },
    "h2oai/h2ogpt-oasst1-512-12b": {
        "code_bleu": [
            {
                "codebleu": 0.18979137565771917
            },
            {
                "ngram_match_score": 0.0028547084015994864
            },
            {
                "weighted_ngram_match_score": 0.004321404308852796
            },
            {
                "syntax_match_score": 0.2692307692307692
            },
            {
                "dataflow_match_score": 0.4827586206896552
            }
        ],
        "cosine_similarity": 0.9340341091156006,
        "meteor": 0.04830917874396135
    },
    "google/codegemma-7b": {
        "code_bleu": [
            {
                "codebleu": 0.14606881105899333
            },
            {
                "ngram_match_score": 0.003821570781407475
            },
            {
                "weighted_ngram_match_score": 0.008576088673755344
            },
            {
                "syntax_match_score": 0.27956989247311825
            },
            {
                "dataflow_match_score": 0.2923076923076923
            }
        ],
        "cosine_similarity": 0.980445384979248,
        "meteor": 0.09486607142857142
    },
    "meta-llama/LlamaGuard-7b": {
        "code_bleu": [
            {
                "codebleu": 0.3997251866894841
            },
            {
                "ngram_match_score": 0.04946635704532241
            },
            {
                "weighted_ngram_match_score": 0.0869343897126139
            },
            {
                "syntax_match_score": 0.4625
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9701671600341797,
        "meteor": 0.15831705842691407
    },
    "microsoft/conditional-detr-resnet-50": {
        "code_bleu": [
            {
                "codebleu": 0.13592620685336038
            },
            {
                "ngram_match_score": 0.008878604850025448
            },
            {
                "weighted_ngram_match_score": 0.022931575154750007
            },
            {
                "syntax_match_score": 0.102803738317757
            },
            {
                "dataflow_match_score": 0.4090909090909091
            }
        ],
        "cosine_similarity": 0.9737808108329773,
        "meteor": 0.05916419628003554
    },
    "deepmind/vision-perceiver-conv": {
        "code_bleu": [
            {
                "codebleu": 0.26161232859427264
            },
            {
                "ngram_match_score": 0.004661679616622297
            },
            {
                "weighted_ngram_match_score": 0.0110184039912373
            },
            {
                "syntax_match_score": 0.4153846153846154
            },
            {
                "dataflow_match_score": 0.6153846153846154
            }
        ],
        "cosine_similarity": 0.9579145312309265,
        "meteor": 0.09515570934256057
    },
    "microsoft/swin-base-patch4-window7-224-in22k": {
        "code_bleu": [
            {
                "codebleu": 0.33566660233095713
            },
            {
                "ngram_match_score": 0.05136204152421221
            },
            {
                "weighted_ngram_match_score": 0.12463770113294959
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 0.6666666666666666
            }
        ],
        "cosine_similarity": 0.9484257698059082,
        "meteor": 0.20478714961429792
    },
    "facebook/galactica-1.3b": {
        "code_bleu": [
            {
                "codebleu": 0.14591468048701547
            },
            {
                "ngram_match_score": 0.0050761993383782985
            },
            {
                "weighted_ngram_match_score": 0.005350199377360326
            },
            {
                "syntax_match_score": 0.29545454545454547
            },
            {
                "dataflow_match_score": 0.2777777777777778
            }
        ],
        "cosine_similarity": 0.9781038761138916,
        "meteor": 0.07036423841059603
    },
    "VietAI/vit5-base": {
        "code_bleu": [
            {
                "codebleu": 0.24224303904402225
            },
            {
                "ngram_match_score": 0.004456333363907705
            },
            {
                "weighted_ngram_match_score": 0.09438595268231116
            },
            {
                "syntax_match_score": 0.14285714285714285
            },
            {
                "dataflow_match_score": 0.7272727272727273
            }
        ],
        "cosine_similarity": 0.9216102957725525,
        "meteor": 0.0808720112517581
    },
    "facebook/xlm-roberta-xxl": {
        "code_bleu": [
            {
                "codebleu": 0.13317366341059078
            },
            {
                "ngram_match_score": 0.009770336085167584
            },
            {
                "weighted_ngram_match_score": 0.01750559834537284
            },
            {
                "syntax_match_score": 0.05714285714285714
            },
            {
                "dataflow_match_score": 0.4482758620689655
            }
        ],
        "cosine_similarity": 0.965667724609375,
        "meteor": 0.045071526553008036
    },
    "Qwen/Qwen-1/8B": {
        "code_bleu": [
            {
                "codebleu": 0.2397225092937897
            },
            {
                "ngram_match_score": 0.002756322094390396
            },
            {
                "weighted_ngram_match_score": 0.0033206479301330872
            },
            {
                "syntax_match_score": 0.1896551724137931
            },
            {
                "dataflow_match_score": 0.7631578947368421
            }
        ],
        "cosine_similarity": 0.9227950572967529,
        "meteor": 0.05083179297597042
    },
    "deepseek-ai/deepseek-llm-7b-base": {
        "code_bleu": [
            {
                "codebleu": 0.21573494751206776
            },
            {
                "ngram_match_score": 0.012814175010309254
            },
            {
                "weighted_ngram_match_score": 0.020661822887602495
            },
            {
                "syntax_match_score": 0.47761194029850745
            },
            {
                "dataflow_match_score": 0.35185185185185186
            }
        ],
        "cosine_similarity": 0.9571778178215027,
        "meteor": 0.11265228240129165
    },
    "codeparrot/codeparrot": {
        "code_bleu": [
            {
                "codebleu": 0.15994315649832097
            },
            {
                "ngram_match_score": 0.0016196660044770497
            },
            {
                "weighted_ngram_match_score": 0.00884261516122072
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.3793103448275862
            }
        ],
        "cosine_similarity": 0.9242748618125916,
        "meteor": 0.04705882352941177
    },
    "openbmb/MiniCPM-2B-sft-bf16": {
        "code_bleu": [
            {
                "codebleu": 0.36802335338842346
            },
            {
                "ngram_match_score": 0.1863161938392376
            },
            {
                "weighted_ngram_match_score": 0.1895346356669941
            },
            {
                "syntax_match_score": 0.6097560975609756
            },
            {
                "dataflow_match_score": 0.4864864864864865
            }
        ],
        "cosine_similarity": 0.9781657457351685,
        "meteor": 0.26920047660399454
    },
    "databricks/dolly-v2-12b": {
        "code_bleu": [
            {
                "codebleu": 0.1866483103718967
            },
            {
                "ngram_match_score": 0.004205591893605076
            },
            {
                "weighted_ngram_match_score": 0.0040417849323275456
            },
            {
                "syntax_match_score": 0.2857142857142857
            },
            {
                "dataflow_match_score": 0.45263157894736844
            }
        ],
        "cosine_similarity": 0.9778674840927124,
        "meteor": 0.03728560775540641
    },
    "deepset/xlm-roberta-large-squad2": {
        "code_bleu": [
            {
                "codebleu": 0.14940887369789674
            },
            {
                "ngram_match_score": 0.002234540514014117
            },
            {
                "weighted_ngram_match_score": 0.002503046858422632
            },
            {
                "syntax_match_score": 0.3157894736842105
            },
            {
                "dataflow_match_score": 0.27710843373493976
            }
        ],
        "cosine_similarity": 0.9690709114074707,
        "meteor": 0.01905280348394121
    },
    "microsoft/beit-large-patch16-224-pt22k-ft22k": {
        "code_bleu": [
            {
                "codebleu": 0.047619047619047616
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.19047619047619047
            }
        ],
        "cosine_similarity": 0.9359819889068604,
        "meteor": 0.0
    },
    "TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ": {
        "code_bleu": [
            {
                "codebleu": 0.14409448440012557
            },
            {
                "ngram_match_score": 0.0023072496703118615
            },
            {
                "weighted_ngram_match_score": 0.002899516759019248
            },
            {
                "syntax_match_score": 0.23333333333333334
            },
            {
                "dataflow_match_score": 0.33783783783783783
            }
        ],
        "cosine_similarity": 0.9881337881088257,
        "meteor": 0.05135387488328664
    },
    "bigscience/T0/3B": {
        "code_bleu": [
            {
                "codebleu": 0.20442626557004537
            },
            {
                "ngram_match_score": 0.0009859073446293598
            },
            {
                "weighted_ngram_match_score": 0.008365346581743706
            },
            {
                "syntax_match_score": 0.08108108108108109
            },
            {
                "dataflow_match_score": 0.7272727272727273
            }
        ],
        "cosine_similarity": 0.9253370761871338,
        "meteor": 0.018518518518518517
    },
    "adept/fuyu-8b": {
        "code_bleu": [
            {
                "codebleu": 0.19617727857346406
            },
            {
                "ngram_match_score": 0.004192832194544357
            },
            {
                "weighted_ngram_match_score": 0.008943955055286702
            },
            {
                "syntax_match_score": 0.18666666666666668
            },
            {
                "dataflow_match_score": 0.5849056603773585
            }
        ],
        "cosine_similarity": 0.9758943319320679,
        "meteor": 0.11834319526627218
    },
    "bert-large-cased-whole-word-masking": {
        "code_bleu": [
            {
                "codebleu": 0.23933588576221626
            },
            {
                "ngram_match_score": 0.0005806605088117682
            },
            {
                "weighted_ngram_match_score": 0.0012701016235434318
            },
            {
                "syntax_match_score": 0.5148148148148148
            },
            {
                "dataflow_match_score": 0.4406779661016949
            }
        ],
        "cosine_similarity": 0.9709051847457886,
        "meteor": 0.027624309392265192
    },
    "gogamza/kobart-summarization": {
        "code_bleu": [
            {
                "codebleu": 0.18899521531100477
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.21052631578947367
            },
            {
                "dataflow_match_score": 0.5454545454545454
            }
        ],
        "cosine_similarity": 0.9018828868865967,
        "meteor": 0.0
    },
    "tiiuae/falcon-180B": {
        "code_bleu": [
            {
                "codebleu": 0.22675747267054155
            },
            {
                "ngram_match_score": 0.004627170853968056
            },
            {
                "weighted_ngram_match_score": 0.008845296859010452
            },
            {
                "syntax_match_score": 0.27450980392156865
            },
            {
                "dataflow_match_score": 0.6190476190476191
            }
        ],
        "cosine_similarity": 0.9684719443321228,
        "meteor": 0.11226005324365979
    },
    "HuggingFaceM4/idefics-9b": {
        "code_bleu": [
            {
                "codebleu": 0.032026294599346194
            },
            {
                "ngram_match_score": 0.005283861120782009
            },
            {
                "weighted_ngram_match_score": 0.007548741329869524
            },
            {
                "syntax_match_score": 0.04119850187265917
            },
            {
                "dataflow_match_score": 0.07407407407407407
            }
        ],
        "cosine_similarity": 0.9543834924697876,
        "meteor": 0.02058319039451115
    },
    "TheBloke/Nous-Hermes-13B-GPTQ": {
        "code_bleu": [
            {
                "codebleu": 0.09968218445119244
            },
            {
                "ngram_match_score": 0.00521248420795398
            },
            {
                "weighted_ngram_match_score": 0.007814576633002893
            },
            {
                "syntax_match_score": 0.20388349514563106
            },
            {
                "dataflow_match_score": 0.18181818181818182
            }
        ],
        "cosine_similarity": 0.9934234619140625,
        "meteor": 0.09644670050761421
    },
    "facebook/maskformer-swin-base-ade": {
        "code_bleu": [
            {
                "codebleu": 0.15980375124219665
            },
            {
                "ngram_match_score": 0.0020741801940794245
            },
            {
                "weighted_ngram_match_score": 0.003425603825443747
            },
            {
                "syntax_match_score": 0.12307692307692308
            },
            {
                "dataflow_match_score": 0.5106382978723404
            }
        ],
        "cosine_similarity": 0.8934152126312256,
        "meteor": 0.05016722408026756
    },
    "kakaobrain/karlo-v1-alpha": {
        "code_bleu": [
            {
                "codebleu": 0.27157329468219626
            },
            {
                "ngram_match_score": 0.06296163362960945
            },
            {
                "weighted_ngram_match_score": 0.09393275855753185
            },
            {
                "syntax_match_score": 0.3783783783783784
            },
            {
                "dataflow_match_score": 0.5510204081632653
            }
        ],
        "cosine_similarity": 0.9699496626853943,
        "meteor": 0.14326635090896422
    },
    "baichuan-inc/Baichuan2-7B-Base": {
        "code_bleu": [
            {
                "codebleu": 0.23479384278728005
            },
            {
                "ngram_match_score": 0.001966957290844235
            },
            {
                "weighted_ngram_match_score": 0.008699641928451338
            },
            {
                "syntax_match_score": 0.40350877192982454
            },
            {
                "dataflow_match_score": 0.525
            }
        ],
        "cosine_similarity": 0.9519633054733276,
        "meteor": 0.08293838862559243
    },
    "nvidia/segformer-b1-finetuned-cityscapes-1024-1024": {
        "code_bleu": [
            {
                "codebleu": 0.2211878474781853
            },
            {
                "ngram_match_score": 0.0011291625970351218
            },
            {
                "weighted_ngram_match_score": 0.009336513029991762
            },
            {
                "syntax_match_score": 0.16
            },
            {
                "dataflow_match_score": 0.7142857142857143
            }
        ],
        "cosine_similarity": 0.9505561590194702,
        "meteor": 0.05862646566164155
    },
    "Salesforce/codegen-350M-multi": {
        "code_bleu": [
            {
                "codebleu": 0.30815040121935
            },
            {
                "ngram_match_score": 0.002520660934869459
            },
            {
                "weighted_ngram_match_score": 0.010568748820579371
            },
            {
                "syntax_match_score": 0.21951219512195122
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9469180107116699,
        "meteor": 0.07530120481927711
    },
    "nlp-waseda/roberta-base-japanese-with-auto-jumanpp": {
        "code_bleu": [
            {
                "codebleu": 0.20638616310072283
            },
            {
                "ngram_match_score": 0.0018980802981805563
            },
            {
                "weighted_ngram_match_score": 0.013189055764841435
            },
            {
                "syntax_match_score": 0.2222222222222222
            },
            {
                "dataflow_match_score": 0.5882352941176471
            }
        ],
        "cosine_similarity": 0.8935969471931458,
        "meteor": 0.08196721311475409
    },
    "RWKV/rwkv-4-169m-pile": {
        "code_bleu": [
            {
                "codebleu": 0.1510939268200226
            },
            {
                "ngram_match_score": 0.027669845924654244
            },
            {
                "weighted_ngram_match_score": 0.03540458837099483
            },
            {
                "syntax_match_score": 0.08415841584158416
            },
            {
                "dataflow_match_score": 0.45714285714285713
            }
        ],
        "cosine_similarity": 0.965540885925293,
        "meteor": 0.03847798204360838
    },
    "google/vit-hybrid-base-bit-384": {
        "code_bleu": [
            {
                "codebleu": 0.40530238947248987
            },
            {
                "ngram_match_score": 0.15497018909794713
            },
            {
                "weighted_ngram_match_score": 0.28476039999961084
            },
            {
                "syntax_match_score": 0.5223880597014925
            },
            {
                "dataflow_match_score": 0.6590909090909091
            }
        ],
        "cosine_similarity": 0.9713746309280396,
        "meteor": 0.39250946643717727
    },
    "google/pix2struct-base": {
        "code_bleu": [
            {
                "codebleu": 0.2247052378190061
            },
            {
                "ngram_match_score": 0.0021821432764033825
            },
            {
                "weighted_ngram_match_score": 0.013781665142478123
            },
            {
                "syntax_match_score": 0.24
            },
            {
                "dataflow_match_score": 0.6428571428571429
            }
        ],
        "cosine_similarity": 0.9213933348655701,
        "meteor": 0.025
    },
    "bigscience/bloomz-7b1-mt": {
        "code_bleu": [
            {
                "codebleu": 0.10276640248470971
            },
            {
                "ngram_match_score": 0.0033713351246535076
            },
            {
                "weighted_ngram_match_score": 0.003421892195346014
            },
            {
                "syntax_match_score": 0.2283464566929134
            },
            {
                "dataflow_match_score": 0.17592592592592593
            }
        ],
        "cosine_similarity": 0.9706528186798096,
        "meteor": 0.07944389275074479
    },
    "google/electra-base-generator": {
        "code_bleu": [
            {
                "codebleu": 0.15896501912258082
            },
            {
                "ngram_match_score": 0.002989232719814957
            },
            {
                "weighted_ngram_match_score": 0.006608217507882082
            },
            {
                "syntax_match_score": 0.18181818181818182
            },
            {
                "dataflow_match_score": 0.4444444444444444
            }
        ],
        "cosine_similarity": 0.8683515787124634,
        "meteor": 0.03389830508474577
    },
    "stabilityai/stablelm-base-alpha-3b": {
        "code_bleu": [
            {
                "codebleu": 0.19120369439545953
            },
            {
                "ngram_match_score": 0.002444993300661746
            },
            {
                "weighted_ngram_match_score": 0.007241579152971296
            },
            {
                "syntax_match_score": 0.28846153846153844
            },
            {
                "dataflow_match_score": 0.4666666666666667
            }
        ],
        "cosine_similarity": 0.926548182964325,
        "meteor": 0.028653295128939826
    },
    "mosaicml/mpt-7b-chat": {
        "code_bleu": [
            {
                "codebleu": 0.2891219352499248
            },
            {
                "ngram_match_score": 0.0074252243700737835
            },
            {
                "weighted_ngram_match_score": 0.020622149657148363
            },
            {
                "syntax_match_score": 0.12844036697247707
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9723131656646729,
        "meteor": 0.04583333333333333
    },
    "voidful/albert/chinese/tiny": {
        "code_bleu": [
            {
                "codebleu": 0.20459724251700423
            },
            {
                "ngram_match_score": 0.0416082294087655
            },
            {
                "weighted_ngram_match_score": 0.07958822729026747
            },
            {
                "syntax_match_score": 0.32954545454545453
            },
            {
                "dataflow_match_score": 0.36764705882352944
            }
        ],
        "cosine_similarity": 0.9640339016914368,
        "meteor": 0.21397016195906624
    },
    "SenseTime/deformable-detr-with-box-refine-two-stage": {
        "code_bleu": [
            {
                "codebleu": 0.14667847455946414
            },
            {
                "ngram_match_score": 0.009795872944210632
            },
            {
                "weighted_ngram_match_score": 0.01956883243043438
            },
            {
                "syntax_match_score": 0.102803738317757
            },
            {
                "dataflow_match_score": 0.45454545454545453
            }
        ],
        "cosine_similarity": 0.9648809432983398,
        "meteor": 0.0977366255144033
    },
    "Musixmatch/umberto-commoncrawl-cased-v1": {
        "code_bleu": [
            {
                "codebleu": 0.2741419748952962
            },
            {
                "ngram_match_score": 0.0013133940665178202
            },
            {
                "weighted_ngram_match_score": 0.0012695431086518281
            },
            {
                "syntax_match_score": 0.40350877192982454
            },
            {
                "dataflow_match_score": 0.6904761904761905
            }
        ],
        "cosine_similarity": 0.9565989971160889,
        "meteor": 0.003720238095238096
    },
    "deepmind/language-perceiver": {
        "code_bleu": [
            {
                "codebleu": 0.21386041117880442
            },
            {
                "ngram_match_score": 0.003947298124297246
            },
            {
                "weighted_ngram_match_score": 0.009153126167508283
            },
            {
                "syntax_match_score": 0.3150684931506849
            },
            {
                "dataflow_match_score": 0.5272727272727272
            }
        ],
        "cosine_similarity": 0.9729386568069458,
        "meteor": 0.1187782805429864
    },
    "Salesforce/codegen-16B-nl": {
        "code_bleu": [
            {
                "codebleu": 0.25
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9166650176048279,
        "meteor": 0.0
    },
    "openlm-research/open/llama/13b": {
        "code_bleu": [
            {
                "codebleu": 0.1647250881137167
            },
            {
                "ngram_match_score": 0.0038101923016370613
            },
            {
                "weighted_ngram_match_score": 0.006110568316495017
            },
            {
                "syntax_match_score": 0.16326530612244897
            },
            {
                "dataflow_match_score": 0.4857142857142857
            }
        ],
        "cosine_similarity": 0.9421038627624512,
        "meteor": 0.07952286282306163
    },
    "facebook/maskformer-swin-small-coco": {
        "code_bleu": [
            {
                "codebleu": 0.1458135558652719
            },
            {
                "ngram_match_score": 0.0068633656970614135
            },
            {
                "weighted_ngram_match_score": 0.014941582401707338
            },
            {
                "syntax_match_score": 0.10144927536231885
            },
            {
                "dataflow_match_score": 0.46
            }
        ],
        "cosine_similarity": 0.9693456888198853,
        "meteor": 0.030000000000000002
    },
    "Salesforce/codegen-6B-nl": {
        "code_bleu": [
            {
                "codebleu": 0.2618180590506761
            },
            {
                "ngram_match_score": 0.0033442188044387903
            },
            {
                "weighted_ngram_match_score": 0.013909255672186817
            },
            {
                "syntax_match_score": 0.4146341463414634
            },
            {
                "dataflow_match_score": 0.6153846153846154
            }
        ],
        "cosine_similarity": 0.9624208807945251,
        "meteor": 0.11182108626198084
    },
    "google/mobilenet/v1/1.0/224": {
        "code_bleu": [
            {
                "codebleu": 0.15153774892492153
            },
            {
                "ngram_match_score": 0.001061546680426688
            },
            {
                "weighted_ngram_match_score": 0.007686851616662047
            },
            {
                "syntax_match_score": 0.12121212121212122
            },
            {
                "dataflow_match_score": 0.47619047619047616
            }
        ],
        "cosine_similarity": 0.9565103054046631,
        "meteor": 0.05050505050505051
    },
    "stabilityai/stablelm-tuned-alpha-3b": {
        "code_bleu": [
            {
                "codebleu": 0.09636692375353899
            },
            {
                "ngram_match_score": 0.0009111265398361607
            },
            {
                "weighted_ngram_match_score": 0.001267443805884752
            },
            {
                "syntax_match_score": 0.038461538461538464
            },
            {
                "dataflow_match_score": 0.3448275862068966
            }
        ],
        "cosine_similarity": 0.960340142250061,
        "meteor": 0.010700909577314075
    },
    "OpenAssistant/oasst-sft-1-pythia-12b": {
        "code_bleu": [
            {
                "codebleu": 0.19398355077175478
            },
            {
                "ngram_match_score": 0.002199705065815234
            },
            {
                "weighted_ngram_match_score": 0.00279432708103291
            },
            {
                "syntax_match_score": 0.15555555555555556
            },
            {
                "dataflow_match_score": 0.6153846153846154
            }
        ],
        "cosine_similarity": 0.8628813624382019,
        "meteor": 0.03007518796992481
    },
    "allenai/OLMo-7B-hf": {
        "code_bleu": [
            {
                "codebleu": 0.09847154537347586
            },
            {
                "ngram_match_score": 0.007168689024791258
            },
            {
                "weighted_ngram_match_score": 0.015873502699291213
            },
            {
                "syntax_match_score": 0.08823529411764706
            },
            {
                "dataflow_match_score": 0.2826086956521739
            }
        ],
        "cosine_similarity": 0.9576414227485657,
        "meteor": 0.033444816053511704
    },
    "jbilcke-hf/sdxl-cinematic-1": {
        "code_bleu": [
            {
                "codebleu": 0.015
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.06
            }
        ],
        "cosine_similarity": 0.9267757534980774,
        "meteor": 0.0
    },
    "albert/albert-xlarge-v2": {
        "code_bleu": [
            {
                "codebleu": 0.24994993320354092
            },
            {
                "ngram_match_score": 0.00373947059163949
            },
            {
                "weighted_ngram_match_score": 0.0055392327184437835
            },
            {
                "syntax_match_score": 0.44814814814814813
            },
            {
                "dataflow_match_score": 0.5423728813559322
            }
        ],
        "cosine_similarity": 0.9542455673217773,
        "meteor": 0.04372847301951781
    },
    "microsoft/beit-base-finetuned-ade-640-640": {
        "code_bleu": [
            {
                "codebleu": 0.16563725402215518
            },
            {
                "ngram_match_score": 0.005511557399392673
            },
            {
                "weighted_ngram_match_score": 0.01013718914744907
            },
            {
                "syntax_match_score": 0.07547169811320754
            },
            {
                "dataflow_match_score": 0.5714285714285714
            }
        ],
        "cosine_similarity": 0.9513978362083435,
        "meteor": 0.0987070964749536
    },
    "Qwen/Qwen-14B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.2262312184977915
            },
            {
                "ngram_match_score": 0.002145567265098031
            },
            {
                "weighted_ngram_match_score": 0.0027793067260678947
            },
            {
                "syntax_match_score": 0.4
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9662358164787292,
        "meteor": 0.04567814476458187
    },
    "facebook/galactica-125m": {
        "code_bleu": [
            {
                "codebleu": 0.15417348967318906
            },
            {
                "ngram_match_score": 0.031689924603484194
            },
            {
                "weighted_ngram_match_score": 0.04622337030860824
            },
            {
                "syntax_match_score": 0.10227272727272728
            },
            {
                "dataflow_match_score": 0.4365079365079365
            }
        ],
        "cosine_similarity": 0.9685157537460327,
        "meteor": 0.056478360156589816
    },
    "CompVis/stable-diffusion-v1-1": {
        "code_bleu": [
            {
                "codebleu": 0.1252225506360744
            },
            {
                "ngram_match_score": 0.004517047783741061
            },
            {
                "weighted_ngram_match_score": 0.006013677636373615
            },
            {
                "syntax_match_score": 0.3014705882352941
            },
            {
                "dataflow_match_score": 0.18888888888888888
            }
        ],
        "cosine_similarity": 0.9852727651596069,
        "meteor": 0.06290257648953303
    },
    "timm/ViT-SO400M-14-SigLIP": {
        "code_bleu": [
            {
                "codebleu": 0.12460934711843445
            },
            {
                "ngram_match_score": 0.006366920687006029
            },
            {
                "weighted_ngram_match_score": 0.006298110063154542
            },
            {
                "syntax_match_score": 0.25
            },
            {
                "dataflow_match_score": 0.23577235772357724
            }
        ],
        "cosine_similarity": 0.9729760885238647,
        "meteor": 0.07930785868781542
    },
    "valhalla/t5-small-e2e-qg": {
        "code_bleu": [
            {
                "codebleu": 0.2688370872960002
            },
            {
                "ngram_match_score": 0.0019420545110794586
            },
            {
                "weighted_ngram_match_score": 0.005224476491103265
            },
            {
                "syntax_match_score": 0.3181818181818182
            },
            {
                "dataflow_match_score": 0.75
            }
        ],
        "cosine_similarity": 0.8589855432510376,
        "meteor": 0.030211480362537763
    },
    "mosaicml/mpt-30b-chat": {
        "code_bleu": [
            {
                "codebleu": 0.2506827988488436
            },
            {
                "ngram_match_score": 0.0007204055584839955
            },
            {
                "weighted_ngram_match_score": 0.0020107898368904634
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9608199596405029,
        "meteor": 0.034324942791762014
    },
    "warp-ai/wuerstchen": {
        "code_bleu": [
            {
                "codebleu": 0.20133667731430763
            },
            {
                "ngram_match_score": 0.005143218240442063
            },
            {
                "weighted_ngram_match_score": 0.014489205302502743
            },
            {
                "syntax_match_score": 0.2857142857142857
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9596332311630249,
        "meteor": 0.13517375400091453
    },
    "facebook/convnext-base-224-22k": {
        "code_bleu": [
            {
                "codebleu": 0.19890763067233685
            },
            {
                "ngram_match_score": 0.00500229325209247
            },
            {
                "weighted_ngram_match_score": 0.012408833570800295
            },
            {
                "syntax_match_score": 0.2647058823529412
            },
            {
                "dataflow_match_score": 0.5135135135135135
            }
        ],
        "cosine_similarity": 0.9538586139678955,
        "meteor": 0.11316872427983543
    },
    "facebook/regnet-y-040": {
        "code_bleu": [
            {
                "codebleu": 0.2003482696449912
            },
            {
                "ngram_match_score": 0.0025672405984320337
            },
            {
                "weighted_ngram_match_score": 0.005120740478966813
            },
            {
                "syntax_match_score": 0.25316455696202533
            },
            {
                "dataflow_match_score": 0.5405405405405406
            }
        ],
        "cosine_similarity": 0.9456750154495239,
        "meteor": 0.06746626686656672
    },
    "Musixmatch/umberto-wikipedia-uncased-v1": {
        "code_bleu": [
            {
                "codebleu": 0.19723428032968565
            },
            {
                "ngram_match_score": 0.0022770552699531866
            },
            {
                "weighted_ngram_match_score": 0.002198913166583901
            },
            {
                "syntax_match_score": 0.40350877192982454
            },
            {
                "dataflow_match_score": 0.38095238095238093
            }
        ],
        "cosine_similarity": 0.9466419219970703,
        "meteor": 0.03345724907063197
    },
    "CohereForAI/c4ai-command-r-plus-4bit": {
        "code_bleu": [
            {
                "codebleu": 0.015207582242745337
            },
            {
                "ngram_match_score": 5.0188116065944714e-15
            },
            {
                "weighted_ngram_match_score": 0.001433770194764949
            },
            {
                "syntax_match_score": 0.04608294930875576
            },
            {
                "dataflow_match_score": 0.013313609467455622
            }
        ],
        "cosine_similarity": 0.9475274085998535,
        "meteor": 0.007247689798876607
    },
    "nghuyong/ernie-1.0-base-zh": {
        "code_bleu": [
            {
                "codebleu": 0.15975409710901997
            },
            {
                "ngram_match_score": 0.0018153132360887014
            },
            {
                "weighted_ngram_match_score": 0.019554016376461803
            },
            {
                "syntax_match_score": 0.11764705882352941
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9132773876190186,
        "meteor": 0.04608294930875577
    },
    "microsoft/swin-base-patch4-window12-384-in22k": {
        "code_bleu": [
            {
                "codebleu": 0.2208294888757894
            },
            {
                "ngram_match_score": 0.002460866376712175
            },
            {
                "weighted_ngram_match_score": 0.006398214667571038
            },
            {
                "syntax_match_score": 0.30303030303030304
            },
            {
                "dataflow_match_score": 0.5714285714285714
            }
        ],
        "cosine_similarity": 0.9415823817253113,
        "meteor": 0.10398613518197573
    },
    "internlm/internlm-7b": {
        "code_bleu": [
            {
                "codebleu": 0.2000704326462298
            },
            {
                "ngram_match_score": 0.03134249539935859
            },
            {
                "weighted_ngram_match_score": 0.040411914826279254
            },
            {
                "syntax_match_score": 0.4550898203592814
            },
            {
                "dataflow_match_score": 0.2734375
            }
        ],
        "cosine_similarity": 0.9817279577255249,
        "meteor": 0.10236604670107313
    },
    "stabilityai/stablelm-tuned-alpha-7b": {
        "code_bleu": [
            {
                "codebleu": 0.20005279399377243
            },
            {
                "ngram_match_score": 0.009076917142420121
            },
            {
                "weighted_ngram_match_score": 0.010762906047523755
            },
            {
                "syntax_match_score": 0.17692307692307693
            },
            {
                "dataflow_match_score": 0.603448275862069
            }
        ],
        "cosine_similarity": 0.9876089692115784,
        "meteor": 0.09125443655723159
    },
    "Salesforce/codet5-large": {
        "code_bleu": [
            {
                "codebleu": 0.3590048408215648
            },
            {
                "ngram_match_score": 0.02029992367669014
            },
            {
                "weighted_ngram_match_score": 0.09958435518180171
            },
            {
                "syntax_match_score": 0.5853658536585366
            },
            {
                "dataflow_match_score": 0.7307692307692307
            }
        ],
        "cosine_similarity": 0.9609752893447876,
        "meteor": 0.22718145064124876
    },
    "allenai/led-large-16384-arxiv": {
        "code_bleu": [
            {
                "codebleu": 0.1247260778855515
            },
            {
                "ngram_match_score": 4.197912743406083e-37
            },
            {
                "weighted_ngram_match_score": 6.710223988040534e-05
            },
            {
                "syntax_match_score": 0.15
            },
            {
                "dataflow_match_score": 0.3488372093023256
            }
        ],
        "cosine_similarity": 0.869056224822998,
        "meteor": 0.0016053183251808342
    },
    "microsoft/beit-base-patch16-224-pt22k": {
        "code_bleu": [
            {
                "codebleu": 0.07376859913428524
            },
            {
                "ngram_match_score": 0.026653834710917628
            },
            {
                "weighted_ngram_match_score": 0.03413484754050907
            },
            {
                "syntax_match_score": 0.12
            },
            {
                "dataflow_match_score": 0.11428571428571428
            }
        ],
        "cosine_similarity": 0.9433609247207642,
        "meteor": 0.05830903790087464
    },
    "google/bigbird-roberta-large": {
        "code_bleu": [
            {
                "codebleu": 0.14810379632758086
            },
            {
                "ngram_match_score": 0.002327479199664437
            },
            {
                "weighted_ngram_match_score": 0.004742878524452112
            },
            {
                "syntax_match_score": 0.275
            },
            {
                "dataflow_match_score": 0.3103448275862069
            }
        ],
        "cosine_similarity": 0.9687193036079407,
        "meteor": 0.05891016200294552
    },
    "albert-xlarge-v2": {
        "code_bleu": [
            {
                "codebleu": 0.22219388732867268
            },
            {
                "ngram_match_score": 0.001924181993397457
            },
            {
                "weighted_ngram_match_score": 0.002293928526566219
            },
            {
                "syntax_match_score": 0.4777777777777778
            },
            {
                "dataflow_match_score": 0.4067796610169492
            }
        ],
        "cosine_similarity": 0.9781933426856995,
        "meteor": 0.041952707856598014
    },
    "google/efficientnet-b7": {
        "code_bleu": [
            {
                "codebleu": 0.19213346901503287
            },
            {
                "ngram_match_score": 0.006594531217261788
            },
            {
                "weighted_ngram_match_score": 0.005977500645731284
            },
            {
                "syntax_match_score": 0.3235294117647059
            },
            {
                "dataflow_match_score": 0.43243243243243246
            }
        ],
        "cosine_similarity": 0.9803882837295532,
        "meteor": 0.07900677200902935
    },
    "stabilityai/StableBeluga2": {
        "code_bleu": [
            {
                "codebleu": 0.3000607907636412
            },
            {
                "ngram_match_score": 0.0024738180584429116
            },
            {
                "weighted_ngram_match_score": 0.003891793975713793
            },
            {
                "syntax_match_score": 0.19387755102040816
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9535019397735596,
        "meteor": 0.07914338919925511
    },
    "bofenghuang/vigogne-2-7b-chat": {
        "code_bleu": [
            {
                "codebleu": 0.2658975651910561
            },
            {
                "ngram_match_score": 4.2903877015909255e-05
            },
            {
                "weighted_ngram_match_score": 0.007367581606309587
            },
            {
                "syntax_match_score": 0.056179775280898875
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9411443471908569,
        "meteor": 0.018952446588559616
    },
    "albert-large-v1": {
        "code_bleu": [
            {
                "codebleu": 0.22221462730633462
            },
            {
                "ngram_match_score": 0.0034141400264543743
            },
            {
                "weighted_ngram_match_score": 0.006160000084006565
            },
            {
                "syntax_match_score": 0.45555555555555555
            },
            {
                "dataflow_match_score": 0.423728813559322
            }
        ],
        "cosine_similarity": 0.9404406547546387,
        "meteor": 0.04914926527455529
    },
    "google/mobilenet/v2/1.4/224": {
        "code_bleu": [
            {
                "codebleu": 0.17493778253224557
            },
            {
                "ngram_match_score": 0.0025862165093628294
            },
            {
                "weighted_ngram_match_score": 0.004524220978926712
            },
            {
                "syntax_match_score": 0.2878787878787879
            },
            {
                "dataflow_match_score": 0.40476190476190477
            }
        ],
        "cosine_similarity": 0.9419385194778442,
        "meteor": 0.02814258911819887
    },
    "google-bert/bert-large-cased-whole-word-masking": {
        "code_bleu": [
            {
                "codebleu": 0.05370647332827838
            },
            {
                "ngram_match_score": 0.0008182762352674497
            },
            {
                "weighted_ngram_match_score": 0.02656254488701116
            },
            {
                "syntax_match_score": 0.05185185185185185
            },
            {
                "dataflow_match_score": 0.13559322033898305
            }
        ],
        "cosine_similarity": 0.9252474308013916,
        "meteor": 0.04530557645681799
    },
    "weiweishi/roc-bert-base-zh": {
        "code_bleu": [
            {
                "codebleu": 0.25
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9745132923126221,
        "meteor": 0.0
    },
    "internlm/internlm-chat-7b": {
        "code_bleu": [
            {
                "codebleu": 0.12644731244146973
            },
            {
                "ngram_match_score": 0.0036328156784007594
            },
            {
                "weighted_ngram_match_score": 0.0051635710258117655
            },
            {
                "syntax_match_score": 0.2679425837320574
            },
            {
                "dataflow_match_score": 0.22905027932960895
            }
        ],
        "cosine_similarity": 0.9455966353416443,
        "meteor": 0.0490621996885045
    },
    "deepmind/vision-perceiver-learned": {
        "code_bleu": [
            {
                "codebleu": 0.12857906144878556
            },
            {
                "ngram_match_score": 0.0025963742211370646
            },
            {
                "weighted_ngram_match_score": 0.004621218497922112
            },
            {
                "syntax_match_score": 0.23880597014925373
            },
            {
                "dataflow_match_score": 0.2682926829268293
            }
        ],
        "cosine_similarity": 0.962020754814148,
        "meteor": 0.017985611510791366
    },
    "codeparrot/codeparrot-small": {
        "code_bleu": [
            {
                "codebleu": 0.20666457922559356
            },
            {
                "ngram_match_score": 0.0050472763309428875
            },
            {
                "weighted_ngram_match_score": 0.022237999819080197
            },
            {
                "syntax_match_score": 0.45454545454545453
            },
            {
                "dataflow_match_score": 0.3448275862068966
            }
        ],
        "cosine_similarity": 0.9422646760940552,
        "meteor": 0.15387409200968521
    },
    "mosaicml/mpt-7b-8k-chat": {
        "code_bleu": [
            {
                "codebleu": 0.2765146164016118
            },
            {
                "ngram_match_score": 0.0007932360189256089
            },
            {
                "weighted_ngram_match_score": 0.001816953725452536
            },
            {
                "syntax_match_score": 0.10344827586206896
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9508936405181885,
        "meteor": 0.022813688212927757
    },
    "stabilityai/stable-diffusion-xl-base-0.9": {
        "code_bleu": [
            {
                "codebleu": 0.14359573735646225
            },
            {
                "ngram_match_score": 0.009303926858598736
            },
            {
                "weighted_ngram_match_score": 0.04519265893088658
            },
            {
                "syntax_match_score": 0.15625
            },
            {
                "dataflow_match_score": 0.36363636363636365
            }
        ],
        "cosine_similarity": 0.9237282276153564,
        "meteor": 0.08454106280193237
    },
    "bigscience/bloomz": {
        "code_bleu": [
            {
                "codebleu": 0.10582092155563802
            },
            {
                "ngram_match_score": 0.010857254460183376
            },
            {
                "weighted_ngram_match_score": 0.021641946489694468
            },
            {
                "syntax_match_score": 0.09448818897637795
            },
            {
                "dataflow_match_score": 0.2962962962962963
            }
        ],
        "cosine_similarity": 0.9702163338661194,
        "meteor": 0.03816793893129771
    },
    "Salesforce/codegen-6B-multi": {
        "code_bleu": [
            {
                "codebleu": 0.18361529800099546
            },
            {
                "ngram_match_score": 0.006132910803680854
            },
            {
                "weighted_ngram_match_score": 0.0125684312941096
            },
            {
                "syntax_match_score": 0.2926829268292683
            },
            {
                "dataflow_match_score": 0.4230769230769231
            }
        ],
        "cosine_similarity": 0.9718340039253235,
        "meteor": 0.057471264367816084
    },
    "HuggingFaceH4/starchat-beta": {
        "code_bleu": [
            {
                "codebleu": 0.13049714407173804
            },
            {
                "ngram_match_score": 0.00459301965775427
            },
            {
                "weighted_ngram_match_score": 0.006559643316504387
            },
            {
                "syntax_match_score": 0.15789473684210525
            },
            {
                "dataflow_match_score": 0.35294117647058826
            }
        ],
        "cosine_similarity": 0.9734129905700684,
        "meteor": 0.0819672131147541
    },
    "RWKV/rwkv-4-430m-pile": {
        "code_bleu": [
            {
                "codebleu": 0.126305841665668
            },
            {
                "ngram_match_score": 0.0037199430231739895
            },
            {
                "weighted_ngram_match_score": 0.007797624488154198
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.9774574041366577,
        "meteor": 0.06066983320792575
    },
    "albert-xlarge-v1": {
        "code_bleu": [
            {
                "codebleu": 0.17856764752071866
            },
            {
                "ngram_match_score": 0.0006431184649350867
            },
            {
                "weighted_ngram_match_score": 0.0011353184478202533
            },
            {
                "syntax_match_score": 0.4074074074074074
            },
            {
                "dataflow_match_score": 0.3050847457627119
            }
        ],
        "cosine_similarity": 0.9682657122612,
        "meteor": 0.02324680356450988
    },
    "timm/ViT-L-16-SigLIP-384": {
        "code_bleu": [
            {
                "codebleu": 0.07638118994826312
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.10227272727272728
            },
            {
                "dataflow_match_score": 0.2032520325203252
            }
        ],
        "cosine_similarity": 0.9495348930358887,
        "meteor": 0.0
    },
    "baichuan-inc/Baichuan2-13B-Base": {
        "code_bleu": [
            {
                "codebleu": 0.1831896855179786
            },
            {
                "ngram_match_score": 0.0021976303252009962
            },
            {
                "weighted_ngram_match_score": 0.007315497711625645
            },
            {
                "syntax_match_score": 0.2982456140350877
            },
            {
                "dataflow_match_score": 0.425
            }
        ],
        "cosine_similarity": 0.9457635879516602,
        "meteor": 0.06459948320413438
    },
    "togethercomputer/RedPajama-INCITE-7B-Chat": {
        "code_bleu": [
            {
                "codebleu": 0.17821306613313825
            },
            {
                "ngram_match_score": 0.0059772847566062215
            },
            {
                "weighted_ngram_match_score": 0.009780633548045568
            },
            {
                "syntax_match_score": 0.3620689655172414
            },
            {
                "dataflow_match_score": 0.3350253807106599
            }
        ],
        "cosine_similarity": 0.9610987305641174,
        "meteor": 0.059089639208173686
    },
    "microsoft/swin-large-patch4-window7-224": {
        "code_bleu": [
            {
                "codebleu": 0.19372294372294374
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.22727272727272727
            },
            {
                "dataflow_match_score": 0.5476190476190477
            }
        ],
        "cosine_similarity": 0.9525842666625977,
        "meteor": 0.008488964346349744
    },
    "RWKV/rwkv-4-7b-pile": {
        "code_bleu": [
            {
                "codebleu": 0.12636876190400637
            },
            {
                "ngram_match_score": 0.0039716239765275945
            },
            {
                "weighted_ngram_match_score": 0.007797624488154198
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.9773945212364197,
        "meteor": 0.06056046892994843
    },
    "RWKV/rwkv-raven-3b": {
        "code_bleu": [
            {
                "codebleu": 0.126305841665668
            },
            {
                "ngram_match_score": 0.0037199430231739895
            },
            {
                "weighted_ngram_match_score": 0.007797624488154198
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.9763002395629883,
        "meteor": 0.06066983320792575
    },
    "facebook/convnext-base-224": {
        "code_bleu": [
            {
                "codebleu": 0.19204942974020855
            },
            {
                "ngram_match_score": 0.0044935253117907295
            },
            {
                "weighted_ngram_match_score": 0.014896562488471246
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.5135135135135135
            }
        ],
        "cosine_similarity": 0.9616672992706299,
        "meteor": 0.1633435270132518
    },
    "RWKV/rwkv-raven-7b": {
        "code_bleu": [
            {
                "codebleu": 0.12560786273114163
            },
            {
                "ngram_match_score": 0.00185098729441778
            },
            {
                "weighted_ngram_match_score": 0.006874664478805066
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.973237156867981,
        "meteor": 0.035967049541710164
    },
    "RWKV/rwkv-4-3b-pile": {
        "code_bleu": [
            {
                "codebleu": 0.126305841665668
            },
            {
                "ngram_match_score": 0.0037199430231739895
            },
            {
                "weighted_ngram_match_score": 0.007797624488154198
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.9776196479797363,
        "meteor": 0.06066983320792575
    },
    "Salesforce/codegen-350M-nl": {
        "code_bleu": [
            {
                "codebleu": 0.09615384615384616
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.38461538461538464
            }
        ],
        "cosine_similarity": 0.9312989711761475,
        "meteor": 0.013054830287206267
    },
    "RWKV/rwkv-4-14b-pile": {
        "code_bleu": [
            {
                "codebleu": 0.12579723158857214
            },
            {
                "ngram_match_score": 0.0026084627241398594
            },
            {
                "weighted_ngram_match_score": 0.006874664478805066
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.9765262603759766,
        "meteor": 0.04109809604315099
    },
    "facebook/incoder-1B": {
        "code_bleu": [
            {
                "codebleu": 0.375
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 1.0
            }
        ],
        "cosine_similarity": 0.8753378391265869,
        "meteor": 0.0
    },
    "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt": {
        "code_bleu": [
            {
                "codebleu": 0.04861111111111111
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.19444444444444445
            }
        ],
        "cosine_similarity": 0.9773516058921814,
        "meteor": 0.0
    },
    "RWKV/rwkv-raven-1b5": {
        "code_bleu": [
            {
                "codebleu": 0.126305841665668
            },
            {
                "ngram_match_score": 0.0037199430231739895
            },
            {
                "weighted_ngram_match_score": 0.007797624488154198
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.9769090414047241,
        "meteor": 0.06066983320792575
    },
    "microsoft/swin-large-patch4-window7-224-in22k": {
        "code_bleu": [
            {
                "codebleu": 0.21523851647639294
            },
            {
                "ngram_match_score": 0.0025485885084218793
            },
            {
                "weighted_ngram_match_score": 0.005591624583297085
            },
            {
                "syntax_match_score": 0.25757575757575757
            },
            {
                "dataflow_match_score": 0.5952380952380952
            }
        ],
        "cosine_similarity": 0.9582220911979675,
        "meteor": 0.06294964028776978
    },
    "RWKV/rwkv-4-1b5-pile": {
        "code_bleu": [
            {
                "codebleu": 0.126305841665668
            },
            {
                "ngram_match_score": 0.0037199430231739895
            },
            {
                "weighted_ngram_match_score": 0.007797624488154198
            },
            {
                "syntax_match_score": 0.2722772277227723
            },
            {
                "dataflow_match_score": 0.22142857142857142
            }
        ],
        "cosine_similarity": 0.9780638217926025,
        "meteor": 0.06066983320792575
    },
    "llmware/bling-1b-0.1": {
        "code_bleu": [
            {
                "codebleu": 0.175
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.7
            }
        ],
        "cosine_similarity": 0.9182178974151611,
        "meteor": 0.0
    },
    "deepmind/vision-perceiver-fourier": {
        "code_bleu": [
            {
                "codebleu": 0.3072065554264637
            },
            {
                "ngram_match_score": 0.03330970509257108
            },
            {
                "weighted_ngram_match_score": 0.10833702943379647
            },
            {
                "syntax_match_score": 0.4461538461538462
            },
            {
                "dataflow_match_score": 0.6410256410256411
            }
        ],
        "cosine_similarity": 0.9329946041107178,
        "meteor": 0.1812191103789127
    },
    "bigcode/starcoderbase": {
        "code_bleu": [
            {
                "codebleu": 0.18964118994329177
            },
            {
                "ngram_match_score": 0.005975777802532673
            },
            {
                "weighted_ngram_match_score": 0.012983992365644795
            },
            {
                "syntax_match_score": 0.2972972972972973
            },
            {
                "dataflow_match_score": 0.4423076923076923
            }
        ],
        "cosine_similarity": 0.9699050784111023,
        "meteor": 0.104
    },
    "llava-hf/bakLlava-v1-hf": {
        "code_bleu": [
            {
                "codebleu": 0.18700671903734525
            },
            {
                "ngram_match_score": 0.005058036179817766
            },
            {
                "weighted_ngram_match_score": 0.0058259828267061085
            },
            {
                "syntax_match_score": 0.45714285714285713
            },
            {
                "dataflow_match_score": 0.28
            }
        ],
        "cosine_similarity": 0.9890658855438232,
        "meteor": 0.09035749888452652
    },
    "Intel/dpt-large-ade": {
        "code_bleu": [
            {
                "codebleu": 0.10984733720703216
            },
            {
                "ngram_match_score": 0.010535320113731145
            },
            {
                "weighted_ngram_match_score": 0.011908032438792286
            },
            {
                "syntax_match_score": 0.36666666666666664
            },
            {
                "dataflow_match_score": 0.05027932960893855
            }
        ],
        "cosine_similarity": 0.9742933511734009,
        "meteor": 0.09154929577464789
    },
    "nlp-waseda/roberta-base-japanese": {
        "code_bleu": [
            {
                "codebleu": 0.11764705882352941
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.47058823529411764
            }
        ],
        "cosine_similarity": 0.9056681394577026,
        "meteor": 0.0
    },
    "Salesforce/codegen-2B-mono": {
        "code_bleu": [
            {
                "codebleu": 0.15778285672998693
            },
            {
                "ngram_match_score": 0.0023638038029330957
            },
            {
                "weighted_ngram_match_score": 0.010568748820579371
            },
            {
                "syntax_match_score": 0.1951219512195122
            },
            {
                "dataflow_match_score": 0.4230769230769231
            }
        ],
        "cosine_similarity": 0.9025970697402954,
        "meteor": 0.07374631268436578
    },
    "MBZUAI/swiftformer-xs": {
        "code_bleu": [
            {
                "codebleu": 0.26743359109314824
            },
            {
                "ngram_match_score": 0.009271574349074793
            },
            {
                "weighted_ngram_match_score": 0.014566133488563688
            },
            {
                "syntax_match_score": 0.4714285714285714
            },
            {
                "dataflow_match_score": 0.574468085106383
            }
        ],
        "cosine_similarity": 0.9668335914611816,
        "meteor": 0.12684989429175475
    },
    "google/deeplabv3/mobilenet/v2/1.0/513": {
        "code_bleu": [
            {
                "codebleu": 0.027777777777777776
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.1111111111111111
            }
        ],
        "cosine_similarity": 0.9718666076660156,
        "meteor": 0.0
    },
    "flaubert/flaubert/large/cased": {
        "code_bleu": [
            {
                "codebleu": 0.14572390936697993
            },
            {
                "ngram_match_score": 0.005067419040833786
            },
            {
                "weighted_ngram_match_score": 0.00699988709455288
            },
            {
                "syntax_match_score": 0.30612244897959184
            },
            {
                "dataflow_match_score": 0.2647058823529412
            }
        ],
        "cosine_similarity": 0.9587967395782471,
        "meteor": 0.08438818565400844
    },
    "CompVis/ldm-text2im-large-256": {
        "code_bleu": [
            {
                "codebleu": 0.2208032871501834
            },
            {
                "ngram_match_score": 0.0016315280664606425
            },
            {
                "weighted_ngram_match_score": 0.007295906248558736
            },
            {
                "syntax_match_score": 0.11428571428571428
            },
            {
                "dataflow_match_score": 0.76
            }
        ],
        "cosine_similarity": 0.9638699293136597,
        "meteor": 0.08633093525179857
    },
    "sijunhe/nezha-cn-base": {
        "code_bleu": [
            {
                "codebleu": 0.255906211311957
            },
            {
                "ngram_match_score": 0.013034044634357492
            },
            {
                "weighted_ngram_match_score": 0.058977897387664006
            },
            {
                "syntax_match_score": 0.45161290322580644
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9377717971801758,
        "meteor": 0.13986013986013987
    },
    "tiiuae/falcon-180B-chat": {
        "code_bleu": [
            {
                "codebleu": 0.1578869445178822
            },
            {
                "ngram_match_score": 0.004898638184128441
            },
            {
                "weighted_ngram_match_score": 0.010402641287960581
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.38095238095238093
            }
        ],
        "cosine_similarity": 0.9681529402732849,
        "meteor": 0.09293680297397768
    },
    "microsoft/focalnet-base": {
        "code_bleu": [
            {
                "codebleu": 0.15557028320225824
            },
            {
                "ngram_match_score": 0.00223367985805522
            },
            {
                "weighted_ngram_match_score": 0.005977500645731284
            },
            {
                "syntax_match_score": 0.07352941176470588
            },
            {
                "dataflow_match_score": 0.5405405405405406
            }
        ],
        "cosine_similarity": 0.910249650478363,
        "meteor": 0.0746268656716418
    },
    "MAGAer13/mplug-owl-llama-7b": {
        "code_bleu": [
            {
                "codebleu": 0.201219698673306
            },
            {
                "ngram_match_score": 0.006947404612776045
            },
            {
                "weighted_ngram_match_score": 0.007099340773822335
            },
            {
                "syntax_match_score": 0.3135593220338983
            },
            {
                "dataflow_match_score": 0.4772727272727273
            }
        ],
        "cosine_similarity": 0.9900524616241455,
        "meteor": 0.10236586385314148
    },
    "Zetatech/pvt-tiny-224": {
        "code_bleu": [
            {
                "codebleu": 0.023809523809523808
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.09523809523809523
            }
        ],
        "cosine_similarity": 0.9651563763618469,
        "meteor": 0.0
    },
    "bigscience/bloomz-1b7": {
        "code_bleu": [
            {
                "codebleu": 0.2073251565186399
            },
            {
                "ngram_match_score": 0.008075347095609956
            },
            {
                "weighted_ngram_match_score": 0.012170160868713411
            },
            {
                "syntax_match_score": 0.5590551181102362
            },
            {
                "dataflow_match_score": 0.25
            }
        ],
        "cosine_similarity": 0.9675533175468445,
        "meteor": 0.15146125728765442
    },
    "CompVis/stable-diffusion-v1-2": {
        "code_bleu": [
            {
                "codebleu": 0.11840918915977716
            },
            {
                "ngram_match_score": 0.0020171228681429786
            },
            {
                "weighted_ngram_match_score": 0.002992182790573486
            },
            {
                "syntax_match_score": 0.23529411764705882
            },
            {
                "dataflow_match_score": 0.23333333333333334
            }
        ],
        "cosine_similarity": 0.9765235781669617,
        "meteor": 0.06562756357670221
    },
    "SenseTime/deformable-detr-with-box-refine": {
        "code_bleu": [
            {
                "codebleu": 0.054878131293645896
            },
            {
                "ngram_match_score": 0.0031810113370946613
            },
            {
                "weighted_ngram_match_score": 0.0042103017162767944
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.21212121212121213
            }
        ],
        "cosine_similarity": 0.9519699811935425,
        "meteor": 0.033707865168539325
    },
    "Salesforce/codegen-2B-multi": {
        "code_bleu": [
            {
                "codebleu": 0.16650589669574822
            },
            {
                "ngram_match_score": 0.0037638105699971244
            },
            {
                "weighted_ngram_match_score": 0.01967065801412149
            },
            {
                "syntax_match_score": 0.21951219512195122
            },
            {
                "dataflow_match_score": 0.4230769230769231
            }
        ],
        "cosine_similarity": 0.9059379696846008,
        "meteor": 0.08379888268156424
    },
    "xlm-roberta-large-finetuned-conll02-dutch": {
        "code_bleu": [
            {
                "codebleu": 0.17817673235727763
            },
            {
                "ngram_match_score": 0.005771098608257974
            },
            {
                "weighted_ngram_match_score": 0.007142870158326681
            },
            {
                "syntax_match_score": 0.3188405797101449
            },
            {
                "dataflow_match_score": 0.38095238095238093
            }
        ],
        "cosine_similarity": 0.9380307197570801,
        "meteor": 0.051741803278688527
    },
    "cerebras/btlm-3b-8k-base": {
        "code_bleu": [
            {
                "codebleu": 0.13989083542188152
            },
            {
                "ngram_match_score": 0.004297363157007544
            },
            {
                "weighted_ngram_match_score": 0.004916328180868147
            },
            {
                "syntax_match_score": 0.3230769230769231
            },
            {
                "dataflow_match_score": 0.22727272727272727
            }
        ],
        "cosine_similarity": 0.9692158699035645,
        "meteor": 0.08404514972173376
    },
    "shi-labs/versatile-diffusion": {
        "code_bleu": [
            {
                "codebleu": 0.13458044548238202
            },
            {
                "ngram_match_score": 0.016357127136844677
            },
            {
                "weighted_ngram_match_score": 0.022997949855829182
            },
            {
                "syntax_match_score": 0.2835820895522388
            },
            {
                "dataflow_match_score": 0.2153846153846154
            }
        ],
        "cosine_similarity": 0.9673014283180237,
        "meteor": 0.04614941535625828
    },
    "DeepFloyd/IF-I-L-v1.0": {
        "code_bleu": [
            {
                "codebleu": 0.28988480718934717
            },
            {
                "ngram_match_score": 0.012745292747900339
            },
            {
                "weighted_ngram_match_score": 0.019809809025361346
            },
            {
                "syntax_match_score": 0.5555555555555556
            },
            {
                "dataflow_match_score": 0.5714285714285714
            }
        ],
        "cosine_similarity": 0.9729383587837219,
        "meteor": 0.08793969849246232
    },
    "microsoft/swin-small-patch4-window7-224": {
        "code_bleu": [
            {
                "codebleu": 0.4732339812736577
            },
            {
                "ngram_match_score": 0.14498636045186483
            },
            {
                "weighted_ngram_match_score": 0.32370714040034154
            },
            {
                "syntax_match_score": 0.42424242424242425
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.957951545715332,
        "meteor": 0.41165616847998987
    },
    "apple/deeplabv3-mobilevit-xx-small": {
        "code_bleu": [
            {
                "codebleu": 0.344438792702747
            },
            {
                "ngram_match_score": 0.08195941367624678
            },
            {
                "weighted_ngram_match_score": 0.2104299034762046
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 0.5853658536585366
            }
        ],
        "cosine_similarity": 0.9618571996688843,
        "meteor": 0.22935779816513763
    },
    "CompVis/stable-diffusion-v1-3": {
        "code_bleu": [
            {
                "codebleu": 0.11763476497334857
            },
            {
                "ngram_match_score": 0.0022731919884422607
            },
            {
                "weighted_ngram_match_score": 0.0026993810753741174
            },
            {
                "syntax_match_score": 0.24817518248175183
            },
            {
                "dataflow_match_score": 0.21739130434782608
            }
        ],
        "cosine_similarity": 0.981983482837677,
        "meteor": 0.04201680672268907
    },
    "microsoft/beit-large-patch16-224": {
        "code_bleu": [
            {
                "codebleu": 0.3202265832618303
            },
            {
                "ngram_match_score": 0.07878658413046957
            },
            {
                "weighted_ngram_match_score": 0.13934918614628888
            },
            {
                "syntax_match_score": 0.5151515151515151
            },
            {
                "dataflow_match_score": 0.5476190476190477
            }
        ],
        "cosine_similarity": 0.9627465605735779,
        "meteor": 0.33408239700374537
    },
    "jpwahle/longformer-base-plagiarism-detection": {
        "code_bleu": [
            {
                "codebleu": 0.2
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.8
            }
        ],
        "cosine_similarity": 0.9374597668647766,
        "meteor": 0.0
    },
    "microsoft/beit-large-patch16-224-pt22k": {
        "code_bleu": [
            {
                "codebleu": 0.15730990778620982
            },
            {
                "ngram_match_score": 0.05827830530853596
            },
            {
                "weighted_ngram_match_score": 0.12524704012201765
            },
            {
                "syntax_match_score": 0.16
            },
            {
                "dataflow_match_score": 0.2857142857142857
            }
        ],
        "cosine_similarity": 0.9781825542449951,
        "meteor": 0.13661202185792348
    },
    "valhalla/bart-large-finetuned-squadv1": {
        "code_bleu": [
            {
                "codebleu": 0.2611914124912001
            },
            {
                "ngram_match_score": 0.015370430315768692
            },
            {
                "weighted_ngram_match_score": 0.04059074144031508
            },
            {
                "syntax_match_score": 0.36585365853658536
            },
            {
                "dataflow_match_score": 0.6229508196721312
            }
        ],
        "cosine_similarity": 0.9593842029571533,
        "meteor": 0.22187786960514233
    },
    "HuggingFaceM4/idefics-80b": {
        "code_bleu": [
            {
                "codebleu": 0.1229852273128201
            },
            {
                "ngram_match_score": 0.0018051117968363119
            },
            {
                "weighted_ngram_match_score": 0.004017255156126527
            },
            {
                "syntax_match_score": 0.32209737827715357
            },
            {
                "dataflow_match_score": 0.164021164021164
            }
        ],
        "cosine_similarity": 0.9744051694869995,
        "meteor": 0.05828329212292476
    },
    "kakaobrain/karlo-v1-alpha-image-variations": {
        "code_bleu": [
            {
                "codebleu": 0.2385842527106971
            },
            {
                "ngram_match_score": 0.006966867766849182
            },
            {
                "weighted_ngram_match_score": 0.011904064752717958
            },
            {
                "syntax_match_score": 0.4864864864864865
            },
            {
                "dataflow_match_score": 0.4489795918367347
            }
        ],
        "cosine_similarity": 0.9539370536804199,
        "meteor": 0.0847457627118644
    },
    "MediaTek-Research/Breeze-7B-Base-v1/0": {
        "code_bleu": [
            {
                "codebleu": 0.15550766279194173
            },
            {
                "ngram_match_score": 0.0013440296812825467
            },
            {
                "weighted_ngram_match_score": 0.0021008893212778874
            },
            {
                "syntax_match_score": 0.07446808510638298
            },
            {
                "dataflow_match_score": 0.5441176470588235
            }
        ],
        "cosine_similarity": 0.9557845592498779,
        "meteor": 0.025974025974025976
    },
    "julien-c/EsperBERTo-small": {
        "code_bleu": [
            {
                "codebleu": 0.08333333333333333
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.3333333333333333
            }
        ],
        "cosine_similarity": 0.9577489495277405,
        "meteor": 0.0
    },
    "google/bigbird-base-trivia-itc": {
        "code_bleu": [
            {
                "codebleu": 0.2411075948126984
            },
            {
                "ngram_match_score": 0.004844371292879106
            },
            {
                "weighted_ngram_match_score": 0.009731356795123748
            },
            {
                "syntax_match_score": 0.4186046511627907
            },
            {
                "dataflow_match_score": 0.53125
            }
        ],
        "cosine_similarity": 0.970522403717041,
        "meteor": 0.08064516129032258
    },
    "OpenAssistant/reward-model-deberta-v3-large": {
        "code_bleu": [
            {
                "codebleu": 0.27483059754869454
            },
            {
                "ngram_match_score": 0.012673268980131519
            },
            {
                "weighted_ngram_match_score": 0.019982454547980158
            },
            {
                "syntax_match_score": 0.5
            },
            {
                "dataflow_match_score": 0.5666666666666667
            }
        ],
        "cosine_similarity": 0.9559439420700073,
        "meteor": 0.09804209495839453
    },
    "xlm-clm-enfr-1024": {
        "code_bleu": [
            {
                "codebleu": 0.24749729182617658
            },
            {
                "ngram_match_score": 0.0033115124598129906
            },
            {
                "weighted_ngram_match_score": 0.005196173363411874
            },
            {
                "syntax_match_score": 0.48148148148148145
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9350467920303345,
        "meteor": 0.0738255033557047
    },
    "facebook/levit-128S": {
        "code_bleu": [
            {
                "codebleu": 0.24611867527807813
            },
            {
                "ngram_match_score": 0.002502785328693211
            },
            {
                "weighted_ngram_match_score": 0.005781439593143088
            },
            {
                "syntax_match_score": 0.3333333333333333
            },
            {
                "dataflow_match_score": 0.6428571428571429
            }
        ],
        "cosine_similarity": 0.9410978555679321,
        "meteor": 0.0711743772241993
    },
    "microsoft/swin-large-patch4-window12-384": {
        "code_bleu": [
            {
                "codebleu": 0.2012132371386373
            },
            {
                "ngram_match_score": 0.004943587741212107
            },
            {
                "weighted_ngram_match_score": 0.012030572934549188
            },
            {
                "syntax_match_score": 0.2878787878787879
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9303988218307495,
        "meteor": 0.1073345259391771
    },
    "sail/poolformer/s12": {
        "code_bleu": [
            {
                "codebleu": 0.3032465478458519
            },
            {
                "ngram_match_score": 0.02441403242593531
            },
            {
                "weighted_ngram_match_score": 0.05220852259383606
            },
            {
                "syntax_match_score": 0.4696969696969697
            },
            {
                "dataflow_match_score": 0.6666666666666666
            }
        ],
        "cosine_similarity": 0.9624738693237305,
        "meteor": 0.3164296021438878
    },
    "bigscience/T0": {
        "code_bleu": [
            {
                "codebleu": 0.28832406043202163
            },
            {
                "ngram_match_score": 0.009775052692738831
            },
            {
                "weighted_ngram_match_score": 0.03787008338424195
            },
            {
                "syntax_match_score": 0.3783783783783784
            },
            {
                "dataflow_match_score": 0.7272727272727273
            }
        ],
        "cosine_similarity": 0.8823822736740112,
        "meteor": 0.10135135135135134
    },
    "xverse/XVERSE-13B": {
        "code_bleu": [
            {
                "codebleu": 0.25819482658358484
            },
            {
                "ngram_match_score": 0.01137442537765592
            },
            {
                "weighted_ngram_match_score": 0.036736545537409404
            },
            {
                "syntax_match_score": 0.36764705882352944
            },
            {
                "dataflow_match_score": 0.6170212765957447
            }
        ],
        "cosine_similarity": 0.96307373046875,
        "meteor": 0.15754641628767502
    },
    "mrm8488/bert-multi-cased-finetuned-xquadv1": {
        "code_bleu": [
            {
                "codebleu": 0.17969667655838736
            },
            {
                "ngram_match_score": 0.002003314538161416
            },
            {
                "weighted_ngram_match_score": 0.0024976774096737114
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.7142857142857143
            }
        ],
        "cosine_similarity": 0.9198828935623169,
        "meteor": 0.021613832853025934
    },
    "mrm8488/bert-tiny-finetuned-squadv2": {
        "code_bleu": [
            {
                "codebleu": 0.1747773540101828
            },
            {
                "ngram_match_score": 0.004753731294736233
            },
            {
                "weighted_ngram_match_score": 0.008365346581743706
            },
            {
                "syntax_match_score": 0.13043478260869565
            },
            {
                "dataflow_match_score": 0.5555555555555556
            }
        ],
        "cosine_similarity": 0.9823412895202637,
        "meteor": 0.02702702702702702
    },
    "google/mobilenet/v2/0.75/160": {
        "code_bleu": [
            {
                "codebleu": 0.3361111907055617
            },
            {
                "ngram_match_score": 0.0035026270829233708
            },
            {
                "weighted_ngram_match_score": 0.007608802405990251
            },
            {
                "syntax_match_score": 0.3333333333333333
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9544874429702759,
        "meteor": 0.11173184357541899
    },
    "google/mobilenet/v2/0.35/96": {
        "code_bleu": [
            {
                "codebleu": 0.3361399049830805
            },
            {
                "ngram_match_score": 0.0036174841929983888
            },
            {
                "weighted_ngram_match_score": 0.007608802405990251
            },
            {
                "syntax_match_score": 0.3333333333333333
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9536517262458801,
        "meteor": 0.11235955056179774
    },
    "microsoft/focalnet-tiny": {
        "code_bleu": [
            {
                "codebleu": 0.30381589965840156
            },
            {
                "ngram_match_score": 0.010107328552919084
            },
            {
                "weighted_ngram_match_score": 0.019544187409780985
            },
            {
                "syntax_match_score": 0.45588235294117646
            },
            {
                "dataflow_match_score": 0.7297297297297297
            }
        ],
        "cosine_similarity": 0.9597368240356445,
        "meteor": 0.1900647095959596
    },
    "nghuyong/ernie-3.0-medium-zh": {
        "code_bleu": [
            {
                "codebleu": 0.2461513919808477
            },
            {
                "ngram_match_score": 0.005579139764629879
            },
            {
                "weighted_ngram_match_score": 0.067261722276408
            },
            {
                "syntax_match_score": 0.4117647058823529
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9232144951820374,
        "meteor": 0.20698489751887814
    },
    "TsinghuaAI/CPM-Generate": {
        "code_bleu": [
            {
                "codebleu": 0.20610268120711264
            },
            {
                "ngram_match_score": 0.00473275025667737
            },
            {
                "weighted_ngram_match_score": 0.022264181468324897
            },
            {
                "syntax_match_score": 0.1724137931034483
            },
            {
                "dataflow_match_score": 0.625
            }
        ],
        "cosine_similarity": 0.9400473833084106,
        "meteor": 0.07246376811594203
    },
    "microsoft/cvt-21": {
        "code_bleu": [
            {
                "codebleu": 0.16450545510424358
            },
            {
                "ngram_match_score": 0.0028897314731818405
            },
            {
                "weighted_ngram_match_score": 0.005781439593143088
            },
            {
                "syntax_match_score": 0.19696969696969696
            },
            {
                "dataflow_match_score": 0.4523809523809524
            }
        ],
        "cosine_similarity": 0.937599241733551,
        "meteor": 0.08241758241758242
    },
    "microsoft/xprophetnet-large-wiki100-cased": {
        "code_bleu": [
            {
                "codebleu": 0.22725359459015632
            },
            {
                "ngram_match_score": 0.006078548707894933
            },
            {
                "weighted_ngram_match_score": 0.007586992443428013
            },
            {
                "syntax_match_score": 0.3953488372093023
            },
            {
                "dataflow_match_score": 0.5
            }
        ],
        "cosine_similarity": 0.9039558172225952,
        "meteor": 0.07589604136947219
    },
    "bhadresh-savani/electra-base-emotion": {
        "code_bleu": [
            {
                "codebleu": 0.2184670344849447
            },
            {
                "ngram_match_score": 0.003679034746812639
            },
            {
                "weighted_ngram_match_score": 0.003522436526299483
            },
            {
                "syntax_match_score": 0.2
            },
            {
                "dataflow_match_score": 0.6666666666666666
            }
        ],
        "cosine_similarity": 0.953048586845398,
        "meteor": 0.017574692442882248
    },
    "junnyu/roformer/chinese/small": {
        "code_bleu": [
            {
                "codebleu": 0.1693454349300437
            },
            {
                "ngram_match_score": 0.020491163303662983
            },
            {
                "weighted_ngram_match_score": 0.022879647454763136
            },
            {
                "syntax_match_score": 0.30601092896174864
            },
            {
                "dataflow_match_score": 0.328
            }
        ],
        "cosine_similarity": 0.9785962104797363,
        "meteor": 0.16563414902227241
    },
    "AIML-TUDA/stable-diffusion-safe": {
        "code_bleu": [
            {
                "codebleu": 0.22912628212662325
            },
            {
                "ngram_match_score": 0.0029834862420878116
            },
            {
                "weighted_ngram_match_score": 0.01958224832501124
            },
            {
                "syntax_match_score": 0.16666666666666666
            },
            {
                "dataflow_match_score": 0.7272727272727273
            }
        ],
        "cosine_similarity": 0.9103749394416809,
        "meteor": 0.06648936170212767
    },
    "apple/deeplabv3-mobilevit-small": {
        "code_bleu": [
            {
                "codebleu": 0.31699220289223534
            },
            {
                "ngram_match_score": 0.09351573488472929
            },
            {
                "weighted_ngram_match_score": 0.3863958857674753
            },
            {
                "syntax_match_score": 0.27586206896551724
            },
            {
                "dataflow_match_score": 0.5121951219512195
            }
        ],
        "cosine_similarity": 0.966610312461853,
        "meteor": 0.48856509515570934
    },
    "google/ul2": {
        "code_bleu": [
            {
                "codebleu": 0.13539960619080516
            },
            {
                "ngram_match_score": 0.00019217349821102372
            },
            {
                "weighted_ngram_match_score": 0.0019325670544832034
            },
            {
                "syntax_match_score": 0.2894736842105263
            },
            {
                "dataflow_match_score": 0.25
            }
        ],
        "cosine_similarity": 0.9642242789268494,
        "meteor": 0.025285839929639405
    },
    "google/switch-large-128": {
        "code_bleu": [
            {
                "codebleu": 0.13110682442189467
            },
            {
                "ngram_match_score": 0.0020222757401458123
            },
            {
                "weighted_ngram_match_score": 0.005275093052798077
            },
            {
                "syntax_match_score": 0.34065934065934067
            },
            {
                "dataflow_match_score": 0.17647058823529413
            }
        ],
        "cosine_similarity": 0.9809970855712891,
        "meteor": 0.05379388448471121
    },
    "apple/deeplabv3-mobilevit-x-small": {
        "code_bleu": [
            {
                "codebleu": 0.1394565327990194
            },
            {
                "ngram_match_score": 0.07274999134192134
            },
            {
                "weighted_ngram_match_score": 0.1276329102494464
            },
            {
                "syntax_match_score": 0.13793103448275862
            },
            {
                "dataflow_match_score": 0.21951219512195122
            }
        ],
        "cosine_similarity": 0.9678869247436523,
        "meteor": 0.11718750000000001
    },
    "IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese": {
        "code_bleu": [
            {
                "codebleu": 0.3294691255105659
            },
            {
                "ngram_match_score": 0.05005570657386421
            },
            {
                "weighted_ngram_match_score": 0.07104083124657462
            },
            {
                "syntax_match_score": 0.5813953488372093
            },
            {
                "dataflow_match_score": 0.6153846153846154
            }
        ],
        "cosine_similarity": 0.9483439922332764,
        "meteor": 0.10341261633919337
    },
    "microsoft/prophetnet-large-uncased-cnndm": {
        "code_bleu": [
            {
                "codebleu": 0.27625081062455076
            },
            {
                "ngram_match_score": 0.013684632943252709
            },
            {
                "weighted_ngram_match_score": 0.017454973191313995
            },
            {
                "syntax_match_score": 0.4375
            },
            {
                "dataflow_match_score": 0.6363636363636364
            }
        ],
        "cosine_similarity": 0.9130377769470215,
        "meteor": 0.028719126938541065
    },
    "google/electra-large-generator": {
        "code_bleu": [
            {
                "codebleu": 0.2108562869604289
            },
            {
                "ngram_match_score": 0.002594445031287883
            },
            {
                "weighted_ngram_match_score": 0.012547874527599428
            },
            {
                "syntax_match_score": 0.2727272727272727
            },
            {
                "dataflow_match_score": 0.5555555555555556
            }
        ],
        "cosine_similarity": 0.8694974184036255,
        "meteor": 0.07396449704142012
    },
    "Salesforce/codet5p-6b": {
        "code_bleu": [
            {
                "codebleu": 0.2474290848834877
            },
            {
                "ngram_match_score": 0.0022999143313955795
            },
            {
                "weighted_ngram_match_score": 0.008208108529224448
            },
            {
                "syntax_match_score": 0.4426229508196721
            },
            {
                "dataflow_match_score": 0.5365853658536586
            }
        ],
        "cosine_similarity": 0.971415638923645,
        "meteor": 0.11605415860735009
    },
    "facebook/mask2former-swin-small-cityscapes-panoptic": {
        "code_bleu": [
            {
                "codebleu": 0.1638442598916727
            },
            {
                "ngram_match_score": 0.08860195700812248
            },
            {
                "weighted_ngram_match_score": 0.09882636460985038
            },
            {
                "syntax_match_score": 0.14102564102564102
            },
            {
                "dataflow_match_score": 0.3269230769230769
            }
        ],
        "cosine_similarity": 0.98307865858078,
        "meteor": 0.11840673575129537
    },
    "google/long-t5-local-large": {
        "code_bleu": [
            {
                "codebleu": 0.10714285714285714
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0
            },
            {
                "dataflow_match_score": 0.42857142857142855
            }
        ],
        "cosine_similarity": 0.9234122633934021,
        "meteor": 0.0
    },
    "SenseTime/deformable-detr-single-scale": {
        "code_bleu": [
            {
                "codebleu": 0.04047523275390496
            },
            {
                "ngram_match_score": 0.0013609776425885815
            },
            {
                "weighted_ngram_match_score": 0.0042103017162767944
            },
            {
                "syntax_match_score": 0.06542056074766354
            },
            {
                "dataflow_match_score": 0.09090909090909091
            }
        ],
        "cosine_similarity": 0.886620283126831,
        "meteor": 0.029791459781529295
    },
    "thu-ml/unidiffuser-v1": {
        "code_bleu": [
            {
                "codebleu": 0.20653155062098294
            },
            {
                "ngram_match_score": 0.0049929214275513225
            },
            {
                "weighted_ngram_match_score": 0.006265856813956155
            },
            {
                "syntax_match_score": 0.42424242424242425
            },
            {
                "dataflow_match_score": 0.390625
            }
        ],
        "cosine_similarity": 0.9852181077003479,
        "meteor": 0.0953128717582679
    },
    "Salesforce/codegen-16B-multi": {
        "code_bleu": [
            {
                "codebleu": 0.27439024390243905
            },
            {
                "ngram_match_score": 0.0
            },
            {
                "weighted_ngram_match_score": 0.0
            },
            {
                "syntax_match_score": 0.0975609756097561
            },
            {
                "dataflow_match_score": 0.0
            }
        ],
        "cosine_similarity": 0.9332231879234314,
        "meteor": 0.0
    },
    "Intel/neural-chat-7b-v3": {
        "code_bleu": [
            {
                "codebleu": 0.17205703005316655
            },
            {
                "ngram_match_score": 0.001629577024403322
            },
            {
                "weighted_ngram_match_score": 0.003310658497962943
            },
            {
                "syntax_match_score": 0.4117647058823529
            },
            {
                "dataflow_match_score": 0.271523178807947
            }
        ],
        "cosine_similarity": 0.9887246489524841,
        "meteor": 0.045566070802663866
    },
    "Salesforce/codegen-6B-mono": {
        "code_bleu": [
            {
                "codebleu": 0.16241820993599926
            },
            {
                "ngram_match_score": 0.0007788231340875255
            },
            {
                "weighted_ngram_match_score": 0.01287150253861497
            },
            {
                "syntax_match_score": 0.0975609756097561
            },
            {
                "dataflow_match_score": 0.5384615384615384
            }
        ],
        "cosine_similarity": 0.9267163276672363,
        "meteor": 0.0284629981024668
    },
    "Tanrei/GPTSAN-japanese": {
        "code_bleu": [
            {
                "codebleu": 0.15437685347221802
            },
            {
                "ngram_match_score": 0.0017079702630080666
            },
            {
                "weighted_ngram_match_score": 0.0016589354046383142
            },
            {
                "syntax_match_score": 0.34080717488789236
            },
            {
                "dataflow_match_score": 0.2733333333333333
            }
        ],
        "cosine_similarity": 0.9644631147384644,
        "meteor": 0.011111111111111112
    },
    "Salesforce/codegen-16B-mono": {
        "code_bleu": [
            {
                "codebleu": 0.19622410312789307
            },
            {
                "ngram_match_score": 0.0022826355484038443
            },
            {
                "weighted_ngram_match_score": 0.010568748820579371
            },
            {
                "syntax_match_score": 0.1951219512195122
            },
            {
                "dataflow_match_score": 0.5769230769230769
            }
        ],
        "cosine_similarity": 0.9347983002662659,
        "meteor": 0.0728862973760933
    },
    "nvidia/nemotron-3-8b-base-4k": {
        "code_bleu": [
            {
                "codebleu": 0.2177165466170778
            },
            {
                "ngram_match_score": 0.001658685363569533
            },
            {
                "weighted_ngram_match_score": 0.0067736386708792335
            },
            {
                "syntax_match_score": 0.14814814814814814
            },
            {
                "dataflow_match_score": 0.7142857142857143
            }
        ],
        "cosine_similarity": 0.9294523596763611,
        "meteor": 0.03676470588235294
    },
    "google/pix2struct-textcaps-large": {
        "code_bleu": [
            {
                "codebleu": 0.060119424294680125
            },
            {
                "ngram_match_score": 0.01978819714819925
            },
            {
                "weighted_ngram_match_score": 0.034955178251318685
            },
            {
                "syntax_match_score": 0.13043478260869565
            },
            {
                "dataflow_match_score": 0.055299539170506916
            }
        ],
        "cosine_similarity": 0.9645161628723145,
        "meteor": 0.07215392838054516
    },
    "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1": {
        "code_bleu": [
            {
                "codebleu": 0.07814712868462138
            },
            {
                "ngram_match_score": 0.0009515156204305701
            },
            {
                "weighted_ngram_match_score": 0.0015229487458072336
            },
            {
                "syntax_match_score": 0.1652542372881356
            },
            {
                "dataflow_match_score": 0.14485981308411214
            }
        ],
        "cosine_similarity": 0.984872579574585,
        "meteor": 0.029885057471264367
    },
    "Qwen/Qwen-14B-Chat-Int8": {
        "code_bleu": [
            {
                "codebleu": 0.13844072525545464
            },
            {
                "ngram_match_score": 0.009799721254889713
            },
            {
                "weighted_ngram_match_score": 0.01588436203293868
            },
            {
                "syntax_match_score": 0.11428571428571428
            },
            {
                "dataflow_match_score": 0.41379310344827586
            }
        ],
        "cosine_similarity": 0.9210298657417297,
        "meteor": 0.0925925925925926
    },
    "bhadresh-savani/electra-base-squad2": {
        "code_bleu": [
            {
                "codebleu": 0.23780179247580358
            },
            {
                "ngram_match_score": 0.0023897897228515085
            },
            {
                "weighted_ngram_match_score": 0.0022732875688544455
            },
            {
                "syntax_match_score": 0.3707865168539326
            },
            {
                "dataflow_match_score": 0.5757575757575758
            }
        ],
        "cosine_similarity": 0.9687721729278564,
        "meteor": 0.055350553505535055
    }
}