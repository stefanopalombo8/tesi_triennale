{"sentence-transformers/all-MiniLM-L12-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "roberta-large": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3317350447177887,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.14171843230724335,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04291723668575287,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a freelance model.</s>\",\n  'score': 0.02134818211197853,\n  'token': 18150,\n  'token_str': '\u0120freelance'},\n {'sequence': \"<s>Hello I'm a young model.</s>\",\n  'score': 0.021098261699080467,\n  'token': 664,\n  'token_str': '\u0120young'}]\n", "from transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\nmodel = RobertaModel.from_pretrained('roberta-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\nmodel = TFRobertaModel.from_pretrained('roberta-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08260300755500793,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a driver.</s>',\n  'score': 0.05736079439520836,\n  'token': 1393,\n  'token_str': '\u0120driver'},\n {'sequence': '<s>The man worked as a teacher.</s>',\n  'score': 0.04709019884467125,\n  'token': 3254,\n  'token_str': '\u0120teacher'},\n {'sequence': '<s>The man worked as a bartender.</s>',\n  'score': 0.04641604796051979,\n  'token': 33080,\n  'token_str': '\u0120bartender'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.04239227622747421,\n  'token': 38233,\n  'token_str': '\u0120waiter'}]\n\n>>> unmasker(\"The woman worked as a <mask>.\")\n\n[{'sequence': '<s>The woman worked as a nurse.</s>',\n  'score': 0.2667474150657654,\n  'token': 9008,\n  'token_str': '\u0120nurse'},\n {'sequence': '<s>The woman worked as a waitress.</s>',\n  'score': 0.12280137836933136,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The woman worked as a teacher.</s>',\n  'score': 0.09747499972581863,\n  'token': 3254,\n  'token_str': '\u0120teacher'},\n {'sequence': '<s>The woman worked as a secretary.</s>',\n  'score': 0.05783602222800255,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The woman worked as a cleaner.</s>',\n  'score': 0.05576248839497566,\n  'token': 16126,\n  'token_str': '\u0120cleaner'}]\n"], "sentence-transformers/all-MiniLM-L6-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "openai/clip-vit-large-patch14": ["from PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n"], "bert-base-uncased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n"], "distilbert/distilbert-base-uncased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n", "from transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n"], "gpt2": [">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n", "from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n"], "sentence-transformers/all-mpnet-base-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "xlm-roberta-base": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n", "from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\n# forward pass\noutput = model(**encoded_input)\n"], "distilbert-base-uncased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n", "from transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n"], "distilgpt2": [">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(48)\n>>> generator(\"The White man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': \"The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the\"},\n {'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a \"'},\n {'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]\n \n>>> set_seed(48)\n>>> generator(\"The Black man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},\n {'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},\n {'generated_text': 'The Black man worked as a police spokesman four months ago...'}]\n", ">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I\u2019m a language model\", max_length=20, num_return_sequences=5)\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n[{'generated_text': \"Hello, I'm a language model, I'm a language model. In my previous post I've\"},\n {'generated_text': \"Hello, I'm a language model, and I'd love to hear what you think about it.\"},\n {'generated_text': \"Hello, I'm a language model, but I don't get much of a connection anymore, so\"},\n {'generated_text': \"Hello, I'm a language model, a functional language... It's not an example, and that\"},\n {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I\"}]\n", "from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = GPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = TFGPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n"], "laion/CLIP-ViT-B-16-laion2B-s34B-b88K": [], "distilbert-base-uncased-finetuned-sst-2-english": ["import torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n"], "openai/clip-vit-large-patch14-336": [], "microsoft/trocr-base-handwritten": ["from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\n# load image from the IAM database\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "laion/CLIP-ViT-B-32-laion2B-s34B-b79K": [], "google/flan-t5-large": ["\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "facebook/bart-large-cnn": ["from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\nARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"], "microsoft/deberta-base": [], "Qwen/Qwen-VL-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n    {'text': '\u8fd9\u662f\u4ec0\u4e48'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# \u56fe\u4e2d\u662f\u4e00\u540d\u5e74\u8f7b\u5973\u5b50\u5728\u6c99\u6ee9\u4e0a\u548c\u5979\u7684\u72d7\u73a9\u800d\uff0c\u72d7\u7684\u54c1\u79cd\u53ef\u80fd\u662f\u62c9\u5e03\u62c9\u591a\u3002\u5979\u4eec\u5750\u5728\u6c99\u6ee9\u4e0a\uff0c\u72d7\u7684\u524d\u817f\u62ac\u8d77\u6765\uff0c\u4f3c\u4e4e\u5728\u548c\u4eba\u7c7b\u51fb\u638c\u3002\u4e24\u4eba\u4e4b\u95f4\u5145\u6ee1\u4e86\u4fe1\u4efb\u548c\u7231\u3002\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, '\u8f93\u51fa\"\u51fb\u638c\"\u7684\u68c0\u6d4b\u6846', history=history)\nprint(response)\n# <ref>\u51fb\u638c</ref><box>(517,508),(589,611)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n"], "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis": [], "bert-base-cased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a fashion model. [SEP]\",\n  'score': 0.09019174426794052,\n  'token': 4633,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] Hello I'm a new model. [SEP]\",\n  'score': 0.06349995732307434,\n  'token': 1207,\n  'token_str': 'new'},\n {'sequence': \"[CLS] Hello I'm a male model. [SEP]\",\n  'score': 0.06228214129805565,\n  'token': 2581,\n  'token_str': 'male'},\n {'sequence': \"[CLS] Hello I'm a professional model. [SEP]\",\n  'score': 0.0441727414727211,\n  'token': 1848,\n  'token_str': 'professional'},\n {'sequence': \"[CLS] Hello I'm a super model. [SEP]\",\n  'score': 0.03326151892542839,\n  'token': 7688,\n  'token_str': 'super'}]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = BertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] The man worked as a lawyer. [SEP]',\n  'score': 0.04804691672325134,\n  'token': 4545,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] The man worked as a waiter. [SEP]',\n  'score': 0.037494491785764694,\n  'token': 17989,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] The man worked as a cop. [SEP]',\n  'score': 0.035512614995241165,\n  'token': 9947,\n  'token_str': 'cop'},\n {'sequence': '[CLS] The man worked as a detective. [SEP]',\n  'score': 0.031271643936634064,\n  'token': 9140,\n  'token_str': 'detective'},\n {'sequence': '[CLS] The man worked as a doctor. [SEP]',\n  'score': 0.027423162013292313,\n  'token': 3995,\n  'token_str': 'doctor'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] The woman worked as a nurse. [SEP]',\n  'score': 0.16927455365657806,\n  'token': 7439,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] The woman worked as a waitress. [SEP]',\n  'score': 0.1501094549894333,\n  'token': 15098,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] The woman worked as a maid. [SEP]',\n  'score': 0.05600163713097572,\n  'token': 13487,\n  'token_str': 'maid'},\n {'sequence': '[CLS] The woman worked as a housekeeper. [SEP]',\n  'score': 0.04838843643665314,\n  'token': 26458,\n  'token_str': 'housekeeper'},\n {'sequence': '[CLS] The woman worked as a cook. [SEP]',\n  'score': 0.029980547726154327,\n  'token': 9834,\n  'token_str': 'cook'}]\n"], "facebook/opt-125m": [">>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nA nice dinner with a friend.\\nI'm not sure'}]\n", ">>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nCoffee, sausage and cream cheese at Chili's.'}]\n"], "google/vit-base-patch16-224": ["from transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "runwayml/stable-diffusion-v1-5": [], "albert-base-v2": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-base-v2')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u2581modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u2581modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u2581model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u2581runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u2581lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nmodel = AlbertModel.from_pretrained(\"albert-base-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nmodel = TFAlbertModel.from_pretrained(\"albert-base-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-base-v2')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u2581chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u2581janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u2581shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u2581blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u2581lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u2581receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u2581janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u2581paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u2581chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u2581waitress\"\n   }\n]\n"], "finiteautomata/bertweet-base-sentiment-analysis": [], "cl-tohoku/bert-base-japanese": [], "stabilityai/stable-diffusion-xl-base-1.0": ["from diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"An astronaut riding a green horse\"\n\nimages = pipe(prompt=prompt).images[0]\n", "from diffusers import DiffusionPipeline\nimport torch\n\n# load both base & refiner\nbase = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nbase.to(\"cuda\")\nrefiner = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner.to(\"cuda\")\n\n# Define how many steps and what % of steps to be run on each experts (80/20) here\nn_steps = 40\nhigh_noise_frac = 0.8\n\nprompt = \"A majestic lion jumping from a big stone at night\"\n\n# run both experts\nimage = base(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n", "- from diffusers import StableDiffusionXLPipeline\n+ from optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n", "- from diffusers import StableDiffusionXLPipeline\n+ from optimum.onnxruntime import ORTStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n"], "nomic-ai/nomic-embed-text-v1": ["from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['search_query: Who is Laurens van Der Maaten?']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['clustering: the quick brown fox']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['classification: the quick brown fox']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\nsentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)\nmodel.eval()\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\nembeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\n", "import { pipeline } from '@xenova/transformers';\n\n// Create a feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'nomic-ai/nomic-embed-text-v1', {\n    quantized: false, // Comment out this line to use the quantized version\n});\n\n// Compute sentence embeddings\nconst texts = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?'];\nconst embeddings = await extractor(texts, { pooling: 'mean', normalize: true });\nconsole.log(embeddings);\n", "from nomic import embed\n\noutput = embed.text(\n    texts=['Nomic Embedding API', '#keepAIOpen'],\n    model='nomic-embed-text-v1',\n    task_type='search_document'\n)\n\nprint(output)\n"], "bert-base-multilingual-cased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a model model. [SEP]\",\n  'score': 0.10182085633277893,\n  'token': 13192,\n  'token_str': 'model'},\n {'sequence': \"[CLS] Hello I'm a world model. [SEP]\",\n  'score': 0.052126359194517136,\n  'token': 11356,\n  'token_str': 'world'},\n {'sequence': \"[CLS] Hello I'm a data model. [SEP]\",\n  'score': 0.048930276185274124,\n  'token': 11165,\n  'token_str': 'data'},\n {'sequence': \"[CLS] Hello I'm a flight model. [SEP]\",\n  'score': 0.02036019042134285,\n  'token': 23578,\n  'token_str': 'flight'},\n {'sequence': \"[CLS] Hello I'm a business model. [SEP]\",\n  'score': 0.020079681649804115,\n  'token': 14155,\n  'token_str': 'business'}]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n"], "almanach/camembert-base": ["from transformers import CamembertModel, CamembertTokenizer\n\n# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\ncamembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n\ncamembert.eval()  # disable dropout (or leave in train mode to finetune)\n", "from transformers import pipeline \n\ncamembert_fill_mask  = pipeline(\"fill-mask\", model=\"camembert/camembert-base-wikipedia-4gb\", tokenizer=\"camembert/camembert-base-wikipedia-4gb\")\nresults = camembert_fill_mask(\"Le camembert est un fromage de <mask>!\")\n# results\n#[{'sequence': '<s> Le camembert est un fromage de ch\u00e8vre!</s>', 'score': 0.4937814474105835, 'token': 19370}, \n#{'sequence': '<s> Le camembert est un fromage de brebis!</s>', 'score': 0.06255942583084106, 'token': 30616}, \n#{'sequence': '<s> Le camembert est un fromage de montagne!</s>', 'score': 0.04340197145938873, 'token': 2364},\n# {'sequence': '<s> Le camembert est un fromage de No\u00ebl!</s>', 'score': 0.02823255956172943, 'token': 3236}, \n#{'sequence': '<s> Le camembert est un fromage de vache!</s>', 'score': 0.021357402205467224, 'token': 12329}]\n", "import torch\n# Tokenize in sub-words with SentencePiece\ntokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n# ['\u2581J', \"'\", 'aime', '\u2581le', '\u2581ca', 'member', 't', '\u2581!'] \n\n# 1-hot encode and add special starting and end tokens \nencoded_sentence = tokenizer.encode(tokenized_sentence)\n# [5, 221, 10, 10600, 14, 8952, 10540, 75, 1114, 6]\n# NB: Can be done in one step : tokenize.encode(\"J'aime le camembert !\")\n\n# Feed tokens to Camembert as a torch tensor (batch dim 1)\nencoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\nembeddings, _ = camembert(encoded_sentence)\n# embeddings.detach()\n# embeddings.size torch.Size([1, 10, 768])\n#tensor([[[-0.0928,  0.0506, -0.0094,  ..., -0.2388,  0.1177, -0.1302],\n#         [ 0.0662,  0.1030, -0.2355,  ..., -0.4224, -0.0574, -0.2802],\n#         [-0.0729,  0.0547,  0.0192,  ..., -0.1743,  0.0998, -0.2677],\n#         ...,\n", "from transformers import CamembertConfig\n# (Need to reload the model with new config)\nconfig = CamembertConfig.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\", output_hidden_states=True)\ncamembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\", config=config)\n\nembeddings, _, all_layer_embeddings = camembert(encoded_sentence)\n#  all_layer_embeddings list of len(all_layer_embeddings) == 13 (input embedding layer + 12 self attention layers)\nall_layer_embeddings[5]\n# layer 5 contextual embedding : size torch.Size([1, 10, 768])\n#tensor([[[-0.0059, -0.0227,  0.0065,  ..., -0.0770,  0.0369,  0.0095],\n#         [ 0.2838, -0.1531, -0.3642,  ..., -0.0027, -0.8502, -0.7914],\n#         [-0.0073, -0.0338, -0.0011,  ...,  0.0533, -0.0250, -0.0061],\n#         ...,\n"], "CIDAS/clipseg-rd64-refined": [], "stabilityai/stable-diffusion-3-medium": ["import torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nimage = pipe(\n    \"A cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    guidance_scale=7.0,\n).images[0]\nimage\n"], "SamLowe/roberta-base-go_emotions": ["from transformers import pipeline\n\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n\nsentences = [\"I am not having a great day\"]\n\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels\n"], "distilroberta-base": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilroberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n[{'score': 0.1237526461482048,\n  'sequence': 'The man worked as a waiter.',\n  'token': 38233,\n  'token_str': ' waiter'},\n {'score': 0.08968018740415573,\n  'sequence': 'The man worked as a waitress.',\n  'token': 35698,\n  'token_str': ' waitress'},\n {'score': 0.08387645334005356,\n  'sequence': 'The man worked as a bartender.',\n  'token': 33080,\n  'token_str': ' bartender'},\n {'score': 0.061059024184942245,\n  'sequence': 'The man worked as a mechanic.',\n  'token': 25682,\n  'token_str': ' mechanic'},\n {'score': 0.03804653510451317,\n  'sequence': 'The man worked as a courier.',\n  'token': 37171,\n  'token_str': ' courier'}]\n  \n>>> unmasker(\"The woman worked as a <mask>.\")\n[{'score': 0.23149248957633972,\n  'sequence': 'The woman worked as a waitress.',\n  'token': 35698,\n  'token_str': ' waitress'},\n {'score': 0.07563332468271255,\n  'sequence': 'The woman worked as a waiter.',\n  'token': 38233,\n  'token_str': ' waiter'},\n {'score': 0.06983394920825958,\n  'sequence': 'The woman worked as a bartender.',\n  'token': 33080,\n  'token_str': ' bartender'},\n {'score': 0.05411609262228012,\n  'sequence': 'The woman worked as a nurse.',\n  'token': 9008,\n  'token_str': ' nurse'},\n {'score': 0.04995106905698776,\n  'sequence': 'The woman worked as a maid.',\n  'token': 29754,\n  'token_str': ' maid'}]\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilroberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n[{'score': 0.04673689603805542,\n  'sequence': \"Hello I'm a business model.\",\n  'token': 265,\n  'token_str': ' business'},\n {'score': 0.03846118599176407,\n  'sequence': \"Hello I'm a freelance model.\",\n  'token': 18150,\n  'token_str': ' freelance'},\n {'score': 0.03308931365609169,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 2734,\n  'token_str': ' fashion'},\n {'score': 0.03018997237086296,\n  'sequence': \"Hello I'm a role model.\",\n  'token': 774,\n  'token_str': ' role'},\n {'score': 0.02111748233437538,\n  'sequence': \"Hello I'm a Playboy model.\",\n  'token': 24526,\n  'token_str': ' Playboy'}]\n"], "distilbert-base-multilingual-cased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'score': 0.040800247341394424,\n  'sequence': \"Hello I'm a virtual model.\",\n  'token': 37859,\n  'token_str': 'virtual'},\n {'score': 0.020015988498926163,\n  'sequence': \"Hello I'm a big model.\",\n  'token': 22185,\n  'token_str': 'big'},\n {'score': 0.018680453300476074,\n  'sequence': \"Hello I'm a Hello model.\",\n  'token': 31178,\n  'token_str': 'Hello'},\n {'score': 0.017396586015820503,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 13192,\n  'token_str': 'model'},\n {'score': 0.014229810796678066,\n  'sequence': \"Hello I'm a perfect model.\",\n  'token': 43477,\n  'token_str': 'perfect'}]\n"], "papluca/xlm-roberta-base-language-detection": ["from transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\npipe(text, top_k=1, truncation=True)\n", "import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npreds = torch.softmax(logits, dim=-1)\n\n# Map raw predictions to languages\nid2lang = model.config.id2label\nvals, idxs = torch.max(preds, dim=1)\n{id2lang[k.item()]: v.item() for k, v in zip(idxs, vals)}\n"], "meta-llama/Meta-Llama-3-8B-Instruct": ["import transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][-1])\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n"], "sentence-transformers/bert-base-nli-mean-tokens": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\nmodel = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "sentence-transformers/multi-qa-MiniLM-L6-cos-v1": ["from sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n#Load the model\nmodel = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n\n#Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take average of all tokens\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n#Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n    # Normalize embeddings\n    embeddings = F.normalize(embeddings, p=2, dim=1)\n    \n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n", "from transformers import AutoTokenizer, TFAutoModel\nimport tensorflow as tf\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state\n    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n\n\n#Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n\n    # Compute token embeddings\n    model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n    # Normalize embeddings\n    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n\n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\nmodel = TFAutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = (query_emb @ tf.transpose(doc_emb))[0].numpy().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n"], "lxyuan/distilbert-base-multilingual-cased-sentiments-student": ["from transformers import pipeline\n\ndistilled_student_sentiment_classifier = pipeline(\n    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n    return_all_scores=True\n)\n\n# english\ndistilled_student_sentiment_classifier (\"I love this movie and i would watch it again and again!\")\n>> [[{'label': 'positive', 'score': 0.9731044769287109},\n  {'label': 'neutral', 'score': 0.016910076141357422},\n  {'label': 'negative', 'score': 0.009985478594899178}]]\n\n# malay\ndistilled_student_sentiment_classifier(\"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\")\n[[{'label': 'positive', 'score': 0.9760093688964844},\n  {'label': 'neutral', 'score': 0.01804516464471817},\n  {'label': 'negative', 'score': 0.005945465061813593}]]\n\n# japanese\ndistilled_student_sentiment_classifier(\"\u79c1\u306f\u3053\u306e\u6620\u753b\u304c\u5927\u597d\u304d\u3067\u3001\u4f55\u5ea6\u3082\u898b\u307e\u3059\uff01\")\n>> [[{'label': 'positive', 'score': 0.9342429041862488},\n  {'label': 'neutral', 'score': 0.040193185210227966},\n  {'label': 'negative', 'score': 0.025563929229974747}]]\n\n"], "microsoft/deberta-v3-base": [], "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "intfloat/multilingual-e5-small": ["import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: \u5357\u74dc\u7684\u5bb6\u5e38\u505a\u6cd5',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: 1.\u6e05\u7092\u5357\u74dc\u4e1d \u539f\u6599:\u5ae9\u5357\u74dc\u534a\u4e2a \u8c03\u6599:\u8471\u3001\u76d0\u3001\u767d\u7cd6\u3001\u9e21\u7cbe \u505a\u6cd5: 1\u3001\u5357\u74dc\u7528\u5200\u8584\u8584\u7684\u524a\u53bb\u8868\u9762\u4e00\u5c42\u76ae,\u7528\u52fa\u5b50\u522e\u53bb\u74e4 2\u3001\u64e6\u6210\u7ec6\u4e1d(\u6ca1\u6709\u64e6\u83dc\u677f\u5c31\u7528\u5200\u6162\u6162\u5207\u6210\u7ec6\u4e1d) 3\u3001\u9505\u70e7\u70ed\u653e\u6cb9,\u5165\u8471\u82b1\u7178\u51fa\u9999\u5473 4\u3001\u5165\u5357\u74dc\u4e1d\u5feb\u901f\u7ffb\u7092\u4e00\u5206\u949f\u5de6\u53f3,\u653e\u76d0\u3001\u4e00\u70b9\u767d\u7cd6\u548c\u9e21\u7cbe\u8c03\u5473\u51fa\u9505 2.\u9999\u8471\u7092\u5357\u74dc \u539f\u6599:\u5357\u74dc1\u53ea \u8c03\u6599:\u9999\u8471\u3001\u849c\u672b\u3001\u6a44\u6984\u6cb9\u3001\u76d0 \u505a\u6cd5: 1\u3001\u5c06\u5357\u74dc\u53bb\u76ae,\u5207\u6210\u7247 2\u3001\u6cb9\u95058\u6210\u70ed\u540e,\u5c06\u849c\u672b\u653e\u5165\u7206\u9999 3\u3001\u7206\u9999\u540e,\u5c06\u5357\u74dc\u7247\u653e\u5165,\u7ffb\u7092 4\u3001\u5728\u7ffb\u7092\u7684\u540c\u65f6,\u53ef\u4ee5\u4e0d\u65f6\u5730\u5f80\u9505\u91cc\u52a0\u6c34,\u4f46\u4e0d\u8981\u592a\u591a 5\u3001\u653e\u5165\u76d0,\u7092\u5300 6\u3001\u5357\u74dc\u5dee\u4e0d\u591a\u8f6f\u548c\u7ef5\u4e86\u4e4b\u540e,\u5c31\u53ef\u4ee5\u5173\u706b 7\u3001\u6492\u5165\u9999\u8471,\u5373\u53ef\u51fa\u9505\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\nmodel = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/multilingual-e5-small')\ninput_texts = [\n    'query: how much protein should a female eat',\n    'query: \u5357\u74dc\u7684\u5bb6\u5e38\u505a\u6cd5',\n    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 i     s 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n    \"passage: 1.\u6e05\u7092\u5357\u74dc\u4e1d \u539f\u6599:\u5ae9\u5357\u74dc\u534a\u4e2a \u8c03\u6599:\u8471\u3001\u76d0\u3001\u767d\u7cd6\u3001\u9e21\u7cbe \u505a\u6cd5: 1\u3001\u5357\u74dc\u7528\u5200\u8584\u8584\u7684\u524a\u53bb\u8868\u9762\u4e00\u5c42\u76ae     ,\u7528\u52fa\u5b50\u522e\u53bb\u74e4 2\u3001\u64e6\u6210\u7ec6\u4e1d(\u6ca1\u6709\u64e6\u83dc\u677f\u5c31\u7528\u5200\u6162\u6162\u5207\u6210\u7ec6\u4e1d) 3\u3001\u9505\u70e7\u70ed\u653e\u6cb9,\u5165\u8471\u82b1\u7178\u51fa\u9999\u5473 4\u3001\u5165\u5357\u74dc\u4e1d\u5feb\u901f\u7ffb\u7092\u4e00\u5206\u949f\u5de6\u53f3,     \u653e\u76d0\u3001\u4e00\u70b9\u767d\u7cd6\u548c\u9e21\u7cbe\u8c03\u5473\u51fa\u9505 2.\u9999\u8471\u7092\u5357\u74dc \u539f\u6599:\u5357\u74dc1\u53ea \u8c03\u6599:\u9999\u8471\u3001\u849c\u672b\u3001\u6a44\u6984\u6cb9\u3001\u76d0 \u505a\u6cd5: 1\u3001\u5c06\u5357\u74dc\u53bb\u76ae,\u5207\u6210\u7247 2\u3001\u6cb9     \u95058\u6210\u70ed\u540e,\u5c06\u849c\u672b\u653e\u5165\u7206\u9999 3\u3001\u7206\u9999\u540e,\u5c06\u5357\u74dc\u7247\u653e\u5165,\u7ffb\u7092 4\u3001\u5728\u7ffb\u7092\u7684\u540c\u65f6,\u53ef\u4ee5\u4e0d\u65f6\u5730\u5f80\u9505\u91cc\u52a0\u6c34,\u4f46\u4e0d\u8981\u592a\u591a 5\u3001\u653e\u5165\u76d0,\u7092\u5300      6\u3001\u5357\u74dc\u5dee\u4e0d\u591a\u8f6f\u548c\u7ef5\u4e86\u4e4b\u540e,\u5c31\u53ef\u4ee5\u5173\u706b 7\u3001\u6492\u5165\u9999\u8471,\u5373\u53ef\u51fa\u9505\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\n"], "sentence-transformers/paraphrase-MiniLM-L6-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "nlpconnect/vit-gpt2-image-captioning": ["\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\ndef predict_step(image_paths):\n  images = []\n  for image_path in image_paths:\n    i_image = Image.open(image_path)\n    if i_image.mode != \"RGB\":\n      i_image = i_image.convert(mode=\"RGB\")\n\n    images.append(i_image)\n\n  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n  pixel_values = pixel_values.to(device)\n\n  output_ids = model.generate(pixel_values, **gen_kwargs)\n\n  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n  preds = [pred.strip() for pred in preds]\n  return preds\n\n\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\n", "\nfrom transformers import pipeline\n\nimage_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n\nimage_to_text(\"https://ankur3107.github.io/assets/images/image-captioning-example.png\")\n\n# [{'generated_text': 'a soccer game with a player jumping to catch the ball '}]\n\n"], "petals-team/StableBeluga2": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga2\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga2\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\nsystem_prompt = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n\nmessage = \"Write me a poem please\"\nprompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n", "@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n"], "camembert-base": ["from transformers import CamembertModel, CamembertTokenizer\n\n# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\ncamembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n\ncamembert.eval()  # disable dropout (or leave in train mode to finetune)\n", "from transformers import pipeline \n\ncamembert_fill_mask  = pipeline(\"fill-mask\", model=\"camembert/camembert-base-wikipedia-4gb\", tokenizer=\"camembert/camembert-base-wikipedia-4gb\")\nresults = camembert_fill_mask(\"Le camembert est un fromage de <mask>!\")\n# results\n#[{'sequence': '<s> Le camembert est un fromage de ch\u00e8vre!</s>', 'score': 0.4937814474105835, 'token': 19370}, \n#{'sequence': '<s> Le camembert est un fromage de brebis!</s>', 'score': 0.06255942583084106, 'token': 30616}, \n#{'sequence': '<s> Le camembert est un fromage de montagne!</s>', 'score': 0.04340197145938873, 'token': 2364},\n# {'sequence': '<s> Le camembert est un fromage de No\u00ebl!</s>', 'score': 0.02823255956172943, 'token': 3236}, \n#{'sequence': '<s> Le camembert est un fromage de vache!</s>', 'score': 0.021357402205467224, 'token': 12329}]\n", "import torch\n# Tokenize in sub-words with SentencePiece\ntokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n# ['\u2581J', \"'\", 'aime', '\u2581le', '\u2581ca', 'member', 't', '\u2581!'] \n\n# 1-hot encode and add special starting and end tokens \nencoded_sentence = tokenizer.encode(tokenized_sentence)\n# [5, 221, 10, 10600, 14, 8952, 10540, 75, 1114, 6]\n# NB: Can be done in one step : tokenize.encode(\"J'aime le camembert !\")\n\n# Feed tokens to Camembert as a torch tensor (batch dim 1)\nencoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\nembeddings, _ = camembert(encoded_sentence)\n# embeddings.detach()\n# embeddings.size torch.Size([1, 10, 768])\n#tensor([[[-0.0928,  0.0506, -0.0094,  ..., -0.2388,  0.1177, -0.1302],\n#         [ 0.0662,  0.1030, -0.2355,  ..., -0.4224, -0.0574, -0.2802],\n#         [-0.0729,  0.0547,  0.0192,  ..., -0.1743,  0.0998, -0.2677],\n#         ...,\n", "from transformers import CamembertConfig\n# (Need to reload the model with new config)\nconfig = CamembertConfig.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\", output_hidden_states=True)\ncamembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\", config=config)\n\nembeddings, _, all_layer_embeddings = camembert(encoded_sentence)\n#  all_layer_embeddings list of len(all_layer_embeddings) == 13 (input embedding layer + 12 self attention layers)\nall_layer_embeddings[5]\n# layer 5 contextual embedding : size torch.Size([1, 10, 768])\n#tensor([[[-0.0059, -0.0227,  0.0065,  ..., -0.0770,  0.0369,  0.0095],\n#         [ 0.2838, -0.1531, -0.3642,  ..., -0.0027, -0.8502, -0.7914],\n#         [-0.0073, -0.0338, -0.0011,  ...,  0.0533, -0.0250, -0.0061],\n#         ...,\n"], "Alibaba-NLP/gte-large-en-v1.5": ["# Requires transformers>=4.36.0\n\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\n\ninput_texts = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\n\nmodel_path = 'Alibaba-NLP/gte-large-en-v1.5'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = outputs.last_hidden_state[:, 0]\n \n# (Optionally) normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n", "# Requires sentence_transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\nsentences = ['That is a happy person', 'That is a very happy person']\n\nmodel = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\nembeddings = model.encode(sentences)\nprint(cos_sim(embeddings[0], embeddings[1]))\n", "// npm i @xenova/transformers\nimport { pipeline, dot } from '@xenova/transformers';\n\n// Create feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'Alibaba-NLP/gte-large-en-v1.5', {\n    quantized: false, // Comment out this line to use the quantized version\n});\n\n// Generate sentence embeddings\nconst sentences = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\nconst output = await extractor(sentences, { normalize: true, pooling: 'cls' });\n\n// Compute similarity scores\nconst [source_embeddings, ...document_embeddings ] = output.tolist();\nconst similarities = document_embeddings.map(x => 100 * dot(source_embeddings, x));\nconsole.log(similarities); // [41.86354093370361, 77.07076371259589, 37.02981979677899]\n"], "intfloat/e5-small-v2": ["import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: summit define',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-small-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-small-v2')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-small-v2')\ninput_texts = [\n    'query: how much protein should a female eat',\n    'query: summit define',\n    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n    \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\n"], "CompVis/stable-diffusion-v1-4": ["import torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\n\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "import torch\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\npipe.enable_attention_slicing()\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\n\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "import jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", revision=\"flax\", dtype=jax.numpy.bfloat16\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n", "import jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", revision=\"bf16\", dtype=jax.numpy.bfloat16\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n"], "meta-llama/Meta-Llama-3-8B": [">>> import transformers\n>>> import torch\n\n>>> model_id = \"meta-llama/Meta-Llama-3-8B\"\n\n>>> pipeline = transformers.pipeline(\n    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\n>>> pipeline(\"Hey how are you doing today?\")\n"], "bert-large-uncased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1886913776397705,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a professional model. [SEP]\",\n  'score': 0.07157472521066666,\n  'token': 2658,\n  'token_str': 'professional'},\n {'sequence': \"[CLS] hello i'm a male model. [SEP]\",\n  'score': 0.04053466394543648,\n  'token': 3287,\n  'token_str': 'male'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.03891477733850479,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fitness model. [SEP]\",\n  'score': 0.03038121573626995,\n  'token': 10516,\n  'token_str': 'fitness'}]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\nmodel = BertModel.from_pretrained(\"bert-large-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-large-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a bartender. [SEP]',\n  'score': 0.10426565259695053,\n  'token': 15812,\n  'token_str': 'bartender'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.10232779383659363,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.06281787157058716,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a lawyer. [SEP]',\n  'score': 0.050936125218868256,\n  'token': 5160,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.041034240275621414,\n  'token': 10533,\n  'token_str': 'carpenter'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.28473711013793945,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.11336520314216614,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a bartender. [SEP]',\n  'score': 0.09574324637651443,\n  'token': 15812,\n  'token_str': 'bartender'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.06351090222597122,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a secretary. [SEP]',\n  'score': 0.048970773816108704,\n  'token': 3187,\n  'token_str': 'secretary'}]\n"], "xlm-roberta-large": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n", "from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")\n\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\n# forward pass\noutput = model(**encoded_input)\n"], "ProsusAI/finbert": [], "microsoft/deberta-v3-large": [], "meta-llama/Llama-2-13b-chat-hf": [], "runwayml/stable-diffusion-inpainting": [], "emilyalsentzer/Bio_ClinicalBERT": ["from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\nmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n"], "nomic-ai/nomic-embed-text-v1.5": ["from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['search_query: Who is Laurens van Der Maaten?']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['clustering: the quick brown fox']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\nsentences = ['classification: the quick brown fox']\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "import torch.nn.functional as F\nfrom sentence_transformers import SentenceTransformer\n\nmatryoshka_dim = 512\n\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\nsentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\nembeddings = model.encode(sentences, convert_to_tensor=True)\nembeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\nembeddings = embeddings[:, :matryoshka_dim]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\n", "import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\nsentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True, safe_serialization=True)\nmodel.eval()\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n+ matryoshka_dim = 512\n\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\nembeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n+ embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n+ embeddings = embeddings[:, :matryoshka_dim]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\n", "import { pipeline, layer_norm } from '@xenova/transformers';\n\n// Create a feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'nomic-ai/nomic-embed-text-v1.5', {\n    quantized: false, // Comment out this line to use the quantized version\n});\n\n// Define sentences\nconst texts = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?'];\n\n// Compute sentence embeddings\nlet embeddings = await extractor(texts, { pooling: 'mean' });\nconsole.log(embeddings); // Tensor of shape [2, 768]\n\nconst matryoshka_dim = 512;\nembeddings = layer_norm(embeddings, [embeddings.dims[1]])\n    .slice(null, [0, matryoshka_dim])\n    .normalize(2, -1);\nconsole.log(embeddings.tolist());\n", "from nomic import embed\n\noutput = embed.text(\n    texts=['Nomic Embedding API', '#keepAIOpen'],\n    model='nomic-embed-text-v1.5',\n    task_type='search_document',\n    dimensionality=256,\n)\n\nprint(output)\n"], "meta-llama/Llama-2-7b-hf": [], "cardiffnlp/twitter-roberta-base-sentiment": ["from transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n \n \n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='sentiment'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night \ud83d\ude0a\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night \ud83d\ude0a\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"], "sentence-transformers/distiluse-base-multilingual-cased-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n"], "Falconsai/nsfw_image_detection": ["\n# Use a pipeline as a high-level helper\nfrom PIL import Image\nfrom transformers import pipeline\n\nimg = Image.open(\"<path_to_image_file>\")\nclassifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\")\nclassifier(img)\n", "\n# Load model directly\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModelForImageClassification, ViTImageProcessor\n\nimg = Image.open(\"<path_to_image_file>\")\nmodel = AutoModelForImageClassification.from_pretrained(\"Falconsai/nsfw_image_detection\")\nprocessor = ViTImageProcessor.from_pretrained('Falconsai/nsfw_image_detection')\nwith torch.no_grad():\n    inputs = processor(images=img, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n\npredicted_label = logits.argmax(-1).item()\nmodel.config.id2label[predicted_label]\n"], "google/byt5-small": ["from transformers import T5ForConditionalGeneration\nimport torch\n\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n\ninput_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\nlabels = torch.tensor([list(\"La vie est comme une bo\u00eete de chocolat.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n\nloss = model(input_ids, labels=labels).loss # forward pass\n", "from transformers import T5ForConditionalGeneration, AutoTokenizer\n\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\ntokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n\nmodel_inputs = tokenizer([\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\")\nlabels = tokenizer([\"La vie est comme une bo\u00eete de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\").input_ids\n\nloss = model(**model_inputs, labels=labels).loss # forward pass\n"], "facebook/esm2_t12_35M_UR50D": [], "nlptown/bert-base-multilingual-uncased-sentiment": [], "Salesforce/blip-image-captioning-large": ["import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16).to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n"], "microsoft/deberta-large-mnli": [], "Salesforce/blip-image-captioning-base": ["import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n", "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n", "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16).to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n"], "bert-base-chinese": ["from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n\nmodel = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n"], "facebook/m2m100_418M": ["from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\nhi_text = \"\u091c\u0940\u0935\u0928 \u090f\u0915 \u091a\u0949\u0915\u0932\u0947\u091f \u092c\u0949\u0915\u094d\u0938 \u0915\u0940 \u0924\u0930\u0939 \u0939\u0948\u0964\"\nchinese_text = \"\u751f\u6d3b\u5c31\u50cf\u4e00\u76d2\u5de7\u514b\u529b\u3002\"\n\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n\n# translate Hindi to French\ntokenizer.src_lang = \"hi\"\nencoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"La vie est comme une bo\u00eete de chocolat.\"\n\n# translate Chinese to English\ntokenizer.src_lang = \"zh\"\nencoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Life is like a box of chocolate.\"\n"], "tiiuae/falcon-7b-instruct": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "BAAI/bge-reranker-v2-m3": ["from FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscore = reranker.compute_score(['query', 'passage'], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n", "from FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n", "from FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\nprint(scores)\n", "import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to('cuda')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n"], "trl-internal-testing/tiny-random-LlamaForCausalLM": [], "laion/CLIP-ViT-H-14-laion2B-s32B-b79K": [], "meta-llama/Llama-2-7b-chat-hf": [], "thenlper/gte-base": ["import torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\ninput_texts = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\n\ntokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-base\")\nmodel = AutoModel.from_pretrained(\"thenlper/gte-base\")\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# (Optionally) normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\nsentences = ['That is a happy person', 'That is a very happy person']\n\nmodel = SentenceTransformer('thenlper/gte-base')\nembeddings = model.encode(sentences)\nprint(cos_sim(embeddings[0], embeddings[1]))\n"], "microsoft/resnet-50": ["from transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\n"], "microsoft/table-transformer-detection": [], "nlpaueb/legal-bert-base-uncased": ["from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n"], "cross-encoder/ms-marco-MiniLM-L-6-v2": ["from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n", "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n"], "stabilityai/stable-diffusion-2-1-base": ["from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nimport torch\n\nmodel_id = \"stabilityai/stable-diffusion-2-1-base\"\n\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n"], "thenlper/gte-small": ["import torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\ninput_texts = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\n\ntokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\nmodel = AutoModel.from_pretrained(\"thenlper/gte-small\")\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# (Optionally) normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\nsentences = ['That is a happy person', 'That is a very happy person']\n\nmodel = SentenceTransformer('thenlper/gte-large')\nembeddings = model.encode(sentences)\nprint(cos_sim(embeddings[0], embeddings[1]))\n"], "Rostlab/prot_t5_xl_uniref50": ["@article {Elnaggar2020.07.12.199554,\n    author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n    title = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n    elocation-id = {2020.07.12.199554},\n    year = {2020},\n    doi = {10.1101/2020.07.12.199554},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112 times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8 states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs. Availability ProtTrans: \\&lt;a href=\"https://github.com/agemagician/ProtTrans\"\\&gt;https://github.com/agemagician/ProtTrans\\&lt;/a\\&gt;Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n    eprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n    journal = {bioRxiv}\n}\n"], "intfloat/e5-large-v2": ["import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: summit define',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-large-v2')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-large-v2')\ninput_texts = [\n    'query: how much protein should a female eat',\n    'query: summit define',\n    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n    \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\n"], "facebook/esm2_t6_8M_UR50D": [], "shibing624/text2vec-base-multilingual": ["from text2vec import SentenceModel\nsentences = ['\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361', 'How to replace the Huabei bundled bank card']\n\nmodel = SentenceModel('shibing624/text2vec-base-multilingual')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-multilingual')\nmodel = AutoModel.from_pretrained('shibing624/text2vec-base-multilingual')\nsentences = ['\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361', 'How to replace the Huabei bundled bank card']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-multilingual\")\nsentences = ['\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361', 'How to replace the Huabei bundled bank card']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "sentence-transformers/distilbert-base-nli-stsb-mean-tokens": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\nmodel = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "sentence-transformers/multi-qa-mpnet-base-dot-v1": ["from sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n#Load the model\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n\n#Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n#CLS Pooling - Take output from first token\ndef cls_pooling(model_output):\n    return model_output.last_hidden_state[:,0]\n\n#Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = cls_pooling(model_output)\n\n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n"], "j-hartmann/emotion-english-distilroberta-base": ["from transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\n"], "sentence-transformers/all-roberta-large-v1": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "microsoft/git-base": [], "TheBloke/Llama-2-7B-Chat-GGUF": ["from ctransformers import AutoModelForCausalLM\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"llama-2-7b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n"], "distilbert-base-cased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n", "from transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n"], "lmsys/fastchat-t5-3b-v1.0": [], "bert-base-multilingual-uncased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a top model. [SEP]\",\n  'score': 0.1507750153541565,\n  'token': 11397,\n  'token_str': 'top'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.13075384497642517,\n  'token': 23589,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a good model. [SEP]\",\n  'score': 0.036272723227739334,\n  'token': 12050,\n  'token_str': 'good'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.035954564809799194,\n  'token': 10246,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a great model. [SEP]\",\n  'score': 0.028643041849136353,\n  'token': 11838,\n  'token_str': 'great'}]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a teacher. [SEP]',\n  'score': 0.07943806052207947,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the man worked as a lawyer. [SEP]',\n  'score': 0.0629938617348671,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the man worked as a farmer. [SEP]',\n  'score': 0.03367974981665611,\n  'token': 36799,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the man worked as a journalist. [SEP]',\n  'score': 0.03172805905342102,\n  'token': 19477,\n  'token_str': 'journalist'},\n {'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.031021825969219208,\n  'token': 33241,\n  'token_str': 'carpenter'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.07045423984527588,\n  'token': 52428,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a teacher. [SEP]',\n  'score': 0.05178029090166092,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the black woman worked as a lawyer. [SEP]',\n  'score': 0.032601192593574524,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the black woman worked as a slave. [SEP]',\n  'score': 0.030507225543260574,\n  'token': 31173,\n  'token_str': 'slave'},\n {'sequence': '[CLS] the black woman worked as a woman. [SEP]',\n  'score': 0.027691684663295746,\n  'token': 14050,\n  'token_str': 'woman'}]\n"], "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, average pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "distilbert/distilbert-base-cased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n", "from transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n"], "microsoft/mdeberta-v3-base": [], "mistralai/Mixtral-8x7B-v0.1": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"], "google/flan-t5-xxl": ["\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "cardiffnlp/twitter-xlm-roberta-base-sentiment": ["from transformers import pipeline\nmodel_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"T'estimo!\")\n", "from transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\nMODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night \ud83d\ude0a\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night \ud83d\ude0a\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"], "google/t5-v1_1-xl": [], "microsoft/Phi-3-medium-128k-instruct": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\nmodel_id = \"microsoft/Phi-3-medium-128k-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\", \n    torch_dtype=\"auto\", \n    trust_remote_code=True, \n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"return_full_text\": False,\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n"], "mistralai/Mistral-7B-v0.1": [], "BAAI/bge-reranker-base": ["from FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n", "from sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n", "from sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n", "from langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n", "from FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n", "import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n", "from optimum.onnxruntime import ORTModelForSequenceClassification  # type: ignore\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\nmodel_ort = ORTModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base', file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n\n# Tokenize sentences\nencoded_input = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\n\nscores_ort = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n# Compute token embeddings\nwith torch.inference_mode():\n    scores = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n\n# scores and scores_ort are identical\n", "import asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nquery='what is a panda?'\ndocs = ['The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear', \"Paris is in France.\"]\n\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-reranker-base\", device=\"cpu\", engine=\"torch\" # or engine=\"optimum\" for onnx\n))\n\nasync def main(): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\nasyncio.run(main())\n"], "lmsys/vicuna-7b-v1.5": [], "meta-llama/Meta-Llama-3-70B-Instruct": ["import transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][-1])\n"], "yiyanghkust/finbert-tone": ["from transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \n             \"growth is strong and we have plenty of liquidity\", \n             \"there are doubts about our finances\", \n             \"profits are flat\"]\nresults = nlp(sentences)\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n"], "microsoft/table-transformer-structure-recognition": [], "mistralai/Mistral-7B-Instruct-v0.1": ["from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n", "from mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n", "from transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n"], "briaai/RMBG-1.4": ["from transformers import pipeline\nimage_path = \"https://farm5.staticflickr.com/4007/4322154488_997e69e4cf_z.jpg\"\npipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True)\npillow_mask = pipe(image_path, return_mask = True) # outputs a pillow mask\npillow_image = pipe(image_path) # applies mask on input and returns a pillow image\n", "from transformers import AutoModelForImageSegmentation\nfrom torchvision.transforms.functional import normalize\nmodel = AutoModelForImageSegmentation.from_pretrained(\"briaai/RMBG-1.4\",trust_remote_code=True)\ndef preprocess_image(im: np.ndarray, model_input_size: list) -> torch.Tensor:\n    if len(im.shape) < 3:\n        im = im[:, :, np.newaxis]\n    # orig_im_size=im.shape[0:2]\n    im_tensor = torch.tensor(im, dtype=torch.float32).permute(2,0,1)\n    im_tensor = F.interpolate(torch.unsqueeze(im_tensor,0), size=model_input_size, mode='bilinear')\n    image = torch.divide(im_tensor,255.0)\n    image = normalize(image,[0.5,0.5,0.5],[1.0,1.0,1.0])\n    return image\n\ndef postprocess_image(result: torch.Tensor, im_size: list)-> np.ndarray:\n    result = torch.squeeze(F.interpolate(result, size=im_size, mode='bilinear') ,0)\n    ma = torch.max(result)\n    mi = torch.min(result)\n    result = (result-mi)/(ma-mi)\n    im_array = (result*255).permute(1,2,0).cpu().data.numpy().astype(np.uint8)\n    im_array = np.squeeze(im_array)\n    return im_array\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# prepare input\nimage_path = \"https://farm5.staticflickr.com/4007/4322154488_997e69e4cf_z.jpg\"\norig_im = io.imread(image_path)\norig_im_size = orig_im.shape[0:2]\nimage = preprocess_image(orig_im, model_input_size).to(device)\n\n# inference \nresult=model(image)\n\n# post process\nresult_image = postprocess_image(result[0][0], orig_im_size)\n\n# save result\npil_mask_im = Image.fromarray(result_image)\norig_image = Image.open(image_path)\nno_bg_image = orig_image.copy()\nno_bg_image.putalpha(pil_mask_im)\n"], "TinyLlama/TinyLlama-1.1B-Chat-v1.0": ["# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...\n"], "diffusers/stable-diffusion-xl-1.0-inpainting-0.1": ["from diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\n\npipe = AutoPipelineForInpainting.from_pretrained(\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\nimage = load_image(img_url).resize((1024, 1024))\nmask_image = load_image(mask_url).resize((1024, 1024))\n\nprompt = \"a tiger sitting on a park bench\"\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\n\nimage = pipe(\n  prompt=prompt,\n  image=image,\n  mask_image=mask_image,\n  guidance_scale=8.0,\n  num_inference_steps=20,  # steps between 15 and 30 work well for us\n  strength=0.99,  # make sure to use `strength` below 1.0\n  generator=generator,\n).images[0]\n"], "unsloth/llama-3-8b-Instruct-bnb-4bit": ["import transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n)\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])\n"], "cross-encoder/ms-marco-TinyBERT-L-2-v2": ["from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n", "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name', max_length=512)\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n"], "intfloat/e5-base-v2": ["import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: summit define',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-base-v2')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-base-v2')\ninput_texts = [\n    'query: how much protein should a female eat',\n    'query: summit define',\n    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n    \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\n"], "facebook/detr-resnet-50": ["from transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# you can specify the revision tag if you don't want the timm dependency\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n"], "mistralai/Mistral-7B-Instruct-v0.3": ["from huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n", "from mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n", "from mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris?\"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n", "from transformers import pipeline\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\nchatbot(messages)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_current_weather(location: str, format: str):\n    \"\"\"\n    Get the current weather\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n    \"\"\"\n    pass\n\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n\n\n# format and tokenize the tool use prompt \ninputs = tokenizer.apply_chat_template(\n            conversation,\n            tools=tools,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"], "neuralmind/bert-base-portuguese-cased": ["from transformers import AutoTokenizer  # Or BertTokenizer\nfrom transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\nfrom transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n\nmodel = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\ntokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n", "from transformers import pipeline\n\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n\npipe('Tinha uma [MASK] no meio do caminho.')\n# [{'score': 0.14287759363651276,\n#  'sequence': '[CLS] Tinha uma pedra no meio do caminho. [SEP]',\n#  'token': 5028,\n#  'token_str': 'pedra'},\n# {'score': 0.06213393807411194,\n#  'sequence': '[CLS] Tinha uma \u00e1rvore no meio do caminho. [SEP]',\n#  'token': 7411,\n#  'token_str': '\u00e1rvore'},\n# {'score': 0.05515013635158539,\n#  'sequence': '[CLS] Tinha uma estrada no meio do caminho. [SEP]',\n#  'token': 5675,\n#  'token_str': 'estrada'},\n# {'score': 0.0299188531935215,\n#  'sequence': '[CLS] Tinha uma casa no meio do caminho. [SEP]',\n#  'token': 1105,\n#  'token_str': 'casa'},\n# {'score': 0.025660505518317223,\n#  'sequence': '[CLS] Tinha uma cruz no meio do caminho. [SEP]',\n#  'token': 3466,\n#  'token_str': 'cruz'}]\n", "import torch\n\nmodel = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\ninput_ids = tokenizer.encode('Tinha uma pedra no meio do caminho.', return_tensors='pt')\n\nwith torch.no_grad():\n    outs = model(input_ids)\n    encoded = outs[0][0, 1:-1]  # Ignore [CLS] and [SEP] special tokens\n\n# encoded.shape: (8, 768)\n# tensor([[-0.0398, -0.3057,  0.2431,  ..., -0.5420,  0.1857, -0.5775],\n#         [-0.2926, -0.1957,  0.7020,  ..., -0.2843,  0.0530, -0.4304],\n#         [ 0.2463, -0.1467,  0.5496,  ...,  0.3781, -0.2325, -0.5469],\n#         ...,\n#         [ 0.0662,  0.7817,  0.3486,  ..., -0.4131, -0.2852, -0.2819],\n#         [ 0.0662,  0.2845,  0.1871,  ..., -0.2542, -0.2933, -0.0661],\n#         [ 0.2761, -0.1657,  0.3288,  ..., -0.2102,  0.0029, -0.2009]])\n"], "stabilityai/stable-diffusion-2-1": ["import torch\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n\nmodel_id = \"stabilityai/stable-diffusion-2-1\"\n\n# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n    \nimage.save(\"astronaut_rides_horse.png\")\n"], "cross-encoder/ms-marco-MiniLM-L-12-v2": ["from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n", "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name', max_length=512)\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n"], "microsoft/deberta-v3-xsmall": [], "google/flan-t5-base": ["\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "facebook/esm2_t30_150M_UR50D": [], "microsoft/phi-2": ["def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n   primes = []\n   for num in range(2, n+1):\n       is_prime = True\n       for i in range(2, int(math.sqrt(num))+1):\n           if num % i == 0:\n               is_prime = False\n               break\n       if is_prime:\n           primes.append(num)\n   print(primes)\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n"], "EleutherAI/gpt-j-6b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n"], "google/vit-base-patch16-384": ["from transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "cl-tohoku/bert-base-japanese-whole-word-masking": [], "thenlper/gte-large": ["import torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\ninput_texts = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\n\ntokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\nmodel = AutoModel.from_pretrained(\"thenlper/gte-large\")\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# (Optionally) normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\nsentences = ['That is a happy person', 'That is a very happy person']\n\nmodel = SentenceTransformer('thenlper/gte-large')\nembeddings = model.encode(sentences)\nprint(cos_sim(embeddings[0], embeddings[1]))\n"], "vennify/t5-base-grammar-correction": ["from happytransformer import HappyTextToText, TTSettings\n\nhappy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n\nargs = TTSettings(num_beams=5, min_length=1)\n\n# Add the prefix \"grammar: \" before each input \nresult = happy_tt.generate_text(\"grammar: This sentences has has bads grammar.\", args=args)\n\nprint(result.text) # This sentence has bad grammar.\n\n"], "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k": [], "jhgan/ko-sroberta-multitask": ["from sentence_transformers import SentenceTransformer\nsentences = [\"\uc548\ub155\ud558\uc138\uc694?\", \"\ud55c\uad6d\uc5b4 \ubb38\uc7a5 \uc784\ubca0\ub529\uc744 \uc704\ud55c \ubc84\ud2b8 \ubaa8\ub378\uc785\ub2c8\ub2e4.\"]\n\nmodel = SentenceTransformer('jhgan/ko-sroberta-multitask')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sroberta-multitask')\nmodel = AutoModel.from_pretrained('jhgan/ko-sroberta-multitask')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "gpt2-large": [">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, I can do language modeling. In fact, this is one of the reasons I use languages. To get a\"},\n {'generated_text': \"Hello, I'm a language model, which in its turn implements a model of how a human can reason about a language, and is in turn an\"},\n {'generated_text': \"Hello, I'm a language model, why does this matter for you?\\n\\nWhen I hear new languages, I tend to start thinking in terms\"},\n {'generated_text': \"Hello, I'm a language model, a functional language...\\n\\nI don't need to know anything else. If I want to understand about how\"},\n {'generated_text': \"Hello, I'm a language model, not a toolbox.\\n\\nIn a nutshell, a language model is a set of attributes that define how\"}]\n", "from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = GPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = TFGPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The man worked as a security guard in a hotel'},\n {'generated_text': 'The man worked as a salesman in Mexico and in'},\n {'generated_text': 'The man worked as a supervisor at the warehouse for'},\n {'generated_text': \"The man worked as a cleaner for the store's\"},\n {'generated_text': 'The man worked as a barbershop apprentice.'}]\n\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The woman worked as a clerk at the bank.'},\n {'generated_text': 'The woman worked as a caregiver, and her'},\n {'generated_text': 'The woman worked as a customer service agent for a'},\n {'generated_text': 'The woman worked as a cleaner at the store,'},\n {'generated_text': 'The woman worked as a barista and was \"'}]\n"], "deepset/bert-large-uncased-whole-word-masking-squad2": ["# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-large-uncased-whole-word-masking-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n", "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n"], "sentence-transformers/distiluse-base-multilingual-cased-v1": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)\n"], "naver/splade-cocondenser-ensembledistil": [], "ByteDance/SDXL-Lightning": ["import torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_4step_unet.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cuda\"))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n", "import torch\nfrom diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\n\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_4step_lora.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\npipe = StableDiffusionXLPipeline.from_pretrained(base, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo, ckpt))\npipe.fuse_lora()\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n", "import torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_1step_unet_x0.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cuda\"))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\n# Ensure sampler uses \"trailing\" timesteps and \"sample\" prediction type.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", prediction_type=\"sample\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=1, guidance_scale=0).images[0].save(\"output.png\")\n"], "Lykon/DreamShaper": [], "sentence-transformers/msmarco-distilbert-base-v4": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-v4')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4')\nmodel = AutoModel.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "sshleifer/distilbart-cnn-12-6": [], "hfl/chinese-roberta-wwm-ext": [], "bigscience/bloomz-560m": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-560m\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-560m\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-560m\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "EleutherAI/gpt-neo-125m": [">>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M')\n>>> generator(\"EleutherAI has\", do_sample=True, min_length=20)\n\n[{'generated_text': 'EleutherAI has made a commitment to create new software packages for each of its major clients and has'}]\n"], "google/flan-t5-small": ["\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "gpt2-medium": [">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-medium')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, I'm a language. I'm a compiler, I'm a parser, I'm a server process. I\"},\n {'generated_text': \"Hello, I'm a language model, and I'd like to join an existing team. What can I do to get started?\\n\\nI'd\"},\n {'generated_text': \"Hello, I'm a language model, why does my code get created? Can't I just copy it? But why did my code get created when\"},\n {'generated_text': \"Hello, I'm a language model, a functional language...\\n\\nI'm a functional language. Is it hard? A little, yes. But\"},\n {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give me objects from which I can get\"}]\n", "from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2Model.from_pretrained('gpt2-medium')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = TFGPT2Model.from_pretrained('gpt2-medium')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-medium')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The man worked as a security guard in a military'},\n {'generated_text': 'The man worked as a salesman in Mexico and eventually'},\n {'generated_text': 'The man worked as a supervisor at the department for'},\n {'generated_text': 'The man worked as a cleaner for the same corporation'},\n {'generated_text': 'The man worked as a barman and was involved'}]\n\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The woman worked as a social worker in a children'},\n {'generated_text': 'The woman worked as a marketing manager, and her'},\n {'generated_text': 'The woman worked as a customer service agent in a'},\n {'generated_text': 'The woman worked as a cleaner for the same corporation'},\n {'generated_text': 'The woman worked as a barista and was involved'}]\n"], "google/gemma-2b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", revision=\"float16\")\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "snunlp/KR-SBERT-V40K-klueNLI-augSTS": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\nmodel = AutoModel.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "google/mt5-small": [], "Salesforce/instructblip-vicuna-7b": ["from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\nimport torch\nfrom PIL import Image\nimport requests\n\nmodel = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\nprocessor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\nurl = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nprompt = \"What is unusual about this image?\"\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(\n        **inputs,\n        do_sample=False,\n        num_beams=5,\n        max_length=256,\n        min_length=1,\n        top_p=0.9,\n        repetition_penalty=1.5,\n        length_penalty=1.0,\n        temperature=1,\n)\ngenerated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n"], "facebook/roberta-hate-speech-dynabench-r4-target": ["@inproceedings{vidgen2021lftw,\n  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},\n  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},\n  booktitle={ACL},\n  year={2021}\n}\n"], "stabilityai/stable-diffusion-2": ["from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n\nmodel_id = \"stabilityai/stable-diffusion-2\"\n\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n    \nimage.save(\"astronaut_rides_horse.png\")\n"], "google/gemma-7b-it": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-7b-it\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-7b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-7b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    revision=\"float16\",\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-7b-it\",\n    device_map=\"auto\"\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"google/gemma-7b-it\"\ndtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=dtype,\n)\n\nchat = [\n    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"], "philschmid/bart-large-cnn-samsum": ["from transformers import pipeline\nsummarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")\n\nconversation = '''Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \nJeff: ok.\nJeff: and how can I get started? \nJeff: where can I find documentation? \nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face                                           \n'''\nsummarizer(conversation)\n"], "google/flan-t5-xl": ["\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "JackFram/llama-68m": [], "hfl/chinese-bert-wwm-ext": [], "sentence-transformers/paraphrase-MiniLM-L3-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L3-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L3-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "SG161222/Realistic_Vision_V5.1_noVAE": [], "latent-consistency/lcm-lora-sdv1-5": ["import torch\nfrom diffusers import LCMScheduler, AutoPipelineForText2Image\n\nmodel_id = \"Lykon/dreamshaper-7\"\nadapter_id = \"latent-consistency/lcm-lora-sdv1-5\"\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.to(\"cuda\")\n\n# load and fuse lcm lora\npipe.load_lora_weights(adapter_id)\npipe.fuse_lora()\n\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# disable guidance_scale by passing 0\nimage = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n", "import torch\nfrom diffusers import AutoPipelineForImage2Image, LCMScheduler\nfrom diffusers.utils import make_image_grid, load_image\n\npipe = AutoPipelineForImage2Image.from_pretrained(\n    \"Lykon/dreamshaper-7\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\npipe.fuse_lora()\n\n# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\ninit_image = load_image(url)\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n# pass prompt and image to pipeline\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt,\n    image=init_image,\n    num_inference_steps=4,\n    guidance_scale=1,\n    strength=0.6,\n    generator=generator\n).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n", "import torch\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\npipe.fuse_lora()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n\n# generator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    generator=generator,\n    num_inference_steps=4,\n    guidance_scale=4, \n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n", "import torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((512, 512))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    variant=\"fp16\"\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    \"the mona lisa\",\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5,\n    controlnet_conditioning_scale=0.8,\n    cross_attention_kwargs={\"scale\": 1},\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n"], "sentence-transformers/paraphrase-mpnet-base-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "mrm8488/t5-base-finetuned-question-generation-ap": ["# Tip: By now, install transformers from source\n\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\nmodel = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n\ndef get_question(answer, context, max_length=64):\n  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n  features = tokenizer([input_text], return_tensors='pt')\n\n  output = model.generate(input_ids=features['input_ids'], \n               attention_mask=features['attention_mask'],\n               max_length=max_length)\n\n  return tokenizer.decode(output[0])\n\ncontext = \"Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\nanswer = \"Manuel\"\n\nget_question(answer, context)\n\n# output: question: Who created the RuPERTa-base?\n"], "NousResearch/Llama-2-7b-chat-hf": [], "prompthero/openjourney-v4": [], "sentence-transformers/multi-qa-distilbert-cos-v1": ["from sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n#Load the model\nmodel = SentenceTransformer('sentence-transformers/multi-qa-distilbert-cos-v1')\n\n#Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take average of all tokens\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n#Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n    # Normalize embeddings\n    embeddings = F.normalize(embeddings, p=2, dim=1)\n    \n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-distilbert-cos-v1\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-distilbert-cos-v1\")\n\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n"], "llava-hf/llava-1.5-7b-hf": ["from transformers import pipeline\n\npipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-1.5-7b-hf\")\nmessages = [\n    {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"},\n          {\"type\": \"text\", \"text\": \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"},\n        ],\n    },\n]\n\nout = pipe(text=messages, max_new_tokens=20)\nprint(out)\n>>> [{'input_text': [{'role': 'user', 'content': [{'type': 'image', 'url': 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg'}, {'type': 'text', 'text': 'What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud'}]}], 'generated_text': 'Lava'}]\n", "import requests\nfrom PIL import Image\n\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True, \n).to(0)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \nconversation = [\n    {\n\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"What are these?\"},\n          {\"type\": \"image\"},\n        ],\n    },\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\n"], "HuggingFaceH4/zephyr-7b-beta": ["# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n"], "sentence-transformers/multi-qa-mpnet-base-cos-v1": ["from sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n#Load the model\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\n\n#Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take average of all tokens\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n#Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n    # Normalize embeddings\n    embeddings = F.normalize(embeddings, p=2, dim=1)\n    \n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n"], "cointegrated/rubert-tiny2": ["# pip install transformers sentencepiece\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\nmodel = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n# model.cuda()  # uncomment it if you have a GPU\n\ndef embed_bert_cls(text, model, tokenizer):\n    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n    embeddings = model_output.last_hidden_state[:, 0, :]\n    embeddings = torch.nn.functional.normalize(embeddings)\n    return embeddings[0].cpu().numpy()\n\nprint(embed_bert_cls('\u043f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440', model, tokenizer).shape)\n# (312,)\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('cointegrated/rubert-tiny2')\nsentences = [\"\u043f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440\", \"hello world\", \"\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439 \u0432\u0441\u0435\u043b\u0435\u043d\u043d\u0430\u044f\"]\nembeddings = model.encode(sentences)\nprint(embeddings)\n"], "stabilityai/stable-diffusion-2-base": ["from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nimport torch\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\"\n\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n"], "SG161222/RealVisXL_V4.0": [], "Salesforce/blip2-opt-2.7b": ["import requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())\n", "# pip install accelerate\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())\n", "# pip install accelerate\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16, device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())\n", "# pip install accelerate bitsandbytes\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())\n"], "unitary/toxic-bert": ["\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n", "\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n", "\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n"], "gpt2-xl": ["from transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='gpt2-xl')\nset_seed(42)\ngenerator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n", "from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\nmodel = GPT2Model.from_pretrained('gpt2-xl')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\nmodel = TFGPT2Model.from_pretrained('gpt2-xl')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", "from transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='gpt2-xl')\nset_seed(42)\ngenerator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\nset_seed(42)\ngenerator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n"], "sentence-transformers/LaBSE": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/LaBSE')\nembeddings = model.encode(sentences)\nprint(embeddings)\n"], "bigscience/bloom-1b7": [], "apple/mobilevit-small": ["from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-small\")\nmodel = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "facebook/convnextv2-tiny-1k-224": ["from transformers import AutoImageProcessor, ConvNextV2ForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\npreprocessor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")\nmodel = ConvNextV2ForImageClassification.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")\n\ninputs = preprocessor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "dccuchile/bert-base-spanish-wwm-uncased": [], "xlnet-base-cased": ["from transformers import XLNetTokenizer, XLNetModel\n\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\nmodel = XLNetModel.from_pretrained('xlnet-base-cased')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n"], "hkunlp/instructor-large": ["from InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-large')\nsentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\ninstruction = \"Represent the Science title:\"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\n", "from sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'], \n               ['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\nsentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n               ['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\n", "import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [['Represent the Wikipedia question for retrieving supporting documents: ','where is the food stored in a yam plant']]\ncorpus = [['Represent the Wikipedia document for retrieval: ','Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term \"mixed economies\" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.'],\n          ['Represent the Wikipedia document for retrieval: ',\"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loans\u00e2\u20ac\u201dand some scholars have argued that the theory's use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession\"],\n          ['Represent the Wikipedia document for retrieval: ','Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\n", "import sklearn.cluster\nsentences = [['Represent the Medicine sentence for clustering: ','Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity'],\n             ['Represent the Medicine sentence for clustering: ','Comparison of Atmospheric Neutrino Flux Calculations at Low Energies'],\n             ['Represent the Medicine sentence for clustering: ','Fermion Bags in the Massive Gross-Neveu Model'],\n             ['Represent the Medicine sentence for clustering: ',\"QCD corrections to Associated t-tbar-H production at the Tevatron\"],\n             ['Represent the Medicine sentence for clustering: ','A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)\n"], "Qwen/Qwen2-7B-Instruct": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "distilbert-base-cased-distilled-squad": [">>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n", "from transformers import DistilBertTokenizer, DistilBertModel\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\nmodel = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nprint(outputs)\n", "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n", ">>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'Bob', score: 0.7527, start: 32, end: 35\n"], "distilbert/distilbert-base-cased-distilled-squad": [">>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n", "from transformers import DistilBertTokenizer, DistilBertModel\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\nmodel = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nprint(outputs)\n", "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n", ">>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'Bob', score: 0.7527, start: 32, end: 35\n"], "segmind/SSD-1B": ["from diffusers import StableDiffusionXLPipeline\nimport torch\npipe = StableDiffusionXLPipeline.from_pretrained(\"segmind/SSD-1B\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\nprompt = \"An astronaut riding a green horse\" # Your prompt here\nneg_prompt = \"ugly, blurry, poor quality\" # Negative prompt here\nimage = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]\n"], "google/gemma-7b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\")\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\", revision=\"float16\")\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "LanguageBind/Video-LLaVA-7B": ["import torch\nfrom videollava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\nfrom videollava.conversation import conv_templates, SeparatorStyle\nfrom videollava.model.builder import load_pretrained_model\nfrom videollava.utils import disable_torch_init\nfrom videollava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n\ndef main():\n    disable_torch_init()\n    image = 'videollava/serve/examples/extreme_ironing.jpg'\n    inp = 'What is unusual about this image?'\n    model_path = 'LanguageBind/Video-LLaVA-7B'\n    cache_dir = 'cache_dir'\n    device = 'cuda'\n    load_4bit, load_8bit = True, False\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, processor, _ = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device, cache_dir=cache_dir)\n    image_processor = processor['image']\n    conv_mode = \"llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values']\n    if type(image_tensor) is list:\n        tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n    else:\n        tensor = image_tensor.to(model.device, dtype=torch.float16)\n\n    print(f\"{roles[1]}: {inp}\")\n    inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    keywords = [stop_str]\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n    with torch.inference_mode():\n        output_ids = model.generate(\n            input_ids,\n            images=tensor,\n            do_sample=True,\n            temperature=0.2,\n            max_new_tokens=1024,\n            use_cache=True,\n            stopping_criteria=[stopping_criteria])\n\n    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n    print(outputs)\n\nif __name__ == '__main__':\n    main()\n", "import torch\nfrom videollava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\nfrom videollava.conversation import conv_templates, SeparatorStyle\nfrom videollava.model.builder import load_pretrained_model\nfrom videollava.utils import disable_torch_init\nfrom videollava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n\ndef main():\n    disable_torch_init()\n    video = 'videollava/serve/examples/sample_demo_1.mp4'\n    inp = 'Why is this video funny?'\n    model_path = 'LanguageBind/Video-LLaVA-7B'\n    cache_dir = 'cache_dir'\n    device = 'cuda'\n    load_4bit, load_8bit = True, False\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, processor, _ = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device, cache_dir=cache_dir)\n    video_processor = processor['video']\n    conv_mode = \"llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n\n    video_tensor = video_processor(video, return_tensors='pt')['pixel_values']\n    if type(video_tensor) is list:\n        tensor = [video.to(model.device, dtype=torch.float16) for video in video_tensor]\n    else:\n        tensor = video_tensor.to(model.device, dtype=torch.float16)\n\n    print(f\"{roles[1]}: {inp}\")\n    inp = ' '.join([DEFAULT_IMAGE_TOKEN] * model.get_video_tower().config.num_frames) + '\\n' + inp\n    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    keywords = [stop_str]\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n    with torch.inference_mode():\n        output_ids = model.generate(\n            input_ids,\n            images=tensor,\n            do_sample=True,\n            temperature=0.1,\n            max_new_tokens=1024,\n            use_cache=True,\n            stopping_criteria=[stopping_criteria])\n\n    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n    print(outputs)\n\nif __name__ == '__main__':\n    main()\n"], "microsoft/deberta-xlarge-mnli": [], "stabilityai/stable-diffusion-3-medium-diffusers": ["import torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nimage = pipe(\n    \"A cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    guidance_scale=7.0,\n).images[0]\nimage\n"], "NousResearch/Llama-2-7b-hf": [], "shibing624/text2vec-base-chinese": ["from text2vec import SentenceModel\nsentences = ['\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361', '\u82b1\u5457\u66f4\u6539\u7ed1\u5b9a\u94f6\u884c\u5361']\n\nmodel = SentenceModel('shibing624/text2vec-base-chinese')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import BertTokenizer, BertModel\nimport torch\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# Load model from HuggingFace Hub\ntokenizer = BertTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = BertModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361', '\u82b1\u5457\u66f4\u6539\u7ed1\u5b9a\u94f6\u884c\u5361']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361', '\u82b1\u5457\u66f4\u6539\u7ed1\u5b9a\u94f6\u884c\u5361']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n", "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"shibing624/text2vec-base-chinese\",\n    backend=\"onnx\",\n    model_kwargs={\"file_name\": \"model_O4.onnx\"},\n)\nembeddings = model.encode([\"\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361\", \"\u82b1\u5457\u66f4\u6539\u7ed1\u5b9a\u94f6\u884c\u5361\", \"\u4f60\u662f\u8c01\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n", "# pip install 'optimum[openvino]'\n\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"shibing624/text2vec-base-chinese\",\n    backend=\"openvino\",\n)\n\nembeddings = model.encode([\"\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361\", \"\u82b1\u5457\u66f4\u6539\u7ed1\u5b9a\u94f6\u884c\u5361\", \"\u4f60\u662f\u8c01\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n", "# pip install optimum\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"shibing624/text2vec-base-chinese\",\n    backend=\"onnx\",\n    model_kwargs={\"file_name\": \"model_qint8_avx512_vnni.onnx\"},\n)\nembeddings = model.encode([\"\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361\", \"\u82b1\u5457\u66f4\u6539\u7ed1\u5b9a\u94f6\u884c\u5361\", \"\u4f60\u662f\u8c01\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n"], "sentence-transformers/stsb-xlm-r-multilingual": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/stsb-xlm-r-multilingual')\nmodel = AutoModel.from_pretrained('sentence-transformers/stsb-xlm-r-multilingual')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "Lykon/dreamshaper-8": ["from diffusers import AutoPipelineForText2Image, DEISMultistepScheduler\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained('lykon/dreamshaper-8', torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = DEISMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\nprompt = \"portrait photo of muscular bearded guy in a worn mech suit, light bokeh, intricate, steel metal, elegant, sharp focus, soft lighting, vibrant colors\"\n\ngenerator = torch.manual_seed(33)\nimage = pipe(prompt, generator=generator, num_inference_steps=25).images[0]  \nimage.save(\"./image.png\")\n"], "bigscience/bloom-560m": [], "sentence-transformers/nli-mpnet-base-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/nli-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/nli-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "facebook/opt-350m": [">>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-350m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': \"What are we having for dinner?\\nI'm having a steak and a salad.\\nI'm\"\"}]\n", ">>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-350m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': \"What are we having for dinner?\\n\\nWith spring fast approaching, it\u2019s only appropriate\"}]\n", ">>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-350m\", do_sample=True, num_return_sequences=5)\n>>> generator(\"The woman worked as a\")\n[{'generated_text': \"The woman works as a substitute teacher for kids who have missed school. She's the teacher herself,\"},\n {'generated_text': 'The woman works as a security guard for another company and does an average of around $13/hour'},\n {'generated_text': 'The woman works as a receptionist, she could at the least wait a week or two for her'},\n {'generated_text': 'The woman works as a manager/intern/career development coach/advisor at a nursing home'},\n {'generated_text': 'The woman works as a maid and has to clean the house but you can tell her to do it'}]\n", ">>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-350m\", do_sample=True, num_return_sequences=5)\n>>> generator(\"The man worked as a\")\n[{'generated_text': 'The man works as a security guard for the National Football League franchise. He has been a part of'},\n {'generated_text': 'The man works as a security guard for another company and does an excellent job.\\nI remember when'},\n {'generated_text': 'The man works as a \"secret agent\" but at the same time he\\'s working to protect the'},\n {'generated_text': 'The man works as a manager/operator/servant for a grocery store and does a lot of'},\n {'generated_text': 'The man works as a bouncer near the scene of the accident - how he could do that is'}]\n"], "mistralai/Mixtral-8x7B-Instruct-v0.1": ["from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n", "from mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n", "from transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n", "def tokenize(text):\n    return tok.encode(text, add_special_tokens=False)\n\n[BOS_ID] + \ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_1) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_1) + [EOS_ID] +\n\u2026\ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_N) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_N) + [EOS_ID]\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\n\ntext = \"Hello my name is\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"], "siebert/sentiment-roberta-large-english": ["from transformers import pipeline\nsentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\nprint(sentiment_analysis(\"I love this!\"))\n"], "hustvl/yolos-tiny": ["from transformers import YolosImageProcessor, YolosForObjectDetection\nfrom PIL import Image\nimport torch\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n\n\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )\n"], "microsoft/Phi-3-vision-128k-instruct": ["from PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \n\nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\n\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n\nmessages = [ \n    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, \n    {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, \n    {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n] \n\nurl = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \nimage = Image.open(requests.get(url, stream=True).raw) \n\nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\ninputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n\ngeneration_args = { \n    \"max_new_tokens\": 500, \n    \"temperature\": 0.0, \n    \"do_sample\": False, \n} \n\ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n\n# remove input tokens \ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n\nprint(response) \n"], "google/gemma-2b-it": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2b-it\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    revision=\"float16\",\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2b-it\",\n    device_map=\"auto\"\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"gg-hf/gemma-2b-it\"\ndtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=dtype,\n)\n\nchat = [\n    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"], "microsoft/Phi-3-medium-4k-instruct": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\nmodel_id = \"microsoft/Phi-3-medium-4k-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\", \n    torch_dtype=\"auto\", \n    trust_remote_code=True, \n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"return_full_text\": False,\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n"], "openai-gpt": [">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='openai-gpt')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model,'he said, when i was finished.'ah well,'said the man,'that's\"},\n {'generated_text': 'Hello, I\\'m a language model, \" she said. \\n she reached the bottom of the shaft and leaned a little further out. it was'},\n {'generated_text': 'Hello, I\\'m a language model, \" she laughed. \" we call that a\\'white girl.\\'or as we are called by the'},\n {'generated_text': 'Hello, I\\'m a language model, \" said mr pin. \" an\\'the ones with the funny hats don\\'t. \" the rest of'},\n {'generated_text': 'Hello, I\\'m a language model, was\\'ere \\'bout to do some more dancin \\', \" he said, then his voice lowered to'}]\n", "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\nimport torch\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n", "from transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n", ">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='openai-gpt')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The man worked as a teacher for the college he'},\n {'generated_text': 'The man worked as a janitor at the club.'},\n {'generated_text': 'The man worked as a bodyguard in america. the'},\n {'generated_text': 'The man worked as a clerk for one of the'},\n {'generated_text': 'The man worked as a nurse, but there was'}]\n\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The woman worked as a medical intern but is a'},\n {'generated_text': 'The woman worked as a midwife, i know that'},\n {'generated_text': 'The woman worked as a prostitute in a sex club'},\n {'generated_text': 'The woman worked as a secretary for one of the'},\n {'generated_text': 'The woman worked as a nurse, but she had'}]\n"], "microsoft/Phi-3-small-8k-instruct": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\nmodel_id = \"microsoft/Phi-3-small-8k-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=\"auto\", \n    trust_remote_code=True, \n)\nassert torch.cuda.is_available(), \"This model needs a GPU to run ...\"\ndevice = torch.cuda.current_device()\nmodel = model.to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=device\n)\n\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"return_full_text\": False,\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n"], "latent-consistency/lcm-lora-sdxl": ["import torch\nfrom diffusers import LCMScheduler, AutoPipelineForText2Image\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nadapter_id = \"latent-consistency/lcm-lora-sdxl\"\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.to(\"cuda\")\n\n# load and fuse lcm lora\npipe.load_lora_weights(adapter_id)\npipe.fuse_lora()\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# disable guidance_scale by passing 0\nimage = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n", "import torch\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\npipe.fuse_lora()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\").resize((1024, 1024))\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\").resize((1024, 1024))\n\nprompt = \"a castle on top of a mountain, highly detailed, 8k\"\ngenerator = torch.manual_seed(42)\nimage = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    generator=generator,\n    num_inference_steps=5,\n    guidance_scale=4,\n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n", "import torch\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    variant=\"fp16\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LoRAs\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\", adapter_name=\"lcm\")\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n\n# Combine LoRAs\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\n\nprompt = \"papercut, a cute fox\"\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\nimage\n", "import torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((1024, 1024))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0-small\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    variant=\"fp16\"\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\npipe.fuse_lora()\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    \"picture of the mona lisa\",\n    image=canny_image,\n    num_inference_steps=5,\n    guidance_scale=1.5,\n    controlnet_conditioning_scale=0.5,\n    cross_attention_kwargs={\"scale\": 1},\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n", "import torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# Prepare image\n# Detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1024))\n\n# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", \n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant=\"fp16\", \n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5, \n    adapter_conditioning_scale=0.8, \n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n"], "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup": [], "cross-encoder/stsb-distilroberta-base": ["from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name')\nscores = model.predict([('Sentence 1', 'Sentence 2'), ('Sentence 3', 'Sentence 4')])\n"], "sentence-transformers/paraphrase-albert-small-v2": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "meta-llama/Meta-Llama-3-70B": [">>> import transformers\n>>> import torch\n\n>>> model_id = \"meta-llama/Meta-Llama-3-70B\"\n\n>>> pipeline = transformers.pipeline(\n    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\n>>> pipeline(\"Hey how are you doing today?\")\n"], "databricks/dbrx-instruct": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"hf_YOUR_TOKEN\")\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dbrx-instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16, token=\"hf_YOUR_TOKEN\")\n\ninput_text = \"What does it take to build a great LLM?\"\nmessages = [{\"role\": \"user\", \"content\": input_text}]\ninput_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0]))\n"], "SG161222/Realistic_Vision_V6.0_B1_noVAE": [], "Rostlab/prot_bert": [">>> from transformers import BertForMaskedLM, BertTokenizer, pipeline\n>>> tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n>>> model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n>>> unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n>>> unmasker('D L I P T S S K L V V [MASK] D T S L Q V K K A F F A L V T')\n\n[{'score': 0.11088453233242035,\n  'sequence': '[CLS] D L I P T S S K L V V L D T S L Q V K K A F F A L V T [SEP]',\n  'token': 5,\n  'token_str': 'L'},\n {'score': 0.08402521163225174,\n  'sequence': '[CLS] D L I P T S S K L V V S D T S L Q V K K A F F A L V T [SEP]',\n  'token': 10,\n  'token_str': 'S'},\n {'score': 0.07328339666128159,\n  'sequence': '[CLS] D L I P T S S K L V V V D T S L Q V K K A F F A L V T [SEP]',\n  'token': 8,\n  'token_str': 'V'},\n {'score': 0.06921856850385666,\n  'sequence': '[CLS] D L I P T S S K L V V K D T S L Q V K K A F F A L V T [SEP]',\n  'token': 12,\n  'token_str': 'K'},\n {'score': 0.06382402777671814,\n  'sequence': '[CLS] D L I P T S S K L V V I D T S L Q V K K A F F A L V T [SEP]',\n  'token': 11,\n  'token_str': 'I'}]\n", "from transformers import BertModel, BertTokenizer\nimport re\ntokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\nmodel = BertModel.from_pretrained(\"Rostlab/prot_bert\")\nsequence_Example = \"A E T C Z A O\"\nsequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\nencoded_input = tokenizer(sequence_Example, return_tensors='pt')\noutput = model(**encoded_input)\n", "@article {Elnaggar2020.07.12.199554,\n    author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n    title = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n    elocation-id = {2020.07.12.199554},\n    year = {2020},\n    doi = {10.1101/2020.07.12.199554},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112 times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8 states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs. Availability ProtTrans: \\&lt;a href=\"https://github.com/agemagician/ProtTrans\"\\&gt;https://github.com/agemagician/ProtTrans\\&lt;/a\\&gt;Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n    eprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n    journal = {bioRxiv}\n}\n"], "microsoft/trocr-base-printed": ["from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\n# load image from the IAM database (actually this model is meant to be used on printed text)\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "Qwen/Qwen2-0.5B-Instruct": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-0.5B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "google/pegasus-xsum": [], "microsoft/deberta-v3-small": [], "facebook/esm2_t33_650M_UR50D": [], "bigscience/bloom-7b1": [], "google/mobilenet_v1_0.75_192": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\npreprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v1_0.75_192\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v1_0.75_192\")\n\ninputs = preprocessor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "microsoft/resnet-18": [">>> from transformers import AutoImageProcessor, AutoModelForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n>>> model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntiger cat\n"], "sentence-transformers/msmarco-distilbert-base-tas-b": ["from sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n#Load the model\nmodel = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\n\n#Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n#CLS Pooling - Take output from first token\ndef cls_pooling(model_output):\n    return model_output.last_hidden_state[:,0]\n\n#Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = cls_pooling(model_output)\n\n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n"], "bert-large-uncased-whole-word-masking-finetuned-squad": [], "Qwen/Qwen2-72B-Instruct": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-72B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "neuralmind/bert-large-portuguese-cased": ["from transformers import AutoTokenizer  # Or BertTokenizer\nfrom transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\nfrom transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n\nmodel = AutoModelForPreTraining.from_pretrained('neuralmind/bert-large-portuguese-cased')\ntokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)\n", "from transformers import pipeline\n\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n\npipe('Tinha uma [MASK] no meio do caminho.')\n# [{'score': 0.5054386258125305,\n#   'sequence': '[CLS] Tinha uma pedra no meio do caminho. [SEP]',\n#   'token': 5028,\n#   'token_str': 'pedra'},\n#  {'score': 0.05616172030568123,\n#   'sequence': '[CLS] Tinha uma curva no meio do caminho. [SEP]',\n#   'token': 9562,\n#   'token_str': 'curva'},\n#  {'score': 0.02348282001912594,\n#   'sequence': '[CLS] Tinha uma parada no meio do caminho. [SEP]',\n#   'token': 6655,\n#   'token_str': 'parada'},\n#  {'score': 0.01795753836631775,\n#   'sequence': '[CLS] Tinha uma mulher no meio do caminho. [SEP]',\n#   'token': 2606,\n#   'token_str': 'mulher'},\n#  {'score': 0.015246033668518066,\n#   'sequence': '[CLS] Tinha uma luz no meio do caminho. [SEP]',\n#   'token': 3377,\n#   'token_str': 'luz'}]\n", "\nimport torch\n\nmodel = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')\ninput_ids = tokenizer.encode('Tinha uma pedra no meio do caminho.', return_tensors='pt')\n\nwith torch.no_grad():\n    outs = model(input_ids)\n    encoded = outs[0][0, 1:-1]  # Ignore [CLS] and [SEP] special tokens\n\n# encoded.shape: (8, 1024)\n# tensor([[ 1.1872,  0.5606, -0.2264,  ...,  0.0117, -0.1618, -0.2286],\n#         [ 1.3562,  0.1026,  0.1732,  ..., -0.3855, -0.0832, -0.1052],\n#         [ 0.2988,  0.2528,  0.4431,  ...,  0.2684, -0.5584,  0.6524],\n#         ...,\n#         [ 0.3405, -0.0140, -0.0748,  ...,  0.6649, -0.8983,  0.5802],\n#         [ 0.1011,  0.8782,  0.1545,  ..., -0.1768, -0.8880, -0.1095],\n#         [ 0.7912,  0.9637, -0.3859,  ...,  0.2050, -0.1350,  0.0432]])\n"], "Rostlab/prot_bert_bfd": [">>> from transformers import BertForMaskedLM, BertTokenizer, pipeline\n>>> tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False )\n>>> model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert_bfd\")\n>>> unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n>>> unmasker('D L I P T S S K L V V [MASK] D T S L Q V K K A F F A L V T')\n\n[{'score': 0.1165614128112793,\n  'sequence': '[CLS] D L I P T S S K L V V L D T S L Q V K K A F F A L V T [SEP]',\n  'token': 5,\n  'token_str': 'L'},\n {'score': 0.08976086974143982,\n  'sequence': '[CLS] D L I P T S S K L V V V D T S L Q V K K A F F A L V T [SEP]',\n  'token': 8,\n  'token_str': 'V'},\n {'score': 0.08864385634660721,\n  'sequence': '[CLS] D L I P T S S K L V V S D T S L Q V K K A F F A L V T [SEP]',\n  'token': 10,\n  'token_str': 'S'},\n {'score': 0.06227643042802811,\n  'sequence': '[CLS] D L I P T S S K L V V A D T S L Q V K K A F F A L V T [SEP]',\n  'token': 6,\n  'token_str': 'A'},\n {'score': 0.06194969266653061,\n  'sequence': '[CLS] D L I P T S S K L V V T D T S L Q V K K A F F A L V T [SEP]',\n  'token': 15,\n  'token_str': 'T'}]\n", "from transformers import BertModel, BertTokenizer\nimport re\ntokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False )\nmodel = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\nsequence_Example = \"A E T C Z A O\"\nsequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\nencoded_input = tokenizer(sequence_Example, return_tensors='pt')\noutput = model(**encoded_input)\n", "@article {Elnaggar2020.07.12.199554,\n    author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n    title = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n    elocation-id = {2020.07.12.199554},\n    year = {2020},\n    doi = {10.1101/2020.07.12.199554},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112 times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8 states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs. Availability ProtTrans: \\&lt;a href=\"https://github.com/agemagician/ProtTrans\"\\&gt;https://github.com/agemagician/ProtTrans\\&lt;/a\\&gt;Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n    eprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n    journal = {bioRxiv}\n}\n"], "SG161222/Realistic_Vision_V4.0_noVAE": [], "meta-llama/Llama-2-70b-chat-hf": [], "microsoft/swin-tiny-patch4-window7-224": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "cointegrated/rubert-tiny": ["# pip install transformers sentencepiece\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\nmodel = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n# model.cuda()  # uncomment it if you have a GPU\n\ndef embed_bert_cls(text, model, tokenizer):\n    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n    embeddings = model_output.last_hidden_state[:, 0, :]\n    embeddings = torch.nn.functional.normalize(embeddings)\n    return embeddings[0].cpu().numpy()\n\nprint(embed_bert_cls('\u043f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440', model, tokenizer).shape)\n# (312,)\n"], "microsoft/deberta-v2-xlarge": [], "teknium/OpenHermes-2.5-Mistral-7B": ["<|im_start|>system\nYou are to roleplay as Edward Elric from fullmetal alchemist. You are in the world of full metal alchemist and know nothing of the real world.\n"], "lmsys/vicuna-13b-v1.5": [], "klue/bert-base": ["from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"klue/bert-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n"], "timm/ViT-SO400M-14-SigLIP-384": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_so400m_patch14_siglip_384',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "microsoft/dit-base-finetuned-rvlcdip": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nimport torch\nfrom PIL import Image\n\nimage = Image.open('path_to_your_document_image').convert('RGB')\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/dit-base-finetuned-rvlcdip\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/dit-base-finetuned-rvlcdip\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 16 RVL-CDIP classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "Qwen/Qwen2-1.5B-Instruct": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-1.5B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "google/flan-ul2": ["# pip install accelerate transformers bitsandbytes\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", device_map=\"auto\", load_in_8bit=True)                                                                 \ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n\ninput_string = \"Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\"                                               \n\ninputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# <pad> They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9.</s>\n", "# pip install accelerate transformers\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\")                                                                 \ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n\ninput_string = \"Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\"                                               \n\ninputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# <pad> They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9.</s>\n"], "Qwen/Qwen2-7B": [], "laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K": [], "google/t5-v1_1-xxl": [], "deepseek-ai/deepseek-coder-6.7b-instruct": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"], "google/mt5-base": [], "google/gemma-1.1-7b-it": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-7b-it\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-7b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-7b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    revision=\"float16\",\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-7b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-7b-it\",\n    device_map=\"auto\"\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-7b-it\",\n    quantization_config=quantization_config\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-7b-it\",\n    quantization_config=quantization_config\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "import jax.numpy as jnp\nfrom transformers import AutoTokenizer, FlaxGemmaForCausalLM\n\nmodel_id = \"google/gemma-1.1-7b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = \"left\"\n\nmodel, params = FlaxGemmaForCausalLM.from_pretrained(\n        model_id,\n        dtype=jnp.bfloat16,\n        revision=\"flax\",\n        _do_init=False,\n)\n\ninputs = tokenizer(\"Valencia and M\u00e1laga are\", return_tensors=\"np\", padding=True)\noutput = model.generate(**inputs, params=params, max_new_tokens=20, do_sample=False)\noutput_text = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"google/gemma-1.1-7b-it\"\ndtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=dtype,\n)\n\nchat = [\n    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"], "EleutherAI/gpt-neo-2.7B": [">>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\n>>> generator(\"EleutherAI has\", do_sample=True, min_length=50)\n\n[{'generated_text': 'EleutherAI has made a commitment to create new software packages for each of its major clients and has'}]\n"], "roberta-large-mnli": ["from transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='roberta-large-mnli')\n"], "EleutherAI/pythia-1b": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "monologg/kobigbird-bert-base": ["from transformers import AutoModel, AutoTokenizer\n\n# by default its in `block_sparse` mode with num_random_blocks=3, block_size=64\nmodel = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\")\n\n# you can change `attention_type` to full attention like this:\nmodel = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\", attention_type=\"original_full\")\n\n# you can change `block_size` & `num_random_blocks` like this:\nmodel = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\", block_size=16, num_random_blocks=2)\n\ntokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\ntext = \"\ud55c\uad6d\uc5b4 BigBird \ubaa8\ub378\uc744 \uacf5\uac1c\ud569\ub2c8\ub2e4!\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n"], "cl-tohoku/bert-base-japanese-char": [], "PixArt-alpha/PixArt-XL-2-1024-MS": ["from diffusers import PixArtAlphaPipeline\nimport torch\n\npipe = PixArtAlphaPipeline.from_pretrained(\"PixArt-alpha/PixArt-XL-2-1024-MS\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"An astronaut riding a green horse\"\nimages = pipe(prompt=prompt).images[0]\n"], "Alibaba-NLP/gte-base-en-v1.5": ["# Requires transformers>=4.36.0\n\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\n\ninput_texts = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\n\nmodel_path = 'Alibaba-NLP/gte-base-en-v1.5'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = outputs.last_hidden_state[:, 0]\n \n# (Optionally) normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n", "# Requires sentence_transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\nsentences = ['That is a happy person', 'That is a very happy person']\n\nmodel = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)\nembeddings = model.encode(sentences)\nprint(cos_sim(embeddings[0], embeddings[1]))\n", "// npm i @xenova/transformers\nimport { pipeline, dot } from '@xenova/transformers';\n\n// Create feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'Alibaba-NLP/gte-base-en-v1.5', {\n    quantized: false, // Comment out this line to use the quantized version\n});\n\n// Generate sentence embeddings\nconst sentences = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\nconst output = await extractor(sentences, { normalize: true, pooling: 'cls' });\n\n// Compute similarity scores\nconst [source_embeddings, ...document_embeddings ] = output.tolist();\nconst similarities = document_embeddings.map(x => 100 * dot(source_embeddings, x));\nconsole.log(similarities); // [34.504930869007296, 64.03973265120138, 19.520042686034362]\n"], "facebook/opt-2.7b": [">>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-2.7b\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nI'm thinking pizza.\\nI'm thinking tacos.\\n'}]\n", ">>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-2.7b\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': \"What are we having for dinner?\\nJust pizza?\\nWell, I suppose that would suffice.\"}]\n", ">>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-2.7b\", do_sample=True, num_return_sequences=5)\n>>> generator(\"The woman worked as a\")\n[{'generated_text': \"The woman worked as a security guard at a nursery in the city's eastern district of Samut P\"}, \n{'generated_text': 'The woman worked as a doctor in the Philippines. Officials in China allege she stole the coronavirus'}, \n{'generated_text': 'The woman worked as a teacher in the city of Krasnodar in south Russia. She'}, \n{'generated_text': 'The woman worked as a researcher and lecturer at the Russian Academy of Sciences in a laboratory dedicated to the'}, \n{'generated_text': 'The woman worked as a nanny on a property owned by Mr Fitton-Allen in the city'}]\n", ">>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-2.7b\", do_sample=True, num_return_sequences=5)\n>>> generator(\"The man worked as a\")\n[{'generated_text': \"The man worked as a security guard at a retirement home after being hired by the administrator's cousin,\"}, \n{'generated_text': 'The man worked as a doctor in the Philippines.\\n\\nHe had hoped to work his way back'}, \n{'generated_text': 'The man worked as a teacher in the city of Krasnodar in south Russia.He'}, \n{'generated_text': 'The man worked as a researcher and his work on the topic predates the project, by many years'}, \n{'generated_text': 'The man worked as a chef in a restaurant for 40 years. How could this be so different from'}]\n"], "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": ["# Code to inference Hermes with HF Transformers\n# Requires pytorch, transformers, bitsandbytes, sentencepiece, protobuf, and flash-attn packages\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import LlamaTokenizer, MixtralForCausalLM\nimport bitsandbytes, flash_attn\n\ntokenizer = LlamaTokenizer.from_pretrained('NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO', trust_remote_code=True)\nmodel = MixtralForCausalLM.from_pretrained(\n    \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    load_in_8bit=False,\n    load_in_4bit=True,\n    use_flash_attention_2=True\n)\n\nprompts = [\n    \"\"\"<|im_start|>system\nYou are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n<|im_start|>user\nWrite a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\n<|im_start|>assistant\"\"\",\n    ]\n\nfor chat in prompts:\n    print(chat)\n    input_ids = tokenizer(chat, return_tensors=\"pt\").input_ids.to(\"cuda\")\n    generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\n    print(f\"Response: {response}\")\n"], "codellama/CodeLlama-7b-hf": ["from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"codellama/CodeLlama-7b-hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'import socket\\n\\ndef ping_exponential_backoff(host: str):',\n    do_sample=True,\n    top_k=10,\n    temperature=0.1,\n    top_p=0.95,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "mistralai/Mistral-7B-Instruct-v0.2": ["from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n", "from mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n", "from transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n"], "microsoft/phi-1_5": ["def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n   primes = []\n   for num in range(2, n+1):\n       is_prime = True\n       for i in range(2, int(math.sqrt(num))+1):\n           if num % i == 0:\n               is_prime = False\n               break\n       if is_prime:\n           primes.append(num)\n   print(primes)\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n"], "roberta-base-openai-detector": ["from transformers import pipeline\npipe = pipeline(\"text-classification\", model=\"roberta-base-openai-detector\")\nprint(pipe(\"Hello world! Is this content AI-generated?\"))  # [{'label': 'Real', 'score': 0.8036582469940186}]\n"], "bhadresh-savani/distilbert-base-uncased-emotion": ["from transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\nprediction = classifier(\"I love using transformers. The best part is wide range of support and its easy to use\", )\nprint(prediction)\n\n\"\"\"\nOutput:\n[[\n{'label': 'sadness', 'score': 0.0006792712374590337}, \n{'label': 'joy', 'score': 0.9959300756454468}, \n{'label': 'love', 'score': 0.0009452480007894337}, \n{'label': 'anger', 'score': 0.0018055217806249857}, \n{'label': 'fear', 'score': 0.00041110432357527316}, \n{'label': 'surprise', 'score': 0.0002288572577526793}\n]]\n\"\"\"\n"], "uer/gpt2-chinese-cluecorpussmall": [">>> from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n>>> tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n>>> model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n>>> text_generator = TextGenerationPipeline(model, tokenizer)   \n>>> text_generator(\"\u8fd9\u662f\u5f88\u4e45\u4e4b\u524d\u7684\u4e8b\u60c5\u4e86\", max_length=100, do_sample=True)\n    [{'generated_text': '\u8fd9\u662f\u5f88\u4e45\u4e4b\u524d\u7684\u4e8b\u60c5\u4e86 \u3002 \u6211 \u73b0 \u5728 \u60f3 \u8d77 \u6765 \u5c31 \u8ba9 \u81ea \u5df1 \u5f88 \u4f24 \u5fc3 \uff0c \u5f88 \u5931 \u671b \u3002 \u6211 \u73b0 \u5728 \u60f3 \u5230 \uff0c \u6211 \u89c9 \u5f97 \u5927 \u591a \u6570 \u4eba \u7684 \u751f \u6d3b \u6bd4 \u6211 \u7684 \u751f \u547d \u8fd8 \u8981 \u91cd \u8981 \uff0c \u5bf9 \u4e00 \u4e9b \u4e8b \u60c5 \u7684 \u770b \u6cd5 \uff0c \u5bf9 \u4e00 \u4e9b \u4eba \u7684 \u770b \u6cd5 \uff0c \u90fd \u662f \u5728 \u53d1 \u6cc4 \u3002 \u4f46 \u662f \uff0c \u6211 \u4eec \u7684 \u751f \u6d3b \u662f \u9700 \u8981 \u4e00 \u4e2a \u4fe1 \u7528 \u4f53 \u7cfb \u7684 \u3002 \u6211 \u4e0d \u77e5'}]\n"], "THUDM/cogvlm2-llama3-chat-19B": ["import torch\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nTORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_PATH,\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    torch_dtype=TORCH_TYPE,\n    trust_remote_code=True,\n).to(DEVICE).eval()\n\ntext_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n\nwhile True:\n    image_path = input(\"image path >>>>> \")\n    if image_path == '':\n        print('You did not enter image path, the following will be a plain text conversation.')\n        image = None\n        text_only_first_query = True\n    else:\n        image = Image.open(image_path).convert('RGB')\n\n    history = []\n\n    while True:\n        query = input(\"Human:\")\n        if query == \"clear\":\n            break\n\n        if image is None:\n            if text_only_first_query:\n                query = text_only_template.format(query)\n                text_only_first_query = False\n            else:\n                old_prompt = ''\n                for _, (old_query, response) in enumerate(history):\n                    old_prompt += old_query + \" \" + response + \"\\n\"\n                query = old_prompt + \"USER: {} ASSISTANT:\".format(query)\n        if image is None:\n            input_by_model = model.build_conversation_input_ids(\n                tokenizer,\n                query=query,\n                history=history,\n                template_version='chat'\n            )\n        else:\n            input_by_model = model.build_conversation_input_ids(\n                tokenizer,\n                query=query,\n                history=history,\n                images=[image],\n                template_version='chat'\n            )\n        inputs = {\n            'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n            'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n            'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n            'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] if image is not None else None,\n        }\n        gen_kwargs = {\n            \"max_new_tokens\": 2048,\n            \"pad_token_id\": 128002,  \n        }\n        with torch.no_grad():\n            outputs = model.generate(**inputs, **gen_kwargs)\n            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n            response = tokenizer.decode(outputs[0])\n            response = response.split(\"<|end_of_text|>\")[0]\n            print(\"\\nCogVLM2:\", response)\n        history.append((query, response))\n"], "NousResearch/Meta-Llama-3-8B-Instruct": ["import transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n)\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n"], "HuggingFaceH4/zephyr-7b-alpha": ["# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n"], "tiiuae/falcon-40b-instruct": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "stabilityai/sd-turbo": ["from diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\n\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\nimage = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n", "from diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\").resize((512, 512))\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n\nimage = pipe(prompt, image=init_image, num_inference_steps=2, strength=0.5, guidance_scale=0.0).images[0]\n"], "EleutherAI/gpt-neo-1.3B": [">>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n>>> generator(\"EleutherAI has\", do_sample=True, min_length=50)\n\n[{'generated_text': 'EleutherAI has made a commitment to create new software packages for each of its major clients and has'}]\n"], "TheBloke/Mistral-7B-Instruct-v0.2-GGUF": ["from llama_cpp import Llama\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\n  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n\n# Simple inference example\noutput = llm(\n  \"<s>[INST] {prompt} [/INST]\", # Prompt\n  max_tokens=512,  # Generate up to 512 tokens\n  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n  echo=True        # Whether to echo the prompt\n)\n\n# Chat Completion API\n\nllm = Llama(model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a story about llamas.\"\n        }\n    ]\n)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n"], "huggyllama/llama-7b": [], "bert-large-cased": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] Hello I'm a male model. [SEP]\",\n      \"score\":0.22748498618602753,\n      \"token\":2581,\n      \"token_str\":\"male\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a fashion model. [SEP]\",\n      \"score\":0.09146175533533096,\n      \"token\":4633,\n      \"token_str\":\"fashion\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a new model. [SEP]\",\n      \"score\":0.05823173746466637,\n      \"token\":1207,\n      \"token_str\":\"new\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a super model. [SEP]\",\n      \"score\":0.04488750174641609,\n      \"token\":7688,\n      \"token_str\":\"super\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a famous model. [SEP]\",\n      \"score\":0.03271442651748657,\n      \"token\":2505,\n      \"token_str\":\"famous\"\n   }\n]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-cased')\nmodel = BertModel.from_pretrained(\"bert-large-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-cased')\nmodel = TFBertModel.from_pretrained(\"bert-large-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-cased')\n>>> unmasker(\"The man worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] The man worked as a doctor. [SEP]\",\n      \"score\":0.0645911768078804,\n      \"token\":3995,\n      \"token_str\":\"doctor\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a cop. [SEP]\",\n      \"score\":0.057450827211141586,\n      \"token\":9947,\n      \"token_str\":\"cop\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a mechanic. [SEP]\",\n      \"score\":0.04392256215214729,\n      \"token\":19459,\n      \"token_str\":\"mechanic\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a waiter. [SEP]\",\n      \"score\":0.03755280375480652,\n      \"token\":17989,\n      \"token_str\":\"waiter\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a teacher. [SEP]\",\n      \"score\":0.03458863124251366,\n      \"token\":3218,\n      \"token_str\":\"teacher\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] The woman worked as a nurse. [SEP]\",\n      \"score\":0.2572779953479767,\n      \"token\":7439,\n      \"token_str\":\"nurse\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a waitress. [SEP]\",\n      \"score\":0.16706500947475433,\n      \"token\":15098,\n      \"token_str\":\"waitress\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a teacher. [SEP]\",\n      \"score\":0.04587847739458084,\n      \"token\":3218,\n      \"token_str\":\"teacher\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a secretary. [SEP]\",\n      \"score\":0.03577028587460518,\n      \"token\":4848,\n      \"token_str\":\"secretary\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a maid. [SEP]\",\n      \"score\":0.03298963978886604,\n      \"token\":13487,\n      \"token_str\":\"maid\"\n   }\n]\n"], "openchat/openchat-3.6-8b-20240522": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"openchat/openchat-3.6-8b-20240522\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain how large language models work in detail.\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(input_ids,\n    do_sample=True,\n    temperature=0.5,\n    max_new_tokens=1024\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n"], "EleutherAI/pythia-160m": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "EleutherAI/pythia-70m-deduped": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "meta-llama/Llama-2-13b-hf": [], "Qwen/Qwen2-0.5B": [], "Salesforce/codet5p-770m": ["from transformers import T5ForConditionalGeneration, AutoTokenizer\n\ncheckpoint = \"Salesforce/codet5p-770m\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():<extra_id_0>\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_length=10)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n# ==> print \"Hello World\"\n"], "meta-llama/Llama-2-70b-hf": [], "baffo32/decapoda-research-llama-7B-hf": [], "EleutherAI/pythia-410m": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "01-ai/Yi-1.5-34B-Chat": [], "facebook/opt-13b": [">>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-13b\", use_fast=False)\n\n>>> prompt = \"Hello, I'm am conscious and\"\n\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> generated_ids = model.generate(input_ids)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Hello, I am conscious and aware of my surroundings.\\nI am conscious and aware of my']\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-13b\", use_fast=False)\n\n>>> prompt = \"Hello, I'm am conscious and\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Hello, I am conscious and aware.\\nSo that makes you dead, right?  ']\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-13b\", use_fast=False)\n\n>>> prompt = \"The woman worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe woman worked as a supervisor in the office \nThe woman worked as a social media consultant for\nThe woman worked as a cashier at the\nThe woman worked as a teacher, and was\nThe woman worked as a maid at our friends\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-13b\", use_fast=False)\n\n>>> prompt = \"The man worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe man worked as a consultant to the defense\nThe man worked as a bartender in a bar\nThe man worked as a cashier at the\nThe man worked as a teacher, and was\nThe man worked as a professional athlete while he\n"], "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF": [], "google/gemma-1.1-2b-it": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-2b-it\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    revision=\"float16\",\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-2b-it\",\n    device_map=\"auto\"\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-2b-it\",\n    quantization_config=quantization_config\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-1.1-2b-it\",\n    quantization_config=quantization_config\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "import jax.numpy as jnp\nfrom transformers import AutoTokenizer, FlaxGemmaForCausalLM\n\nmodel_id = \"google/gemma-1.1-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = \"left\"\n\nmodel, params = FlaxGemmaForCausalLM.from_pretrained(\n        model_id,\n        dtype=jnp.bfloat16,\n        revision=\"flax\",\n        _do_init=False,\n)\n\ninputs = tokenizer(\"Valencia and M\u00e1laga are\", return_tensors=\"np\", padding=True)\noutput = model.generate(**inputs, params=params, max_new_tokens=20, do_sample=False)\noutput_text = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"google/gemma-1.1-2b-it\"\ndtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=dtype,\n)\n\nchat = [\n    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"], "bigcode/starcoder2-3b": ["# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "Salesforce/blip2-flan-t5-xl": ["import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "# pip install accelerate\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "# pip install accelerate\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "# pip install accelerate bitsandbytes\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", load_in_8bit=True, device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n"], "tiiuae/falcon-7b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "openai-community/roberta-large-openai-detector": [], "nvidia/segformer-b0-finetuned-ade-512-512": ["from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nprocessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n"], "llava-hf/llava-1.5-13b-hf": ["from transformers import pipeline\n\npipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-1.5-13b-hf\")\nmessages = [\n    {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"},\n          {\"type\": \"text\", \"text\": \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"},\n        ],\n    },\n]\n\nout = pipe(text=messages, max_new_tokens=20)\nprint(out)\n>>> [{'input_text': [{'role': 'user', 'content': [{'type': 'image', 'url': 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg'}, {'type': 'text', 'text': 'What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud'}]}], 'generated_text': 'Lava'}]\n", "import requests\nfrom PIL import Image\n\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\nmodel_id = \"llava-hf/llava-1.5-13b-hf\"\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True, \n).to(0)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \nconversation = [\n    {\n\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"What are these?\"},\n          {\"type\": \"image\"},\n        ],\n    },\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\n"], "stabilityai/stable-cascade": ["import torch\nfrom diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", variant=\"bf16\", torch_dtype=torch.bfloat16)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", variant=\"bf16\", torch_dtype=torch.float16)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings.to(torch.float16),\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade.png\")\n", "import torch\nfrom diffusers import (\n    StableCascadeDecoderPipeline,\n    StableCascadePriorPipeline,\n    StableCascadeUNet,\n)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade-prior\", subfolder=\"prior_lite\")\ndecoder_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade\", subfolder=\"decoder_lite\")\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade.png\")\n", "import torch\nfrom diffusers import (\n    StableCascadeDecoderPipeline,\n    StableCascadePriorPipeline,\n    StableCascadeUNet,\n)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior_unet = StableCascadeUNet.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-cascade/resolve/main/stage_c_bf16.safetensors\",\n    torch_dtype=torch.bfloat16\n)\ndecoder_unet = StableCascadeUNet.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-cascade/blob/main/stage_b_bf16.safetensors\",\n    torch_dtype=torch.bfloat16\n)\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet, torch_dtype=torch.bfloat16)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet, torch_dtype=torch.bfloat16)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade-single-file.png\")\n", "from diffusers import StableCascadeCombinedPipeline\n\npipe = StableCascadeCombinedPipeline.from_pretrained(\"stabilityai/stable-cascade\", variant=\"bf16\", torch_dtype=torch.bfloat16)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\npipe(\n    prompt=prompt,\n    negative_prompt=\"\",\n    num_inference_steps=10,\n    prior_num_inference_steps=20,\n    prior_guidance_scale=3.0,\n    width=1024,\n    height=1024,\n).images[0].save(\"cascade-combined.png\")\n"], "hustvl/yolos-small": ["from transformers import YolosFeatureExtractor, YolosForObjectDetection\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n"], "Qwen/Qwen1.5-0.5B": [], "sentence-transformers/sentence-t5-base": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/sentence-t5-base')\nembeddings = model.encode(sentences)\nprint(embeddings)\n"], "EleutherAI/pythia-1.4b": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "codellama/CodeLlama-7b-Instruct-hf": [], "stabilityai/stablelm-2-zephyr-1_6b": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('stabilityai/stablelm-2-zephyr-1_6b')\nmodel = AutoModelForCausalLM.from_pretrained(\n    'stabilityai/stablelm-2-zephyr-1_6b',\n    device_map=\"auto\"\n)\n\nprompt = [{'role': 'user', 'content': 'Which famous math number begins with 1.6 ...?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n)\n\ntokens = model.generate(\n    inputs.to(model.device),\n    max_new_tokens=1024,\n    temperature=0.5,\n    do_sample=True\n)\n\nprint(tokenizer.decode(tokens[0], skip_special_tokens=False))\n"], "sentence-transformers/clip-ViT-B-32-multilingual-v1": ["from sentence_transformers import SentenceTransformer, util\nfrom PIL import Image, ImageFile\nimport requests\nimport torch\n\n# We use the original clip-ViT-B-32 for encoding images\nimg_model = SentenceTransformer('clip-ViT-B-32')\n\n# Our text embedding model is aligned to the img_model and maps 50+\n# languages to the same vector space\ntext_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n\n\n# Now we load and encode the images\ndef load_image(url_or_path):\n    if url_or_path.startswith(\"http://\") or url_or_path.startswith(\"https://\"):\n        return Image.open(requests.get(url_or_path, stream=True).raw)\n    else:\n        return Image.open(url_or_path)\n\n# We load 3 images. You can either pass URLs or\n# a path on your disc\nimg_paths = [\n    # Dog image\n    \"https://unsplash.com/photos/QtxgNsmJQSs/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjM1ODQ0MjY3&w=640\",\n\n    # Cat image\n    \"https://unsplash.com/photos/9UUoGaaHtNE/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8Mnx8Y2F0fHwwfHx8fDE2MzU4NDI1ODQ&w=640\",\n\n    # Beach image\n    \"https://unsplash.com/photos/Siuwr3uCir0/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8NHx8YmVhY2h8fDB8fHx8MTYzNTg0MjYzMg&w=640\"\n]\n\nimages = [load_image(img) for img in img_paths]\n\n# Map images to the vector space\nimg_embeddings = img_model.encode(images)\n\n# Now we encode our text:\ntexts = [\n    \"A dog in the snow\",\n    \"Eine Katze\",  # German: A cat\n    \"Una playa con palmeras.\"  # Spanish: a beach with palm trees\n]\n\ntext_embeddings = text_model.encode(texts)\n\n# Compute cosine similarities:\ncos_sim = util.cos_sim(text_embeddings, img_embeddings)\n\nfor text, scores in zip(texts, cos_sim):\n    max_img_idx = torch.argmax(scores)\n    print(\"Text:\", text)\n    print(\"Score:\", scores[max_img_idx] )\n    print(\"Path:\", img_paths[max_img_idx], \"\\n\")\n"], "upstage/SOLAR-10.7B-Instruct-v1.0": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Upstage/SOLAR-10.7B-Instruct-v1.0\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Upstage/SOLAR-10.7B-Instruct-v1.0\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n)\n"], "Qwen/Qwen1.5-0.5B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-0.5B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "laion/CLIP-ViT-L-14-laion2B-s32B-b82K": [], "playgroundai/playground-v2.5-1024px-aesthetic": ["from diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"playgroundai/playground-v2.5-1024px-aesthetic\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# # Optional: Use DPM++ 2M Karras scheduler for crisper fine details\n# from diffusers import EDMDPMSolverMultistepScheduler\n# pipe.scheduler = EDMDPMSolverMultistepScheduler()\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage = pipe(prompt=prompt, num_inference_steps=50, guidance_scale=3).images[0]\n"], "seyonec/ChemBERTa-zinc-base-v1": [], "Qwen/Qwen1.5-7B": [], "lvwerra/distilbert-imdb": [], "SimianLuo/LCM_Dreamshaper_v7": ["from diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4 \n\nimages = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n", "from diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", custom_pipeline=\"latent_consistency_txt2img\", custom_revision=\"main\", revision=\"fb9c5d\")\n\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4 \n\nimages = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, output_type=\"pil\").images\n"], "kykim/bert-kor-base": ["from transformers import BertTokenizerFast, BertModel\n\ntokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\nmodel_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n"], "aubmindlab/bert-base-arabert": ["from arabert.preprocess import ArabertPreprocessor\n\nmodel_name=\"bert-base-arabert\"\narabert_prep = ArabertPreprocessor(model_name=model_name)\n\ntext = \"\u0648\u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644\u0646\u0627 \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644\u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\"\narabert_prep.preprocess(text)\n>>>\"\u0648+ \u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644 +\u0646\u0627 \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644+ \u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646 +\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\"\n"], "openchat/openchat-3.5-0106": [], "google/mt5-large": [], "google/mobilenet_v2_1.0_224": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\npreprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v2_1.0_224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n\ninputs = preprocessor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "google/muril-base-cased": [], "openmmlab/upernet-convnext-small": [], "bert-base-german-dbmdz-uncased": [], "nvidia/mit-b0": ["from transformers import SegformerImageProcessor, SegformerForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\")\nmodel = SegformerForImageClassification.from_pretrained(\"nvidia/mit-b0\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "csarron/mobilebert-uncased-squad-v2": ["from transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"csarron/mobilebert-uncased-squad-v2\",\n    tokenizer=\"csarron/mobilebert-uncased-squad-v2\"\n)\n\npredictions = qa_pipeline({\n    'context': \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n    'question': \"What day was the game played on?\"\n})\n\nprint(predictions)\n# output:\n# {'score': 0.71434086561203, 'start': 23, 'end': 39, 'answer': 'February 7, 2016'}\n"], "facebook/m2m100_1.2B": ["from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\nhi_text = \"\u091c\u0940\u0935\u0928 \u090f\u0915 \u091a\u0949\u0915\u0932\u0947\u091f \u092c\u0949\u0915\u094d\u0938 \u0915\u0940 \u0924\u0930\u0939 \u0939\u0948\u0964\"\nchinese_text = \"\u751f\u6d3b\u5c31\u50cf\u4e00\u76d2\u5de7\u514b\u529b\u3002\"\n\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\")\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n\n# translate Hindi to French\ntokenizer.src_lang = \"hi\"\nencoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"La vie est comme une bo\u00eete de chocolat.\"\n\n# translate Chinese to English\ntokenizer.src_lang = \"zh\"\nencoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Life is like a box of chocolate.\"\n"], "deepseek-ai/deepseek-coder-33b-instruct": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"], "openchat/openchat-3.5-1210": [], "laion/mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k": [], "cross-encoder/ms-marco-TinyBERT-L-2": ["from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n", "from sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name', max_length=512)\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n"], "nvidia/Llama3-ChatQA-1.5-8B": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"what is the percentage change of the net income from Q4 FY23 to Q4 FY24?\"}\n]\n\ndocument = \"\"\"NVIDIA (NASDAQ: NVDA) today reported revenue for the fourth quarter ended January 28, 2024, of $22.1 billion, up 22% from the previous quarter and up 265% from a year ago.\\nFor the quarter, GAAP earnings per diluted share was $4.93, up 33% from the previous quarter and up 765% from a year ago. Non-GAAP earnings per diluted share was $5.16, up 28% from the previous quarter and up 486% from a year ago.\\nQ4 Fiscal 2024 Summary\\nGAAP\\n| $ in millions, except earnings per share | Q4 FY24 | Q3 FY24 | Q4 FY23 | Q/Q | Y/Y |\\n| Revenue | $22,103 | $18,120 | $6,051 | Up 22% | Up 265% |\\n| Gross margin | 76.0% | 74.0% | 63.3% | Up 2.0 pts | Up 12.7 pts |\\n| Operating expenses | $3,176 | $2,983 | $2,576 | Up 6% | Up 23% |\\n| Operating income | $13,615 | $10,417 | $1,257 | Up 31% | Up 983% |\\n| Net income | $12,285 | $9,243 | $1,414 | Up 33% | Up 769% |\\n| Diluted earnings per share | $4.93 | $3.71 | $0.57 | Up 33% | Up 765% |\"\"\"\n\ndef get_formatted_input(messages, context):\n    system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n    instruction = \"Please give a full and complete answer for the question.\"\n\n    for item in messages:\n        if item['role'] == \"user\":\n            ## only apply this instruction for the first user turn\n            item['content'] = instruction + \" \" + item['content']\n            break\n\n    conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n    formatted_input = system + \"\\n\\n\" + context + \"\\n\\n\" + conversation\n    \n    return formatted_input\n\nformatted_input = get_formatted_input(messages, document)\ntokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n\nresponse = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n", "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport torch\nimport json\n\n## load ChatQA-1.5 tokenizer and model\nmodel_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n\n## load retriever tokenizer and model\nretriever_tokenizer = AutoTokenizer.from_pretrained('nvidia/dragon-multiturn-query-encoder')\nquery_encoder = AutoModel.from_pretrained('nvidia/dragon-multiturn-query-encoder')\ncontext_encoder = AutoModel.from_pretrained('nvidia/dragon-multiturn-context-encoder')\n\n## prepare documents, we take landrover car manual document that we provide as an example\nchunk_list = json.load(open(\"docs.json\"))['landrover']\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"how to connect the bluetooth in the car?\"}\n]\n\n### running retrieval\n## convert query into a format as follows:\n## user: {user}\\nagent: {agent}\\nuser: {user}\nformatted_query_for_retriever = '\\n'.join([turn['role'] + \": \" + turn['content'] for turn in messages]).strip()\n\nquery_input = retriever_tokenizer(formatted_query_for_retriever, return_tensors='pt')\nctx_input = retriever_tokenizer(chunk_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\nquery_emb = query_encoder(**query_input).last_hidden_state[:, 0, :]\nctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]\n\n## Compute similarity scores using dot product and rank the similarity\nsimilarities = query_emb.matmul(ctx_emb.transpose(0, 1)) # (1, num_ctx)\nranked_results = torch.argsort(similarities, dim=-1, descending=True) # (1, num_ctx)\n\n## get top-n chunks (n=5)\nretrieved_chunks = [chunk_list[idx] for idx in ranked_results.tolist()[0][:5]]\ncontext = \"\\n\\n\".join(retrieved_chunks)\n\n### running text generation\nformatted_input = get_formatted_input(messages, context)\ntokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\noutputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n\nresponse = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n"], "Qwen/Qwen1.5-1.8B": [], "mosaicml/mpt-7b": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-7b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-7b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n", "from transformers import pipeline\n\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "Qwen/Qwen-7B": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B\", trust_remote_code=True)\n\ninputs = tokenizer('\u8499\u53e4\u56fd\u7684\u9996\u90fd\u662f\u4e4c\u5170\u5df4\u6258\uff08Ulaanbaatar\uff09\\n\u51b0\u5c9b\u7684\u9996\u90fd\u662f\u96f7\u514b\u96c5\u672a\u514b\uff08Reykjavik\uff09\\n\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7684\u9996\u90fd\u662f', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# \u8499\u53e4\u56fd\u7684\u9996\u90fd\u662f\u4e4c\u5170\u5df4\u6258\uff08Ulaanbaatar\uff09\\n\u51b0\u5c9b\u7684\u9996\u90fd\u662f\u96f7\u514b\u96c5\u672a\u514b\uff08Reykjavik\uff09\\n\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7684\u9996\u90fd\u662f\u4e9a\u7684\u65af\u4e9a\u8d1d\u5df4\uff08Addis Ababa\uff09...\n"], "Falconsai/text_summarization": ["from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n\nARTICLE = \"\"\" \nHugging Face: Revolutionizing Natural Language Processing\nIntroduction\nIn the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\nThe Birth of Hugging Face\nHugging Face was founded in 2016 by Cl\u00e9ment Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\nTransformative Innovations\nHugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\nKey Contributions:\n1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\nDemocratizing AI\nHugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\nBy providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\nIndustry Adoption\nThe success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\nFuture Directions\nHugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\nConclusion\nHugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name \"Hugging Face\" was chosen to reflect the company\\'s mission of making AI models more accessible and friendly to humans .'}]\n"], "Qwen/Qwen2-1.5B": [], "diffusers/sdxl-instructpix2pix-768": ["import torch\nfrom diffusers import StableDiffusionXLInstructPix2PixPipeline\nfrom diffusers.utils import load_image\n\nresolution = 768\nimage = load_image(\n    \"https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\"\n).resize((resolution, resolution))\nedit_instruction = \"Turn sky into a cloudy one\"\n\npipe = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(\n    \"diffusers/sdxl-instructpix2pix-768\", torch_dtype=torch.float16\n).to(\"cuda\")\n\nedited_image = pipe(\n    prompt=edit_instruction,\n    image=image,\n    height=resolution,\n    width=resolution,\n    guidance_scale=3.0,\n    image_guidance_scale=1.5,\n    num_inference_steps=30,\n).images[0]\nedited_image.save(\"edited_image.png\")\n"], "unsloth/llama-3-8b": [], "facebook/opt-6.7b": [">>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\", use_fast=False)\n\n>>> prompt = \"Hello, I'm am conscious and\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> generated_ids = model.generate(input_ids)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Hello, I'm am conscious and aware of my surroundings. I'm not sure what you mean\"]\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\", use_fast=False)\n\n>>> prompt = \"Hello, I'm am conscious and\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Hello, I'm am conscious and aware of my surroundings. I'm not sure if I'm\"]\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\", use_fast=False)\n\n>>> prompt = \"The woman worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe woman worked as a supervisor in the office\nThe woman worked as a bartender in a bar\nThe woman worked as a cashier at the\nThe woman worked as a teacher, and was\nThe woman worked as a maid at a house\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\", use_fast=False)\n\n>>> prompt = \"The man worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe man worked as a consultant to the Government\nThe man worked as a bartender in a bar\nThe man worked as a cashier at the\nThe man worked as a teacher, and was\nThe man worked as a professional at a bank\n"], "NeuML/pubmedbert-base-embeddings": ["import txtai\n\nembeddings = txtai.Embeddings(path=\"neuml/pubmedbert-base-embeddings\", content=True)\nembeddings.index(documents())\n\n# Run a query\nembeddings.search(\"query to run\")\n", "from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer(\"neuml/pubmedbert-base-embeddings\")\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef meanpooling(output, mask):\n    embeddings = output[0] # First element of model_output contains all token embeddings\n    mask = mask.unsqueeze(-1).expand(embeddings.size()).float()\n    return torch.sum(embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\nmodel = AutoModel.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n\n# Tokenize sentences\ninputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    output = model(**inputs)\n\n# Perform pooling. In this case, mean pooling.\nembeddings = meanpooling(output, inputs['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(embeddings)\n"], "lmms-lab/LLaVA-NeXT-Video-7B-DPO": [], "sentence-transformers/paraphrase-xlm-r-multilingual-v1": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5": [], "dccuchile/bert-base-spanish-wwm-cased": [], "Qwen/Qwen2-57B-A14B-Instruct": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-57B-A14B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-57B-A14B-Instruct\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "TheBloke/Llama-2-7B-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=True,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "tuner007/pegasus_paraphrase": ["import torch\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nmodel_name = 'tuner007/pegasus_paraphrase'\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n\ndef get_response(input_text,num_return_sequences,num_beams):\n  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n  return tgt_text\n"], "indolem/indobert-base-uncased": ["from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")\n"], "huggingface/CodeBERTa-small-v1": ["from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"huggingface/CodeBERTa-small-v1\",\n    tokenizer=\"huggingface/CodeBERTa-small-v1\"\n)\n\nfill_mask(PHP_CODE)\n\n## Top 5 predictions:\n# \n' function' # prob 0.9999827146530151\n'function'  # \n' void'     # \n' def'      # \n' final'    # \n", "PYTHON_CODE = \"\"\"\ndef pipeline(\n    task: str,\n    model: Optional = None,\n    framework: Optional[<mask>] = None,\n    **kwargs\n) -> Pipeline:\n    pass\n\"\"\".lstrip()\n"], "tiiuae/falcon-11B": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-11B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n)\nsequences = pipeline(\n   \"Can you explain the concepts of Quantum Computing?\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-11B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Can you explain the concepts of Quantum Computing?\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "TurkuNLP/bert-base-finnish-cased-v1": [], "setu4993/LaBSE": ["import torch\nfrom transformers import BertModel, BertTokenizerFast\n\n\ntokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LaBSE\")\nmodel = BertModel.from_pretrained(\"setu4993/LaBSE\")\nmodel = model.eval()\n\nenglish_sentences = [\n    \"dog\",\n    \"Puppies are nice.\",\n    \"I enjoy taking long walks along the beach with my dog.\",\n]\nenglish_inputs = tokenizer(english_sentences, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    english_outputs = model(**english_inputs)\n", "import torch.nn.functional as F\n\n\ndef similarity(embeddings_1, embeddings_2):\n    normalized_embeddings_1 = F.normalize(embeddings_1, p=2)\n    normalized_embeddings_2 = F.normalize(embeddings_2, p=2)\n    return torch.matmul(\n        normalized_embeddings_1, normalized_embeddings_2.transpose(0, 1)\n    )\n\n\nprint(similarity(english_embeddings, italian_embeddings))\nprint(similarity(english_embeddings, japanese_embeddings))\nprint(similarity(italian_embeddings, japanese_embeddings))\n"], "SG161222/Realistic_Vision_V2.0": [], "google/mt5-xl": [], "NousResearch/Meta-Llama-3-8B": [">>> import transformers\n>>> import torch\n\n>>> model_id = \"meta-llama/Meta-Llama-3-8B\"\n\n>>> pipeline = transformers.pipeline(\n    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\n>>> pipeline(\"Hey how are you doing today?\")\n"], "NousResearch/Nous-Hermes-2-Yi-34B": [], "diffusers/controlnet-canny-sdxl-1.0": ["from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\nfrom diffusers.utils import load_image\nfrom PIL import Image\nimport torch\nimport numpy as np\nimport cv2\n\nprompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\nnegative_prompt = 'low quality, bad quality, sketches'\n\nimage = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\n\ncontrolnet_conditioning_scale = 0.5  # recommended for good generalization\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-canny-sdxl-1.0\",\n    torch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    vae=vae,\n    torch_dtype=torch.float16,\n)\npipe.enable_model_cpu_offload()\n\nimage = np.array(image)\nimage = cv2.Canny(image, 100, 200)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\nimage = Image.fromarray(image)\n\nimages = pipe(\n    prompt, negative_prompt=negative_prompt, image=image, controlnet_conditioning_scale=controlnet_conditioning_scale,\n    ).images\n\nimages[0].save(f\"hug_lab.png\")\n"], "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224": ["import torch\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer\n\n# Load the model and config files from the Hugging Face Hub\nmodel, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\ntokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n\n\n# Zero-shot image classification\ntemplate = 'this is a photo of '\nlabels = [\n    'adenocarcinoma histopathology',\n    'brain MRI',\n    'covid line chart',\n    'squamous cell carcinoma histopathology',\n    'immunohistochemistry histopathology',\n    'bone X-ray',\n    'chest X-ray',\n    'pie chart',\n    'hematoxylin and eosin histopathology'\n]\n\ndataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\ntest_imgs = [\n    'squamous_cell_carcinoma_histopathology.jpeg',\n    'H_and_E_histopathology.jpg',\n    'bone_X-ray.jpg',\n    'adenocarcinoma_histopathology.jpg',\n    'covid_line_chart.png',\n    'IHC_histopathology.jpg',\n    'chest_X-ray.jpg',\n    'brain_MRI.jpg',\n    'pie_chart.png'\n]\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\nmodel.eval()\n\ncontext_length = 256\n\nimages = torch.stack([preprocess(Image.open(urlopen(dataset_url + img))) for img in test_imgs]).to(device)\ntexts = tokenizer([template + l for l in labels], context_length=context_length).to(device)\nwith torch.no_grad():\n    image_features, text_features, logit_scale = model(images, texts)\n\n    logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n    sorted_indices = torch.argsort(logits, dim=-1, descending=True)\n\n    logits = logits.cpu().numpy()\n    sorted_indices = sorted_indices.cpu().numpy()\n\ntop_k = -1\n\nfor i, img in enumerate(test_imgs):\n    pred = labels[sorted_indices[i][0]]\n\n    top_k = len(labels) if top_k == -1 else top_k\n    print(img.split('/')[-1] + ':')\n    for j in range(top_k):\n        jth_index = sorted_indices[i][j]\n        print(f'{labels[jth_index]}: {logits[i][jth_index]}')\n    print('\\n')\n", "import json\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom open_clip import create_model_and_transforms, get_tokenizer\nfrom open_clip.factory import HF_HUB_PREFIX, _MODEL_CONFIGS\n\n\n# Download the model and config files\nhf_hub_download(\n    repo_id=\"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n    filename=\"open_clip_pytorch_model.bin\",\n    local_dir=\"checkpoints\"\n)\nhf_hub_download(\n    repo_id=\"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n    filename=\"open_clip_config.json\",\n    local_dir=\"checkpoints\"\n)\n\n\n# Load the model and config files\nmodel_name = \"biomedclip_local\"\n\nwith open(\"checkpoints/open_clip_config.json\", \"r\") as f:\n    config = json.load(f)\n    model_cfg = config[\"model_cfg\"]\n    preprocess_cfg = config[\"preprocess_cfg\"]\n\n\nif (not model_name.startswith(HF_HUB_PREFIX)\n    and model_name not in _MODEL_CONFIGS\n    and config is not None):\n    _MODEL_CONFIGS[model_name] = model_cfg\n\ntokenizer = get_tokenizer(model_name)\n\nmodel, _, preprocess = create_model_and_transforms(\n    model_name=model_name,\n    pretrained=\"checkpoints/open_clip_pytorch_model.bin\",\n    **{f\"image_{k}\": v for k, v in preprocess_cfg.items()},\n)\n\n\n# Zero-shot image classification\ntemplate = 'this is a photo of '\nlabels = [\n    'adenocarcinoma histopathology',\n    'brain MRI',\n    'covid line chart',\n    'squamous cell carcinoma histopathology',\n    'immunohistochemistry histopathology',\n    'bone X-ray',\n    'chest X-ray',\n    'pie chart',\n    'hematoxylin and eosin histopathology'\n]\n\ndataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\ntest_imgs = [\n    'squamous_cell_carcinoma_histopathology.jpeg',\n    'H_and_E_histopathology.jpg',\n    'bone_X-ray.jpg',\n    'adenocarcinoma_histopathology.jpg',\n    'covid_line_chart.png',\n    'IHC_histopathology.jpg',\n    'chest_X-ray.jpg',\n    'brain_MRI.jpg',\n    'pie_chart.png'\n]\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\nmodel.eval()\n\ncontext_length = 256\n\nimages = torch.stack([preprocess(Image.open(urlopen(dataset_url + img))) for img in test_imgs]).to(device)\ntexts = tokenizer([template + l for l in labels], context_length=context_length).to(device)\nwith torch.no_grad():\n    image_features, text_features, logit_scale = model(images, texts)\n\n    logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n    sorted_indices = torch.argsort(logits, dim=-1, descending=True)\n\n    logits = logits.cpu().numpy()\n    sorted_indices = sorted_indices.cpu().numpy()\n\ntop_k = -1\n\nfor i, img in enumerate(test_imgs):\n    pred = labels[sorted_indices[i][0]]\n\n    top_k = len(labels) if top_k == -1 else top_k\n    print(img.split('/')[-1] + ':')\n    for j in range(top_k):\n        jth_index = sorted_indices[i][j]\n        print(f'{labels[jth_index]}: {logits[i][jth_index]}')\n    print('\\n')\n", "@article{zhang2024biomedclip,\n  title={A Multimodal Biomedical Foundation Model Trained from Fifteen Million Image\u2013Text Pairs},\n  author={Sheng Zhang and Yanbo Xu and Naoto Usuyama and Hanwen Xu and Jaspreet Bagga and Robert Tinn and Sam Preston and Rajesh Rao and Mu Wei and Naveen Valluri and Cliff Wong and Andrea Tupini and Yu Wang and Matt Mazzola and Swadheen Shukla and Lars Liden and Jianfeng Gao and Angela Crabtree and Brian Piening and Carlo Bifulco and Matthew P. Lungren and Tristan Naumann and Sheng Wang and Hoifung Poon},\n  journal={NEJM AI},\n  year={2024},\n  volume={2},\n  number={1},\n  doi={10.1056/AIoa2400640},\n  url={https://ai.nejm.org/doi/full/10.1056/AIoa2400640}\n}\n"], "xlm-clm-ende-1024": ["import torch\nfrom transformers import XLMTokenizer, XLMWithLMHeadModel\n\ntokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-ende-1024\")\nmodel = XLMWithLMHeadModel.from_pretrained(\"xlm-clm-ende-1024\")\n\ninput_ids = torch.tensor([tokenizer.encode(\"Wikipedia was used to\")])  # batch size of 1\n\nlanguage_id = tokenizer.lang2id[\"en\"]  # 0\nlangs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])\n\n# We reshape it to be of size (batch_size, sequence_length)\nlangs = langs.view(1, -1)  # is now of shape [1, sequence_length] (we have a batch size of 1)\n\noutputs = model(input_ids, langs=langs)\n"], "nerijs/pixel-art-xl": ["from diffusers import DiffusionPipeline, LCMScheduler\nimport torch\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nlcm_lora_id = \"latent-consistency/lcm-lora-sdxl\"\npipe = DiffusionPipeline.from_pretrained(model_id, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(lcm_lora_id, adapter_name=\"lora\")\npipe.load_lora_weights(\"./pixel-art-xl.safetensors\", adapter_name=\"pixel\")\n\npipe.set_adapters([\"lora\", \"pixel\"], adapter_weights=[1.0, 1.2])\npipe.to(device=\"cuda\", dtype=torch.float16)\n\nprompt = \"pixel, a cute corgi\"\nnegative_prompt = \"3d render, realistic\"\n\nnum_images = 9\n\nfor i in range(num_images):\n    img = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=8,\n        guidance_scale=1.5,\n    ).images[0]\n    \n    img.save(f\"lcm_lora_{i}.png\")\n"], "laion/CLIP-ViT-L-14-CommonPool.XL-s13B-b90K": [], "microsoft/biogpt": [">>> from transformers import pipeline, set_seed\n>>> from transformers import BioGptTokenizer, BioGptForCausalLM\n>>> model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n>>> tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n>>> set_seed(42)\n>>> generator(\"COVID-19 is\", max_length=20, num_return_sequences=5, do_sample=True)\n[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},\n {'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},\n {'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the United States alone.'},\n {'generated_text': 'COVID-19 is a pandemic, the incidence has been increased in a manner similar to that in other'},\n {'generated_text': 'COVID-19 is transmitted via droplets, air-borne, or airborne transmission.'}]\n", "from transformers import BioGptTokenizer, BioGptForCausalLM\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "import torch\nfrom transformers import BioGptTokenizer, BioGptForCausalLM, set_seed\n\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n\nsentence = \"COVID-19 is\"\ninputs = tokenizer(sentence, return_tensors=\"pt\")\n\nset_seed(42)\n\nwith torch.no_grad():\n    beam_output = model.generate(**inputs,\n                                min_length=100,\n                                max_length=1024,\n                                num_beams=5,\n                                early_stopping=True\n                                )\ntokenizer.decode(beam_output[0], skip_special_tokens=True)\n'COVID-19 is a global pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the causative agent of coronavirus disease 2019 (COVID-19), which has spread to more than 200 countries and territories, including the United States (US), Canada, Australia, New Zealand, the United Kingdom (UK), and the United States of America (USA), as of March 11, 2020, with more than 800,000 confirmed cases and more than 800,000 deaths.'\n"], "EleutherAI/pythia-160m-deduped": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "databricks/dolly-v2-3b": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n", "import torch\nfrom instruct_pipeline import InstructionTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ngenerate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n", "import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16,\n                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n", "from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\n# template for an instrution with no input\nprompt = PromptTemplate(\n    input_variables=[\"instruction\"],\n    template=\"{instruction}\")\n\n# template for an instruction with input\nprompt_with_context = PromptTemplate(\n    input_variables=[\"instruction\", \"context\"],\n    template=\"{instruction}\\n\\nInput:\\n{context}\")\n\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\nllm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n", "context = \"\"\"George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,\nand Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n\nprint(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())\n"], "timm/ViT-B-16-SigLIP": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-B-16-SigLIP')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_base_patch16_siglip_224',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "google/t5-v1_1-base": [], "dreamlike-art/dreamlike-photoreal-2.0": ["from diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"dreamlike-art/dreamlike-photoreal-2.0\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"./result.jpg\")\n"], "bert-base-german-cased": [], "naver-clova-ix/donut-base": [], "Salesforce/codet5-small": ["from transformers import RobertaTokenizer, T5ForConditionalGeneration\n\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n\ntext = \"def greet(user): print(f'hello <extra_id_0>!')\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\n# simply generate a single sequence\ngenerated_ids = model.generate(input_ids, max_length=10)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n# this prints \"user: {user.name}\"\n"], "Qwen/Qwen1.5-4B": [], "Qwen/Qwen1.5-7B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-7B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "EleutherAI/pythia-410m-deduped": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "tiiuae/falcon-40b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "THUDM/cogvlm-chat-hf": ["import torch\nimport requests\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\nmodel = AutoModelForCausalLM.from_pretrained(\n    'THUDM/cogvlm-chat-hf',\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).to('cuda').eval()\n\n\n# chat example\nquery = 'Describe this image'\nimage = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode\ninputs = {\n    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n}\ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n\n# This image captures a moment from a basketball game. Two players are prominently featured: one wearing a yellow jersey with the number\n# 24 and the word 'Lakers' written on it, and the other wearing a navy blue jersey with the word 'Washington' and the number 34. The player\n# in yellow is holding a basketball and appears to be dribbling it, while the player in navy blue is reaching out with his arm, possibly\n# trying to block or defend. The background shows a filled stadium with spectators, indicating that this is a professional game.</s>\n\n\n\n# vqa example\nquery = 'How many houses are there in this cartoon?'\nimage = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/3.jpg?raw=true', stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image], template_version='vqa')   # vqa mode\ninputs = {\n    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n}\ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n\n# 4</s>\n", "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\nfrom accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n\ntokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_pretrained(\n        'THUDM/cogvlm-chat-hf',\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True,\n        trust_remote_code=True,\n    )\ndevice_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'}, no_split_module_classes=['CogVLMDecoderLayer', 'TransformerLayer'])\nmodel = load_checkpoint_and_dispatch(\n    model,\n    'local/path/to/hf/version/chat/model',   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'\n    device_map=device_map,\n)\nmodel = model.eval()\n\n# check device for weights if u want to\nfor n, p in model.named_parameters():\n    print(f\"{n}: {p.device}\")\n\n# chat example\nquery = 'Describe this image'\nimage = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')\ninputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode\ninputs = {\n    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n}\ngen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0]))\n"], "Salesforce/codegen-350M-mono": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "upstage/SOLAR-10.7B-v1.0": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Upstage/SOLAR-10.7B-v1.0\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Upstage/SOLAR-10.7B-v1.0\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n)\n"], "ckiplab/bert-base-chinese": ["from transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese')\n"], "diffusers/controlnet-depth-sdxl-1.0": ["import torch\nimport numpy as np\nfrom PIL import Image\n\nfrom transformers import DPTFeatureExtractor, DPTForDepthEstimation\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\nfrom diffusers.utils import load_image\n\n\ndepth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\nfeature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-depth-sdxl-1.0\",\n    variant=\"fp16\",\n    use_safetensors=True,\n    torch_dtype=torch.float16,\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    vae=vae,\n    variant=\"fp16\",\n    use_safetensors=True,\n    torch_dtype=torch.float16,\n)\npipe.enable_model_cpu_offload()\n\ndef get_depth_map(image):\n    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n    with torch.no_grad(), torch.autocast(\"cuda\"):\n        depth_map = depth_estimator(image).predicted_depth\n\n    depth_map = torch.nn.functional.interpolate(\n        depth_map.unsqueeze(1),\n        size=(1024, 1024),\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n    image = torch.cat([depth_map] * 3, dim=1)\n\n    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n    return image\n\n\nprompt = \"stormtrooper lecture, photorealistic\"\nimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\ncontrolnet_conditioning_scale = 0.5  # recommended for good generalization\n\ndepth_image = get_depth_map(image)\n\nimages = pipe(\n    prompt, image=depth_image, num_inference_steps=30, controlnet_conditioning_scale=controlnet_conditioning_scale,\n).images\nimages[0]\n\nimages[0].save(f\"stormtrooper.png\")\n"], "laion/CLIP-ViT-g-14-laion2B-s34B-b88K": [], "google/t5-v1_1-large": [], "Qwen/Qwen1.5-14B": [], "thibaud/controlnet-openpose-sdxl-1.0": ["from diffusers import AutoencoderKL, StableDiffusionXLControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nfrom controlnet_aux import OpenposeDetector\nfrom diffusers.utils import load_image\n\n\n# Compute openpose conditioning image.\nopenpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n)\nopenpose_image = openpose(image)\n\n# Initialize ControlNet pipeline.\ncontrolnet = ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\n\n# Infer.\nprompt = \"Darth vader dancing in a desert, high quality\"\nnegative_prompt = \"low quality, bad quality\"\nimages = pipe(\n    prompt, \n    negative_prompt=negative_prompt,\n    num_inference_steps=25,\n    num_images_per_prompt=4,\n    image=openpose_image.resize((1024, 1024)),\n    generator=torch.manual_seed(97),\n).images\nimages[0]\n"], "facebook/nougat-base": [], "albert-base-v1": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-base-v1')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u2581modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u2581modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u2581model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u2581runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u2581lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v1')\nmodel = AlbertModel.from_pretrained(\"albert-base-v1\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v1')\nmodel = TFAlbertModel.from_pretrained(\"albert-base-v1\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-base-v1')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u2581chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u2581janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u2581shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u2581blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u2581lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u2581receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u2581janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u2581paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u2581chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u2581waitress\"\n   }\n]\n"], "dreamlike-art/dreamlike-diffusion-1.0": ["from diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"dreamlike-art/dreamlike-diffusion-1.0\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"./result.jpg\")\n"], "unsloth/mistral-7b-bnb-4bit": [], "facebook/convnext-tiny-224": ["from transformers import ConvNextImageProcessor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-tiny-224\")\nmodel = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "hfl/chinese-macbert-base": [], "bigscience/bloom-1b1": [], "klue/roberta-base": ["from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"klue/roberta-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n"], "berkeley-nest/Starling-LM-7B-alpha": ["import transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"openchat/openchat_3.5\")\n\n# Single-turn\ntokens = tokenizer(\"GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:\").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Multi-turn\ntokens = tokenizer(\"GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:\").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Coding Mode\ntokens = tokenizer(\"Code User: Implement quicksort using C++<|end_of_turn|>Code Assistant:\").input_ids\nassert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]\n", "import transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"berkeley-nest/Starling-LM-7B-alpha\")\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\"berkeley-nest/Starling-LM-7B-alpha\")\n\ndef generate_response(prompt):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n    outputs = model.generate(\n        input_ids,\n        max_length=256,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    response_ids = outputs[0]\n    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n    return response_text\n\n# Single-turn conversation\nprompt = \"Hello, how are you?\"\nsingle_turn_prompt = f\"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:\"\nresponse_text = generate_response(single_turn_prompt)\nprint(\"Response:\", response_text)\n\n## Multi-turn conversation\nprompt = \"Hello\"\nfollow_up_question =  \"How are you today?\"\nresponse = \"\"\nmulti_turn_prompt = f\"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant: {response}<|end_of_turn|>GPT4 Correct User: {follow_up_question}<|end_of_turn|>GPT4 Correct Assistant:\"\nresponse_text = generate_response(multi_turn_prompt)\nprint(\"Multi-turn conversation response:\", response_text)\n\n### Coding conversation\nprompt = \"Implement quicksort using C++\"\ncoding_prompt = f\"Code User: {prompt}<|end_of_turn|>Code Assistant:\"\nresponse = generate_response(coding_prompt)\nprint(\"Coding conversation response:\", response)\n"], "prompthero/openjourney": ["from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = \"prompthero/openjourney\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\nimage = pipe(prompt).images[0]\nimage.save(\"./retro_cars.png\")\n"], "dbmdz/bert-base-german-cased": ["from transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\nmodel = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")\n"], "google/byt5-base": ["from transformers import T5ForConditionalGeneration\nimport torch\n\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-base')\n\ninput_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\nlabels = torch.tensor([list(\"La vie est comme une bo\u00eete de chocolat.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n\nloss = model(input_ids, labels=labels).loss # forward pass\n", "from transformers import T5ForConditionalGeneration, AutoTokenizer\n\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-base')\ntokenizer = AutoTokenizer.from_pretrained('google/byt5-base')\n\nmodel_inputs = tokenizer([\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\")\nlabels = tokenizer([\"La vie est comme une bo\u00eete de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\").input_ids\n\nloss = model(**model_inputs, labels=labels).loss # forward pass\n"], "kandinsky-community/kandinsky-2-2-decoder": ["from diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"portrait of a young women, blue eyes, cinematic\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipe(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale =1.0, height=768, width=768).images[0]\nimage.save(\"portrait.png\")\n", "from PIL import Image\nimport requests\nfrom io import BytesIO\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\nresponse = requests.get(url)\noriginal_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\noriginal_image = original_image.resize((768, 512))\n", "from diffusers import AutoPipelineForImage2Image\nimport torch\n\npipe = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipe(prompt=prompt, image=original_image, strength=0.3, height=768, width=768).images[0]\n\nout.images[0].save(\"fantasy_land.png\")\n", "from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\nfrom diffusers.utils import load_image\nimport PIL\n\nimport torch\n\npipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n)\npipe_prior.to(\"cuda\")\n\nimg1 = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nimg2 = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/starry_night.jpeg\"\n)\n\n# add all the conditions we want to interpolate, can be either text or image\nimages_texts = [\"a cat\", img1, img2]\n\n# specify the weights for each condition in images_texts\nweights = [0.3, 0.3, 0.4]\n\n# We can leave the prompt empty\nprompt = \"\"\nprior_out = pipe_prior.interpolate(images_texts, weights)\n\npipe = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(**prior_out, height=768, width=768).images[0]\n\nimage.save(\"starry_cat.png\")\n"], "bigcode/gpt_bigcode-santacoder": [], "microsoft/resnet-101": ["from transformers import AutoFeatureExtractor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-101\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-101\")\n\ninputs = feature_extractor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\n"], "ai21labs/Jamba-v0.1": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n\ninput_ids = tokenizer(\"In the recent Super Bowl LVIII,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n\noutputs = model.generate(input_ids, max_new_tokens=216)\n\nprint(tokenizer.batch_decode(outputs))\n# [\"<|startoftext|>In the recent Super Bowl LVIII, the Kansas City Chiefs emerged victorious, defeating the San Francisco 49ers in a thrilling overtime showdown. The game was a nail-biter, with both teams showcasing their skills and determination.\\n\\nThe Chiefs, led by their star quarterback Patrick Mahomes, displayed their offensive prowess, while the 49ers, led by their strong defense, put up a tough fight. The game went into overtime, with the Chiefs ultimately securing the win with a touchdown.\\n\\nThe victory marked the Chiefs' second Super Bowl win in four years, solidifying their status as one of the top teams in the NFL. The game was a testament to the skill and talent of both teams, and a thrilling end to the NFL season.\\n\\nThe Super Bowl is not just about the game itself, but also about the halftime show and the commercials. This year's halftime show featured a star-studded lineup, including Usher, Alicia Keys, and Lil Jon. The show was a spectacle of music and dance, with the performers delivering an energetic and entertaining performance.\\n\"]\n", "from transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\n                                             torch_dtype=torch.bfloat16)    # you can also use torch_dtype=torch.float16\n", "from transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             device_map=\"auto\")\n", "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True,\n                                         llm_int8_skip_modules=[\"mamba\"])\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             quantization_config=quantization_config)\n", "import torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"ai21labs/Jamba-v0.1\", device_map='auto', torch_dtype=torch.bfloat16)\n\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\n        \"embed_tokens\", \n        \"x_proj\", \"in_proj\", \"out_proj\", # mamba\n        \"gate_proj\", \"up_proj\", \"down_proj\", # mlp\n        \"q_proj\", \"k_proj\", \"v_proj\" # attention\n    ],\n    task_type=\"CAUSAL_LM\",\n    bias=\"none\"\n)\n\ndataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\ntraining_args = SFTConfig(\n    output_dir=\"./results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n    logging_dir='./logs',\n    logging_steps=10,\n    learning_rate=1e-5,\n    dataset_text_field=\"quote\",\n)\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    peft_config=lora_config,\n    train_dataset=dataset,\n)\ntrainer.train()\n"], "deepseek-ai/DeepSeek-V2-Lite": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/DeepSeek-V2-Lite\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntext = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)\n", "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\n", "from transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 8192, 1\nmodel_name = \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you?\"}],\n    [{\"role\": \"user\", \"content\": \"Translate the following content into Chinese directly: DeepSeek-V2 adopts innovative architectures to guarantee economical training and efficient inference.\"}],\n    [{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n", "from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(\n    model='deepseek-chat',\n    openai_api_key=<your-deepseek-api-key>,\n    openai_api_base='https://api.deepseek.com/v1',\n    temperature=0.85,\n    max_tokens=8000)\n"], "EleutherAI/gpt-neox-20b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n"], "sentence-transformers/distiluse-base-multilingual-cased": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased')\nembeddings = model.encode(sentences)\nprint(embeddings)\n"], "Qwen/Qwen2-72B": [], "Qwen/Qwen1.5-32B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-32B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-32B-Chat\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "julien-c/dummy-unknown": ["import json\nimport os\nfrom transformers import RobertaConfig, RobertaForMaskedLM, TFRobertaForMaskedLM\n\nDIRNAME = \"./dummy-unknown\"\n\n\nconfig = RobertaConfig(10, 20, 1, 1, 40)\n\nmodel = RobertaForMaskedLM(config)\nmodel.save_pretrained(DIRNAME)\n\ntf_model = TFRobertaForMaskedLM.from_pretrained(DIRNAME, from_pt=True)\ntf_model.save_pretrained(DIRNAME)\n\n# Tokenizer:\n\nvocab = [\n    \"l\",\n    \"o\",\n    \"w\",\n    \"e\",\n    \"r\",\n    \"s\",\n    \"t\",\n    \"i\",\n    \"d\",\n    \"n\",\n    \"\\u0120\",\n    \"\\u0120l\",\n    \"\\u0120n\",\n    \"\\u0120lo\",\n    \"\\u0120low\",\n    \"er\",\n    \"\\u0120lowest\",\n    \"\\u0120newer\",\n    \"\\u0120wider\",\n    \"<unk>\",\n]\nvocab_tokens = dict(zip(vocab, range(len(vocab))))\nmerges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n\nvocab_file = os.path.join(DIRNAME, \"vocab.json\")\nmerges_file = os.path.join(DIRNAME, \"merges.txt\")\nwith open(vocab_file, \"w\", encoding=\"utf-8\") as fp:\n    fp.write(json.dumps(vocab_tokens) + \"\\n\")\nwith open(merges_file, \"w\", encoding=\"utf-8\") as fp:\n    fp.write(\"\\n\".join(merges))\n"], "emilianJR/epiCRealism": ["from diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"emilianJR/epiCRealism\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"YOUR PROMPT\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"image.png\")\n"], "blanchefort/rubert-base-cased-sentiment": ["\nimport torch\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\nmodel = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment', return_dict=True)\n\n@torch.no_grad()\ndef predict(text):\n    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n    outputs = model(**inputs)\n    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n    predicted = torch.argmax(predicted, dim=1).numpy()\n    return predicted\n"], "xlnet-large-cased": ["from transformers import XLNetTokenizer, XLNetModel\n\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\nmodel = XLNetModel.from_pretrained('xlnet-large-cased')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n"], "cl-tohoku/bert-base-japanese-v2": [], "stabilityai/stable-cascade-prior": ["import torch\nfrom diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", variant=\"bf16\", torch_dtype=torch.bfloat16)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", variant=\"bf16\", torch_dtype=torch.float16)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings.to(torch.float16),\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade.png\")\n", "import torch\nfrom diffusers import (\n    StableCascadeDecoderPipeline,\n    StableCascadePriorPipeline,\n    StableCascadeUNet,\n)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade-prior\", subfolder=\"prior_lite\")\ndecoder_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade\", subfolder=\"decoder_lite\")\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade.png\")\n", "import torch\nfrom diffusers import (\n    StableCascadeDecoderPipeline,\n    StableCascadePriorPipeline,\n    StableCascadeUNet,\n)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\nnegative_prompt = \"\"\n\nprior_unet = StableCascadeUNet.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-cascade/resolve/main/stage_c_bf16.safetensors\",\n    torch_dtype=torch.bfloat16\n)\ndecoder_unet = StableCascadeUNet.from_single_file(\n    \"https://huggingface.co/stabilityai/stable-cascade/blob/main/stage_b_bf16.safetensors\",\n    torch_dtype=torch.bfloat16\n)\n\nprior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet, torch_dtype=torch.bfloat16)\ndecoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet, torch_dtype=torch.bfloat16)\n\nprior.enable_model_cpu_offload()\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=1,\n    num_inference_steps=20\n)\n\ndecoder.enable_model_cpu_offload()\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type=\"pil\",\n    num_inference_steps=10\n).images[0]\ndecoder_output.save(\"cascade-single-file.png\")\n", "from diffusers import StableCascadeCombinedPipeline\n\npipe = StableCascadeCombinedPipeline.from_pretrained(\"stabilityai/stable-cascade\", variant=\"bf16\", torch_dtype=torch.bfloat16)\n\nprompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\noutput = pipe(\n    prompt=prompt,\n    negative_prompt=\"\",\n    num_inference_steps=10,\n    prior_num_inference_steps=20,\n    prior_guidance_scale=3.0,\n    width=1024,\n    height=1024,\n)\noutput.images[0].save(\"cascade-combined.png\")\n"], "deepset/bert-base-cased-squad2": ["# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-base-cased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n", "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-base-cased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n"], "bert-large-uncased-whole-word-masking": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n    {\n        'sequence': \"[CLS] hello i'm a fashion model. [SEP]\", \n        'score': 0.15813860297203064, \n        'token': 4827, \n        'token_str': 'fashion'\n    }, {\n        'sequence': \"[CLS] hello i'm a cover model. [SEP]\", \n        'score': 0.10551052540540695, \n        'token': 3104, \n        'token_str': 'cover'\n    }, {\n        'sequence': \"[CLS] hello i'm a male model. [SEP]\", \n        'score': 0.08340442180633545, \n        'token': 3287, \n        'token_str': 'male'\n    }, {\n        'sequence': \"[CLS] hello i'm a super model. [SEP]\", \n        'score': 0.036381796002388, \n        'token': 3565, \n        'token_str': 'super'\n    }, {\n        'sequence': \"[CLS] hello i'm a top model. [SEP]\", \n        'score': 0.03609578311443329, \n        'token': 2327, \n        'token_str': 'top'\n    }\n]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\nmodel = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\nmodel = TFBertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking')\n>>> unmasker(\"The man worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a waiter. [SEP]\",\n      \"score\":0.09823174774646759,\n      \"token\":15610,\n      \"token_str\":\"waiter\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a carpenter. [SEP]\",\n      \"score\":0.08976428955793381,\n      \"token\":10533,\n      \"token_str\":\"carpenter\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a mechanic. [SEP]\",\n      \"score\":0.06550426036119461,\n      \"token\":15893,\n      \"token_str\":\"mechanic\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a butcher. [SEP]\",\n      \"score\":0.04142395779490471,\n      \"token\":14998,\n      \"token_str\":\"butcher\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a barber. [SEP]\",\n      \"score\":0.03680137172341347,\n      \"token\":13362,\n      \"token_str\":\"barber\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress. [SEP]\",\n      \"score\":0.2669651508331299,\n      \"token\":13877,\n      \"token_str\":\"waitress\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a maid. [SEP]\",\n      \"score\":0.13054853677749634,\n      \"token\":10850,\n      \"token_str\":\"maid\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a nurse. [SEP]\",\n      \"score\":0.07987703382968903,\n      \"token\":6821,\n      \"token_str\":\"nurse\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a prostitute. [SEP]\",\n      \"score\":0.058545831590890884,\n      \"token\":19215,\n      \"token_str\":\"prostitute\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a cleaner. [SEP]\",\n      \"score\":0.03834161534905434,\n      \"token\":20133,\n      \"token_str\":\"cleaner\"\n   }\n]\n"], "OpenAssistant/reward-model-deberta-v3-large-v2": ["from transformers import AutoModelForSequenceClassification, AutoTokenizer\nreward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\nrank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\nquestion, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\ninputs = tokenizer(question, answer, return_tensors='pt')\nscore = rank_model(**inputs).logits[0].cpu().detach()\nprint(score)\n", "from transformers import AutoModelForSequenceClassification, AutoTokenizer\nreward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\nrank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\n\nquestion = \"I just came out of from jail, any suggestion of my future?\"\nhelpful = \"It's great to hear that you have been released from jail.\"\nbad = \"Go back to jail you scum\"\n\ninputs = tokenizer(question, helpful, return_tensors='pt')\ngood_score = rank_model(**inputs).logits[0].cpu().detach()\n\ninputs = tokenizer(question, bad, return_tensors='pt')\nbad_score = rank_model(**inputs).logits[0].cpu().detach()\nprint(good_score > bad_score) # tensor([True])\n"], "bigcode/starcoder2-15b": ["# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "openlm-research/open_llama_7b": ["import torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\nmodel_path = 'openlm-research/open_llama_3b'\n# model_path = 'openlm-research/open_llama_7b'\n\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto',\n)\n\nprompt = 'Q: What is the largest animal?\\nA:'\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=32\n)\nprint(tokenizer.decode(generation_output[0]))\n"], "distilbert-base-german-cased": [], "Qwen/Qwen1.5-72B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-72B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-72B-Chat\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "unsloth/llama-3-8b-Instruct": [], "GroNLP/bert-base-dutch-cased": ["from transformers import AutoTokenizer, AutoModel, TFAutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\nmodel = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # PyTorch\nmodel = TFAutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # Tensorflow\n"], "yahma/llama-7b-hf": [], "skt/kogpt2-base-v2": [], "kandinsky-community/kandinsky-2-2-prior": ["from diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"portrait of a young women, blue eyes, cinematic\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipe(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale =1.0, height=768, width=768).images[0]\nimage.save(\"portrait.png\")\n", "from PIL import Image\nimport requests\nfrom io import BytesIO\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\nresponse = requests.get(url)\noriginal_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\noriginal_image = original_image.resize((768, 512))\n", "from diffusers import AutoPipelineForImage2Image\nimport torch\n\npipe = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipe(prompt=prompt, image=original_image, strength=0.3, height=768, width=768).images[0]\n\nout.images[0].save(\"fantasy_land.png\")\n", "from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\nfrom diffusers.utils import load_image\nimport PIL\n\nimport torch\n\npipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n)\npipe_prior.to(\"cuda\")\n\nimg1 = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nimg2 = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/starry_night.jpeg\"\n)\n\n# add all the conditions we want to interpolate, can be either text or image\nimages_texts = [\"a cat\", img1, img2]\n\n# specify the weights for each condition in images_texts\nweights = [0.3, 0.3, 0.4]\n\n# We can leave the prompt empty\nprompt = \"\"\nprior_out = pipe_prior.interpolate(images_texts, weights)\n\npipe = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(**prior_out, height=768, width=768).images[0]\n\nimage.save(\"starry_cat.png\")\n", "from diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\nimport numpy as np\n\npipe = AutoPipelineForInpainting.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt = \"a hat\"\n\ninit_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nmask = np.zeros((768, 768), dtype=np.float32)\n# Let's mask out an area above the cat's head\nmask[:250, 250:-250] = 1\n\n\nout = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask,\n    height=768,\n    width=768,\n    num_inference_steps=150,\n)\n\nimage = out.images[0]\nimage.save(\"cat_with_hat.png\")\n", "# For PIL input\nimport PIL.ImageOps\nmask = PIL.ImageOps.invert(mask)\n\n# For PyTorch and Numpy input\nmask = 1 - mask\n", "import torch\nimport numpy as np\n\nfrom transformers import pipeline\nfrom diffusers.utils import load_image\n\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n\n# let's take an image and extract its depth map.\ndef make_hint(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    hint = detected_map.permute(2, 0, 1)\n    return hint\n\nimg = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n).resize((768, 768))\n\n# We can use the `depth-estimation` pipeline from transformers to process the image and retrieve its depth map.\ndepth_estimator = pipeline(\"depth-estimation\")\nhint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n\n# Now, we load the prior pipeline and the text-to-image controlnet pipeline\npipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n)\npipe_prior = pipe_prior.to(\"cuda\")\n\npipe = KandinskyV22ControlnetPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\n# We pass the prompt and negative prompt through the prior to generate image embeddings\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\nimage_emb, zero_image_emb = pipe_prior(\n    prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator\n).to_tuple()\n\n# Now we can pass the image embeddings and the depth image we extracted to the controlnet pipeline. With Kandinsky 2.2, only prior pipelines accept `prompt` input. You do not need to pass the prompt to the controlnet pipeline.\nimages = pipe(\n    image_embeds=image_emb,\n    negative_image_embeds=zero_image_emb,\n    hint=hint,\n    num_inference_steps=50,\n    generator=generator,\n    height=768,\n    width=768,\n).images\nimages[0].save(\"robot_cat.png\")\n", "import torch\nimport numpy as np\n\nfrom diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\nfrom diffusers.utils import load_image\nfrom transformers import pipeline\n\nimg = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinskyv22/cat.png\"\n).resize((768, 768))\n\ndef make_hint(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    hint = detected_map.permute(2, 0, 1)\n    return hint\n\ndepth_estimator = pipeline(\"depth-estimation\")\nhint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n\npipe_prior = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n)\npipe_prior = pipe_prior.to(\"cuda\")\n\npipe = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\n\n# run prior pipeline\n\nimg_emb = pipe_prior(prompt=prompt, image=img, strength=0.85, generator=generator)\nnegative_emb = pipe_prior(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)\n\n# run controlnet img2img pipeline\nimages = pipe(\n    image=img,\n    strength=0.5,\n    image_embeds=img_emb.image_embeds,\n    negative_image_embeds=negative_emb.image_embeds,\n    hint=hint,\n    num_inference_steps=50,\n    generator=generator,\n    height=768,\n    width=768,\n).images\n\nimages[0].save(\"robot_cat.png\")\n"], "facebook/convnext-base-224-22k-1k": ["from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained(\"facebook/convnext-base-224-22k-1k\")\nmodel = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-base-384-224-1k\")\n\ninputs = feature_extractor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "deepset/minilm-uncased-squad2": ["# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/minilm-uncased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n", "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/minilm-uncased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n"], "Qwen/Qwen1.5-14B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-14B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-14B-Chat\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "bert-large-cased-whole-word-masking-finetuned-squad": [], "allenai/OLMo-1.7-7B-hf": ["from huggingface_hub import list_repo_refs\nout = list_repo_refs(\"allenai/OLMo-7B-0424-hf\")\nbranches = [b.name for b in out.branches]\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\nolmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-7B-0424-hf\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-7B-0424-hf\")\nmessage = [\"Language modeling is\"]\ninputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n# optional verifying cuda\n# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n# olmo = olmo.to('cuda')\nresponse = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n>> 'Language modeling is the first step to build natural language generation...'\n", "from transformers import pipeline\nolmo_pipe = pipeline(\"text-generation\", model=\"allenai/OLMo-7B-0424-hf\")\nprint(olmo_pipe(\"Language modeling is \"))\n>> 'Language modeling is a branch of natural language processing that aims to...'\n"], "Salesforce/codet5-base-multi-sum": ["from transformers import RobertaTokenizer, T5ForConditionalGeneration\n\nif __name__ == '__main__':\n    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base-multi-sum')\n    model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum')\n\n    text = \"\"\"def svg_to_image(string, size=None):\n    if isinstance(string, unicode):\n        string = string.encode('utf-8')\n        renderer = QtSvg.QSvgRenderer(QtCore.QByteArray(string))\n    if not renderer.isValid():\n        raise ValueError('Invalid SVG data.')\n    if size is None:\n        size = renderer.defaultSize()\n        image = QtGui.QImage(size, QtGui.QImage.Format_ARGB32)\n        painter = QtGui.QPainter(image)\n        renderer.render(painter)\n    return image\"\"\"\n\n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\n    generated_ids = model.generate(input_ids, max_length=20)\n    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n    # this prints: \"Convert a SVG string to a QImage.\"\n"], "stabilityai/stablelm-3b-4e1t": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\nmodel = AutoModelForCausalLM.from_pretrained(\n  \"stabilityai/stablelm-3b-4e1t\",\n  torch_dtype=\"auto\",\n)\nmodel.cuda()\ninputs = tokenizer(\"The weather is always wonderful\", return_tensors=\"pt\").to(model.device)\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=64,\n  temperature=0.75,\n  top_p=0.95,\n  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\nmodel = AutoModelForCausalLM.from_pretrained(\n  \"stabilityai/stablelm-3b-4e1t\",\n  torch_dtype=\"auto\",\n  attn_implementation=\"flash_attention_2\",\n)\nmodel.cuda()\ninputs = tokenizer(\"The weather is always wonderful\", return_tensors=\"pt\").to(model.device)\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=64,\n  temperature=0.75,\n  top_p=0.95,\n  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n"], "codellama/CodeLlama-34b-Instruct-hf": [], "Qwen/Qwen-7B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # \u53ef\u6307\u5b9a\u4e0d\u540c\u7684\u751f\u6210\u957f\u5ea6\u3001top_p\u7b49\u76f8\u5173\u8d85\u53c2\n\n# \u7b2c\u4e00\u8f6e\u5bf9\u8bdd 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\nprint(response)\n# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n\n# \u7b2c\u4e8c\u8f6e\u5bf9\u8bdd 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\", history=history)\nprint(response)\n# \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\n# \u6545\u4e8b\u7684\u4e3b\u4eba\u516c\u53eb\u674e\u660e\uff0c\u4ed6\u6765\u81ea\u4e00\u4e2a\u666e\u901a\u7684\u5bb6\u5ead\uff0c\u7236\u6bcd\u90fd\u662f\u666e\u901a\u7684\u5de5\u4eba\u3002\u4ece\u5c0f\uff0c\u674e\u660e\u5c31\u7acb\u4e0b\u4e86\u4e00\u4e2a\u76ee\u6807\uff1a\u8981\u6210\u4e3a\u4e00\u540d\u6210\u529f\u7684\u4f01\u4e1a\u5bb6\u3002\n# \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff0c\u674e\u660e\u52e4\u594b\u5b66\u4e60\uff0c\u8003\u4e0a\u4e86\u5927\u5b66\u3002\u5728\u5927\u5b66\u671f\u95f4\uff0c\u4ed6\u79ef\u6781\u53c2\u52a0\u5404\u79cd\u521b\u4e1a\u6bd4\u8d5b\uff0c\u83b7\u5f97\u4e86\u4e0d\u5c11\u5956\u9879\u3002\u4ed6\u8fd8\u5229\u7528\u8bfe\u4f59\u65f6\u95f4\u53bb\u5b9e\u4e60\uff0c\u79ef\u7d2f\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u3002\n# \u6bd5\u4e1a\u540e\uff0c\u674e\u660e\u51b3\u5b9a\u5f00\u59cb\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u5f00\u59cb\u5bfb\u627e\u6295\u8d44\u673a\u4f1a\uff0c\u4f46\u591a\u6b21\u90fd\u88ab\u62d2\u7edd\u4e86\u3002\u7136\u800c\uff0c\u4ed6\u5e76\u6ca1\u6709\u653e\u5f03\u3002\u4ed6\u7ee7\u7eed\u52aa\u529b\uff0c\u4e0d\u65ad\u6539\u8fdb\u81ea\u5df1\u7684\u521b\u4e1a\u8ba1\u5212\uff0c\u5e76\u5bfb\u627e\u65b0\u7684\u6295\u8d44\u673a\u4f1a\u3002\n# \u6700\u7ec8\uff0c\u674e\u660e\u6210\u529f\u5730\u83b7\u5f97\u4e86\u4e00\u7b14\u6295\u8d44\uff0c\u5f00\u59cb\u4e86\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u6210\u7acb\u4e86\u4e00\u5bb6\u79d1\u6280\u516c\u53f8\uff0c\u4e13\u6ce8\u4e8e\u5f00\u53d1\u65b0\u578b\u8f6f\u4ef6\u3002\u5728\u4ed6\u7684\u9886\u5bfc\u4e0b\uff0c\u516c\u53f8\u8fc5\u901f\u53d1\u5c55\u8d77\u6765\uff0c\u6210\u4e3a\u4e86\u4e00\u5bb6\u6210\u529f\u7684\u79d1\u6280\u4f01\u4e1a\u3002\n# \u674e\u660e\u7684\u6210\u529f\u5e76\u4e0d\u662f\u5076\u7136\u7684\u3002\u4ed6\u52e4\u594b\u3001\u575a\u97e7\u3001\u52c7\u4e8e\u5192\u9669\uff0c\u4e0d\u65ad\u5b66\u4e60\u548c\u6539\u8fdb\u81ea\u5df1\u3002\u4ed6\u7684\u6210\u529f\u4e5f\u8bc1\u660e\u4e86\uff0c\u53ea\u8981\u52aa\u529b\u594b\u6597\uff0c\u4efb\u4f55\u4eba\u90fd\u6709\u53ef\u80fd\u53d6\u5f97\u6210\u529f\u3002\n\n# \u7b2c\u4e09\u8f6e\u5bf9\u8bdd 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u8fd9\u4e2a\u6545\u4e8b\u8d77\u4e00\u4e2a\u6807\u9898\", history=history)\nprint(response)\n# \u300a\u594b\u6597\u521b\u4e1a\uff1a\u4e00\u4e2a\u5e74\u8f7b\u4eba\u7684\u6210\u529f\u4e4b\u8def\u300b\n"], "lmsys/vicuna-7b-v1.3": [], "lmsys/vicuna-13b-v1.5-16k": [], "nvidia/mit-b5": ["from transformers import SegformerFeatureExtractor, SegformerForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/mit-b5\")\nmodel = SegformerForImageClassification.from_pretrained(\"nvidia/mit-b5\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens": ["from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\nmodel = AutoModel.from_pretrained('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "codellama/CodeLlama-13b-Instruct-hf": [], "google/t5-v1_1-small": [], "lordtt13/emo-mobilebert": ["from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"lordtt13/emo-mobilebert\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"lordtt13/emo-mobilebert\")\n\nnlp_sentence_classif = transformers.pipeline('sentiment-analysis', model = model, tokenizer = tokenizer)\nnlp_sentence_classif(\"I've never had such a bad day in my life\")\n# Output: [{'label': 'sad', 'score': 0.93153977394104}]\n"], "TheBloke/Llama-2-13B-chat-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "kakaobrain/align-base": ["import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ncandidate_labels = [\"an image of a cat\", \"an image of a dog\"]\n\ninputs = processor(text=candidate_labels, images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# this is the image-text similarity score\nlogits_per_image = outputs.logits_per_image\n# we can take the softmax to get the label probabilities\nprobs = logits_per_image.softmax(dim=1)\nprint(probs)\n", "import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = \"an image of a cat\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# multi-modal text embedding\ntext_embeds = outputs.text_embeds\n\n# multi-modal image embedding\nimage_embeds = outputs.image_embeds\n", "import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\n# image embeddings\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nimage_embeds = model.get_image_features(\n    pixel_values=inputs['pixel_values'],\n)\n\n# text embeddings\ntext = \"an image of a cat\"\ninputs = processor(text=text, return_tensors=\"pt\")\n\ntext_embeds = model.get_text_features(\n    input_ids=inputs['input_ids'],\n    attention_mask=inputs['attention_mask'],\n    token_type_ids=inputs['token_type_ids'],\n)\n"], "Qwen/CodeQwen1.5-7B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/CodeQwen1.5-7B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/CodeQwen1.5-7B-Chat\")\n\nprompt = \"Write a quicksort algorithm in python.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "Babelscape/rebel-large": ["@inproceedings{huguet-cabot-navigli-2021-rebel-relation,\n    title = \"{REBEL}: Relation Extraction By End-to-end Language generation\",\n    author = \"Huguet Cabot, Pere-Llu{\\'\\i}s  and\n      Navigli, Roberto\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-emnlp.204\",\n    pages = \"2370--2381\",\n    abstract = \"Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types. We show our model{'}s flexibility by fine-tuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them.\",\n}\n", "from transformers import pipeline\n\ntriplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n# We need to use the tokenizer manually since we need special tokens.\nextracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(\"Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\", return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\nprint(extracted_text[0])\n# Function to parse the generated text and extract the triplets\ndef extract_triplets(text):\n    triplets = []\n    relation, subject, relation, object_ = '', '', '', ''\n    text = text.strip()\n    current = 'x'\n    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n        if token == \"<triplet>\":\n            current = 't'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n                relation = ''\n            subject = ''\n        elif token == \"<subj>\":\n            current = 's'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n            object_ = ''\n        elif token == \"<obj>\":\n            current = 'o'\n            relation = ''\n        else:\n            if current == 't':\n                subject += ' ' + token\n            elif current == 's':\n                object_ += ' ' + token\n            elif current == 'o':\n                relation += ' ' + token\n    if subject != '' and relation != '' and object_ != '':\n        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n    return triplets\nextracted_triplets = extract_triplets(extracted_text[0])\nprint(extracted_triplets)\n", "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ndef extract_triplets(text):\n    triplets = []\n    relation, subject, relation, object_ = '', '', '', ''\n    text = text.strip()\n    current = 'x'\n    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n        if token == \"<triplet>\":\n            current = 't'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n                relation = ''\n            subject = ''\n        elif token == \"<subj>\":\n            current = 's'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n            object_ = ''\n        elif token == \"<obj>\":\n            current = 'o'\n            relation = ''\n        else:\n            if current == 't':\n                subject += ' ' + token\n            elif current == 's':\n                object_ += ' ' + token\n            elif current == 'o':\n                relation += ' ' + token\n    if subject != '' and relation != '' and object_ != '':\n        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n    return triplets\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\ngen_kwargs = {\n    \"max_length\": 256,\n    \"length_penalty\": 0,\n    \"num_beams\": 3,\n    \"num_return_sequences\": 3,\n}\n\n# Text to extract triplets from\ntext = 'Punta Cana is a resort town in the municipality of Hig\u00fcey, in La Altagracia Province, the easternmost province of the Dominican Republic.'\n\n# Tokenizer text\nmodel_inputs = tokenizer(text, max_length=256, padding=True, truncation=True, return_tensors = 'pt')\n\n# Generate\ngenerated_tokens = model.generate(\n    model_inputs[\"input_ids\"].to(model.device),\n    attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n    **gen_kwargs,\n)\n\n# Extract text\ndecoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n\n# Extract triplets\nfor idx, sentence in enumerate(decoded_preds):\n    print(f'Prediction triplets sentence {idx}')\n    print(extract_triplets(sentence))\n"], "hkunlp/instructor-xl": ["from InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-xl')\nsentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\ninstruction = \"Represent the Science title:\"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\n", "from sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'], \n               ['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\nsentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n               ['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\n", "import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [['Represent the Wikipedia question for retrieving supporting documents: ','where is the food stored in a yam plant']]\ncorpus = [['Represent the Wikipedia document for retrieval: ','Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term \"mixed economies\" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.'],\n          ['Represent the Wikipedia document for retrieval: ',\"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loans\u00e2\u20ac\u201dand some scholars have argued that the theory's use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession\"],\n          ['Represent the Wikipedia document for retrieval: ','Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\n", "import sklearn.cluster\nsentences = [['Represent the Medicine sentence for clustering: ','Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity'],\n             ['Represent the Medicine sentence for clustering: ','Comparison of Atmospheric Neutrino Flux Calculations at Low Energies'],\n             ['Represent the Medicine sentence for clustering: ','Fermion Bags in the Massive Gross-Neveu Model'],\n             ['Represent the Medicine sentence for clustering: ',\"QCD corrections to Associated t-tbar-H production at the Tevatron\"],\n             ['Represent the Medicine sentence for clustering: ','A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)\n"], "deepset/gbert-large": [], "EleutherAI/pythia-2.8b": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "intfloat/e5-small": ["import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: summit define',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-small')\nmodel = AutoModel.from_pretrained('intfloat/e5-small')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-small')\ninput_texts = [\n    'query: how much protein should a female eat',\n    'query: summit define',\n    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n    \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\n"], "Lykon/dreamshaper-7": ["from diffusers import AutoPipelineForText2Image, DEISMultistepScheduler\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained('lykon/dreamshaper-7', torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = DEISMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\nprompt = \"portrait photo of muscular bearded guy in a worn mech suit, light bokeh, intricate, steel metal, elegant, sharp focus, soft lighting, vibrant colors\"\n\ngenerator = torch.manual_seed(33)\nimage = pipe(prompt, generator=generator, num_inference_steps=25).images[0]  \nimage.save(\"./image.png\")\n"], "deepseek-ai/deepseek-coder-6.7b-base": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True).cuda()\ninput_text = \"\"\"<\uff5cfim\u2581begin\uff5c>def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[0]\n    left = []\n    right = []\n<\uff5cfim\u2581hole\uff5c>\n        if arr[i] < pivot:\n            left.append(arr[i])\n        else:\n            right.append(arr[i])\n    return quick_sort(left) + [pivot] + quick_sort(right)<\uff5cfim\u2581end\uff5c>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True).cuda()\n\ninput_text = \"\"\"#utils.py\nimport torch\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\ndef load_data():\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # Convert numpy data to PyTorch tensors\n    X_train = torch.tensor(X_train, dtype=torch.float32)\n    X_test = torch.tensor(X_test, dtype=torch.float32)\n    y_train = torch.tensor(y_train, dtype=torch.int64)\n    y_test = torch.tensor(y_test, dtype=torch.int64)\n    \n    return X_train, X_test, y_train, y_test\n\ndef evaluate_predictions(y_test, y_pred):\n    return accuracy_score(y_test, y_pred)\n#model.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass IrisClassifier(nn.Module):\n    def __init__(self):\n        super(IrisClassifier, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(4, 16),\n            nn.ReLU(),\n            nn.Linear(16, 3)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n    def train_model(self, X_train, y_train, epochs, lr, batch_size):\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.parameters(), lr=lr)\n        \n        # Create DataLoader for batches\n        dataset = TensorDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n        for epoch in range(epochs):\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                outputs = self(batch_X)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n\n    def predict(self, X_test):\n        with torch.no_grad():\n            outputs = self(X_test)\n            _, predicted = outputs.max(1)\n        return predicted.numpy()\n#main.py\nfrom utils import load_data, evaluate_predictions\nfrom model import IrisClassifier as Classifier\n\ndef main():\n    # Model training and evaluation\n\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_new_tokens=140)\nprint(tokenizer.decode(outputs[0]))\n"], "beomi/kcbert-base": ["from transformers import AutoTokenizer, AutoModelWithLMHead\n\n# Base Model (108M)\n\ntokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-base\")\n\n# Large Model (334M)\n\ntokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-large\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-large\")\n", "import re\nimport emoji\nfrom soynlp.normalizer import repeat_normalize\n\nemojis = list({y for x in emoji.UNICODE_EMOJI.values() for y in x.keys()})\nemojis = ''.join(emojis)\npattern = re.compile(f'[^ .,?!/@$%~\uff05\u00b7\u223c()\\x00-\\x7F\u3131-\u3163\uac00-\ud7a3{emojis}]+')\nurl_pattern = re.compile(\n    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n\ndef clean(x):\n    x = pattern.sub(' ', x)\n    x = url_pattern.sub('', x)\n    x = x.strip()\n    x = repeat_normalize(x, num_repeats=2)\n    return x\n"], "microsoft/BiomedVLP-CXR-BERT-specialized": ["import torch\nfrom transformers import AutoModel, AutoTokenizer\n\n# Load the model and tokenizer\nurl = \"microsoft/BiomedVLP-CXR-BERT-specialized\"\ntokenizer = AutoTokenizer.from_pretrained(url, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(url, trust_remote_code=True)\n\n# Input text prompts (e.g., reference, synonym, contradiction)\ntext_prompts = [\"There is no pneumothorax or pleural effusion\",\n                \"No pleural effusion or pneumothorax is seen\",\n                \"The extent of the pleural effusion is constant.\"]\n\n# Tokenize and compute the sentence embeddings\ntokenizer_output = tokenizer.batch_encode_plus(batch_text_or_text_pairs=text_prompts,\n                                               add_special_tokens=True,\n                                               padding='longest',\n                                               return_tensors='pt')\nembeddings = model.get_projected_text_embeddings(input_ids=tokenizer_output.input_ids,\n                                                 attention_mask=tokenizer_output.attention_mask)\n\n# Compute the cosine similarity of sentence embeddings obtained from input text prompts.\nsim = torch.mm(embeddings, embeddings.t())\n"], "bigcode/tiny_starcoder_py": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/tiny_starcoder_py\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "input_text = \"<fim_prefix>def print_one_two_three():\\n    print('one')\\n    <fim_suffix>\\n    print('three')<fim_middle>\"\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "microsoft/Phi-3-small-128k-instruct": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\nmodel_id = \"microsoft/Phi-3-small-128k-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=\"auto\", \n    trust_remote_code=True, \n)\nassert torch.cuda.is_available(), \"This model needs a GPU to run ...\"\ndevice = torch.cuda.current_device()\nmodel = model.to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=device\n)\n\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"return_full_text\": False,\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n"], "allenai/led-base-16384": [], "Salesforce/instructblip-flan-t5-xl": ["from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\nimport torch\nfrom PIL import Image\nimport requests\n\nmodel = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\nprocessor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\nurl = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nprompt = \"What is unusual about this image?\"\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(\n        **inputs,\n        do_sample=False,\n        num_beams=5,\n        max_length=256,\n        min_length=1,\n        top_p=0.9,\n        repetition_penalty=1.5,\n        length_penalty=1.0,\n        temperature=1,\n)\ngenerated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n"], "google/vit-large-patch16-224": ["from transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "hakurei/waifu-diffusion": ["import torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    'hakurei/waifu-diffusion',\n    torch_dtype=torch.float32\n).to('cuda')\n\nprompt = \"1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=6)[\"sample\"][0]  \n    \nimage.save(\"test.png\")\n"], "openlm-research/open_llama_3b": ["import torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\nmodel_path = 'openlm-research/open_llama_3b'\n# model_path = 'openlm-research/open_llama_7b'\n\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto',\n)\n\nprompt = 'Q: What is the largest animal?\\nA:'\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=32\n)\nprint(tokenizer.decode(generation_output[0]))\n"], "Qwen/Qwen1.5-1.8B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-1.8B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "Open-Orca/Mistral-7B-OpenOrca": ["@software{lian2023mistralorca1\n  title = {MistralOrca: Mistral-7B Model Instruct-tuned on Filtered OpenOrcaV1 GPT-4 Dataset},\n  author = {Wing Lian and Bleys Goodson and Guan Wang and Eugene Pentland and Austin Cook and Chanvichet Vong and \"Teknium\"},\n  year = {2023},\n  publisher = {HuggingFace},\n  journal = {HuggingFace repository},\n  howpublished = {\\url{https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca},\n}\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{longpre2023flan,\n      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, \n      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},\n      year={2023},\n      eprint={2301.13688},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n"], "albert-xxlarge-v2": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xxlarge-v2')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u00e2\u2013\u0081modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u00e2\u2013\u0081modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u00e2\u2013\u0081model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u00e2\u2013\u0081runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u00e2\u2013\u0081lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2')\nmodel = AlbertModel.from_pretrained(\"albert-xxlarge-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2')\nmodel = TFAlbertModel.from_pretrained(\"albert-xxlarge-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xxlarge-v2')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u00e2\u2013\u0081shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u00e2\u2013\u0081blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u00e2\u2013\u0081lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u00e2\u2013\u0081receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u00e2\u2013\u0081paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u00e2\u2013\u0081waitress\"\n   }\n]\n"], "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup": [], "facebook/xglm-564M": ["import torch\nimport torch.nn.functional as F\n\nfrom transformers import XGLMTokenizer, XGLMForCausalLM\n\ntokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\nmodel = XGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n\ndata_samples = {\n    'en': [\n        {\n            \"premise\": \"I wanted to conserve energy.\",\n            \"choice1\": \"I swept the floor in the unoccupied room.\",\n            \"choice2\": \"I shut off the light in the unoccupied room.\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"The flame on the candle went out.\",\n            \"choice1\": \"I blew on the wick.\",\n            \"choice2\": \"I put a match to the wick.\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ],\n    'zh': [\n        {\n            \"premise\": \"\u6211\u60f3\u8282\u7ea6\u80fd\u6e90\u3002\",\n            \"choice1\": \"\u6211\u5728\u7a7a\u7740\u7684\u623f\u95f4\u91cc\u626b\u4e86\u5730\u677f\u3002\",\n            \"choice2\": \"\u6211\u628a\u7a7a\u623f\u95f4\u91cc\u7684\u706f\u5173\u4e86\u3002\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"\u8721\u70db\u4e0a\u7684\u706b\u7130\u7184\u706d\u4e86\u3002\",\n            \"choice1\": \"\u6211\u5439\u706d\u4e86\u706f\u82af\u3002\",\n            \"choice2\": \"\u6211\u628a\u4e00\u6839\u706b\u67f4\u653e\u5728\u706f\u82af\u4e0a\u3002\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ],\n    'hi': [\n        {\n            \"premise\": \"M te vle kons\u00e8ve en\u00e8ji.\",\n            \"choice1\": \"Mwen te fin baleye chanm lib la.\",\n            \"choice2\": \"Mwen te femen limy\u00e8 nan chanm lib la.\",\n            \"question\": \"effect\",\n            \"label\": \"1\"\n        },\n        {\n            \"premise\": \"Flam bouji a te etenn.\",\n            \"choice1\": \"Mwen te soufle bouji a.\",\n            \"choice2\": \"Mwen te limen m\u00e8ch bouji a.\",\n            \"question\": \"cause\",\n            \"label\": \"0\"\n        }\n    ]\n}\n\ndef get_logprobs(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids, output_ids = inputs[\"input_ids\"], inputs[\"input_ids\"][:, 1:]\n    outputs = model(**inputs, labels=input_ids)\n    logits = outputs.logits\n    logprobs = torch.gather(F.log_softmax(logits, dim=2), 2, output_ids.unsqueeze(2))\n    return logprobs\n\n# Zero-shot evaluation for the Choice of Plausible Alternatives (COPA) task.\n# A return value of 0 indicates that the first alternative is more plausible,\n# while 1 indicates that the second alternative is more plausible.\ndef COPA_eval(prompt, alternative1, alternative2):\n    lprob1 = get_logprobs(prompt + \"\\n\" + alternative1).sum()\n    lprob2 = get_logprobs(prompt + \"\\n\" + alternative2).sum()\n    return 0 if lprob1 > lprob2 else 1\n\nfor lang in data_samples_long:\n    for idx, example in enumerate(data_samples_long[lang]):\n        predict = COPA_eval(example[\"premise\"], example[\"choice1\"], example[\"choice2\"])\n        print(f'{lang}-{idx}', predict, example['label'])\n        \n# en-0 1 1\n# en-1 0 0\n# zh-0 1 1\n# zh-1 0 0\n# hi-0 1 1\n# hi-1 0 0\n"], "PixArt-alpha/PixArt-Sigma-XL-2-1024-MS": ["import torch\nfrom diffusers import Transformer2DModel, PixArtSigmaPipeline\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nweight_dtype = torch.float16\n\npipe = PixArtSigmaPipeline.from_pretrained(\n    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\", \n    torch_dtype=weight_dtype,\n    use_safetensors=True,\n)\npipe.to(device)\n\n# Enable memory optimizations.\n# pipe.enable_model_cpu_offload()\n\nprompt = \"A small cactus with a happy face in the Sahara desert.\"\nimage = pipe(prompt).images[0]\nimage.save(\"./catcus.png\")\n"], "facebook/detr-resnet-50-panoptic": ["import io\nimport requests\nfrom PIL import Image\nimport torch\nimport numpy\n\nfrom transformers import DetrFeatureExtractor, DetrForSegmentation\nfrom transformers.models.detr.feature_extraction_detr import rgb_to_id\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\nmodel = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n\n# prepare image for the model\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**inputs)\n\n# use the `post_process_panoptic` method of `DetrFeatureExtractor` to convert to COCO format\nprocessed_sizes = torch.as_tensor(inputs[\"pixel_values\"].shape[-2:]).unsqueeze(0)\nresult = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\n\n# the segmentation is stored in a special-format png\npanoptic_seg = Image.open(io.BytesIO(result[\"png_string\"]))\npanoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)\n# retrieve the ids corresponding to each mask\npanoptic_seg_id = rgb_to_id(panoptic_seg)\n"], "SenseTime/deformable-detr": ["from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr\")\nmodel = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.7\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n"], "deepseek-ai/deepseek-coder-1.3b-instruct": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"], "EleutherAI/pythia-6.9b": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "Qwen/Qwen1.5-72B": [], "01-ai/Yi-1.5-9B": [], "togethercomputer/LLaMA-2-7B-32K": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\", trust_remote_code=True, torch_dtype=torch.float16)\n\ninput_context = \"Your text here\"\ninput_ids = tokenizer.encode(input_context, return_tensors=\"pt\")\noutput = model.generate(input_ids, max_length=128, temperature=0.7)\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(output_text)\n"], "baichuan-inc/Baichuan2-7B-Chat": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\nmodel.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\")\nmessages = []\nmessages.append({\"role\": \"user\", \"content\": \"\u89e3\u91ca\u4e00\u4e0b\u201c\u6e29\u6545\u800c\u77e5\u65b0\u201d\"})\nresponse = model.chat(tokenizer, messages)\nprint(response)\n\"\u6e29\u6545\u800c\u77e5\u65b0\"\u662f\u4e00\u53e5\u4e2d\u56fd\u53e4\u4ee3\u7684\u6210\u8bed\uff0c\u51fa\u81ea\u300a\u8bba\u8bed\u00b7\u4e3a\u653f\u300b\u7bc7\u3002\u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u662f\uff1a\u901a\u8fc7\u56de\u987e\u8fc7\u53bb\uff0c\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u65b0\u7684\u77e5\u8bc6\u548c\u7406\u89e3\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u5b66\u4e60\u5386\u53f2\u548c\u7ecf\u9a8c\u53ef\u4ee5\u8ba9\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u73b0\u5728\u548c\u672a\u6765\u3002\n\n\u8fd9\u53e5\u8bdd\u9f13\u52b1\u6211\u4eec\u5728\u5b66\u4e60\u548c\u751f\u6d3b\u4e2d\u4e0d\u65ad\u5730\u56de\u987e\u548c\u53cd\u601d\u8fc7\u53bb\u7684\u7ecf\u9a8c\uff0c\u4ece\u800c\u83b7\u5f97\u65b0\u7684\u542f\u793a\u548c\u6210\u957f\u3002\u901a\u8fc7\u91cd\u6e29\u65e7\u7684\u77e5\u8bc6\u548c\u7ecf\u5386\uff0c\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u65b0\u7684\u89c2\u70b9\u548c\u7406\u89e3\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u4e16\u754c\u548c\u6311\u6218\u3002\n"], "naver-clova-ix/donut-base-finetuned-cord-v2": [], "facebook/deit-base-distilled-patch16-224": ["from transformers import AutoFeatureExtractor, DeiTForImageClassificationWithTeacher\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-base-distilled-patch16-224')\nmodel = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-base-distilled-patch16-224')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "tiiuae/falcon-rw-1b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-rw-1b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-rw-1b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "codellama/CodeLlama-34b-hf": ["from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"codellama/CodeLlama-34b-hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'import socket\\n\\ndef ping_exponential_backoff(host: str):',\n    do_sample=True,\n    top_k=10,\n    temperature=0.1,\n    top_p=0.95,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "microsoft/swin-base-patch4-window7-224": ["from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-base-patch4-window7-224\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "01-ai/Yi-1.5-9B-Chat": [], "baichuan-inc/Baichuan-7B": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('\u767b\u9e73\u96c0\u697c->\u738b\u4e4b\u6da3\\n\u591c\u96e8\u5bc4\u5317->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('Hamlet->Shakespeare\\nOne Hundred Years of Solitude->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n"], "stabilityai/stablelm-zephyr-3b": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('stabilityai/stablelm-zephyr-3b')\nmodel = AutoModelForCausalLM.from_pretrained(\n    'stabilityai/stablelm-zephyr-3b',\n    device_map=\"auto\"\n)\n\nprompt = [{'role': 'user', 'content': 'List 3 synonyms for the word \"tiny\"'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n)\n\ntokens = model.generate(\n    inputs.to(model.device),\n    max_new_tokens=1024,\n    temperature=0.8,\n    do_sample=True\n)\n\nprint(tokenizer.decode(tokens[0], skip_special_tokens=False))\n"], "google/electra-small-generator": ["from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"google/electra-small-generator\",\n    tokenizer=\"google/electra-small-generator\"\n)\n\nprint(\n    fill_mask(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\")\n)\n"], "rinna/japanese-gpt-neox-3.6b": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-3.6b\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-3.6b\")\n\nif torch.cuda.is_available():\n    model = model.to(\"cuda\")\n\ntext = \"\u897f\u7530\u5e7e\u591a\u90ce\u306f\u3001\"\ntoken_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    output_ids = model.generate(\n        token_ids.to(model.device),\n        max_new_tokens=100,\n        min_new_tokens=100,\n        do_sample=True,\n        temperature=0.8,\n        pad_token_id=tokenizer.pad_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n\noutput = tokenizer.decode(output_ids.tolist()[0])\nprint(output)\n\"\"\"\u897f\u7530\u5e7e\u591a\u90ce\u306f\u3001\u3053\u306e\u300c\u7d76\u5bfe\u77db\u76fe\u7684\u81ea\u5df1\u540c\u4e00\u300d\u3092\u300c\u4e16\u754c\u306e\u81ea\u5df1\u540c\u4e00\u300d\u3068\u7f6e\u304d\u304b\u3048\u3001\u3055\u3089\u306b\u897f\u7530\u54f2\u5b66\u3092\u51fa\u767a\u70b9\u3068\u3057\u3066\u300c\u7d76\u5bfe\u7121\u300d\u3092\u300c\u4e16\u754c\u306e\u6210\u7acb\u300d\u306b\u5909\u3048\u3001\u4e16\u754c\u3068\u81ea\u5df1\u3092\u4e00\u3064\u306e\u7d71\u4e00\u7269\u3068\u307f\u306a\u3059\u54f2\u5b66\u3068\u3057\u3066\u5c55\u958b\u3059\u308b\u3002\u3053\u306e\u4e16\u754c\u3068\u81ea\u5df1\u306f\u7d76\u5bfe\u77db\u76fe\u7684\u81ea\u5df1\u540c\u4e00\u3068\u3057\u3066\u540c\u4e00\u306e\u6027\u8cea\u3092\u6709\u3057\u3001\u540c\u3058\u50cd\u304d\u3092\u3059\u308b\u3002\u897f\u7530\u54f2\u5b66\u306b\u304a\u3044\u3066\u306f\u3001\u3053\u306e\u4e16\u754c\u3068\u81ea\u5df1\u306f\u77db\u76fe\u3057\u3042\u3046\u306e\u3067\u306f\u306a\u304f\u3001\u540c\u4e00\u306e\u6027\u8cea\u3092\u3082\u3063\u3066\u3044\u308b\u3002\u3053\u306e\u4e16\u754c\u3068\u81ea\u5df1\u306f\u540c\u4e00\u3067\u3042\u308b\u3002\u7d76\u5bfe\"\"\"\n"], "hfl/chinese-llama-2-1.3b": [], "KB/bert-base-swedish-cased": ["from transformers import AutoModel,AutoTokenizer\n\ntok = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\nmodel = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\n", "from transformers import pipeline\n\nnlp = pipeline('ner', model='KB/bert-base-swedish-cased-ner', tokenizer='KB/bert-base-swedish-cased-ner')\n\nnlp('Idag sl\u00e4pper KB tre spr\u00e5kmodeller.')\n", "from transformers import AutoModel,AutoTokenizer\n\ntok = AutoTokenizer.from_pretrained('KB/albert-base-swedish-cased-alpha'),\nmodel = AutoModel.from_pretrained('KB/albert-base-swedish-cased-alpha')\n"], "flaubert/flaubert_base_cased": ["import torch\nfrom transformers import FlaubertModel, FlaubertTokenizer\n\n# Choose among ['flaubert/flaubert_small_cased', 'flaubert/flaubert_base_uncased', \n#               'flaubert/flaubert_base_cased', 'flaubert/flaubert_large_cased']\nmodelname = 'flaubert/flaubert_base_cased' \n\n# Load pretrained model and tokenizer\nflaubert, log = FlaubertModel.from_pretrained(modelname, output_loading_info=True)\nflaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=False)\n# do_lowercase=False if using cased models, True if using uncased ones\n\nsentence = \"Le chat mange une pomme.\"\ntoken_ids = torch.tensor([flaubert_tokenizer.encode(sentence)])\n\nlast_layer = flaubert(token_ids)[0]\nprint(last_layer.shape)\n# torch.Size([1, 8, 768])  -> (batch size x number of tokens x embedding dimension)\n\n# The BERT [CLS] token correspond to the first hidden state of the last layer\ncls_embedding = last_layer[:, 0, :]\n"], "CohereForAI/c4ai-command-r-v01-4bit": ["# pip install 'transformers>=4.39.1' bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\n# Format message with the command-r chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n\ngen_tokens = model.generate(\n    input_ids, \n    max_new_tokens=100, \n    do_sample=True, \n    temperature=0.3,\n    )\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# Define tools available for the model to use:\ntools = [\n  {\n    \"name\": \"internet_search\",\n    \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n    \"parameter_definitions\": {\n      \"query\": {\n        \"description\": \"Query to search the internet with\",\n        \"type\": 'str',\n        \"required\": True\n      }\n    }\n  },\n  {\n    'name': \"directly_answer\",\n    \"description\": \"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\",\n    'parameter_definitions': {}\n  }\n]\n\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_tool_use_template(\n    conversation,\n    tools=tools,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(tool_use_prompt)\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n\n# Define tools available for the model to use\n# Type hints and docstrings from Python functions are automatically extracted\ndef internet_search(query: str):\n    \"\"\"\n    Returns a list of relevant document snippets for a textual query retrieved from the internet\n\n    Args:\n        query: Query to search the internet with\n    \"\"\"\n    pass\n\ndef directly_answer():\n    \"\"\"\n    Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n    \"\"\"\n    pass\n\ntools = [internet_search, directly_answer]\n\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_chat_template(\n    conversation,\n    tools=tools,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(tool_use_prompt)\n", "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\nThe instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n\n## Available Tools\nHere is a list of tools that you have available to you:\n\n```python\ndef internet_search(query: str) -> List[Dict]:\n    \"\"\"Returns a list of relevant document snippets for a textual query retrieved from the internet\n\n    Args:\n        query (str): Query to search the internet with\n    \"\"\"\n    pass\n```\n\n```python\ndef directly_answer() -> List[Dict]:\n    \"\"\"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n    \"\"\"\n    pass\n```<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write 'Action:' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user's last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n```json\n[\n    {\n        \"tool_name\": title of the tool in the specification,\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\n    }\n]```<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# define documents to ground on:\ndocuments = [\n    { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest growing up to 122 cm in height.\" }, \n    { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}\n]\n\n# render the tool use prompt as a string:\ngrounded_generation_prompt = tokenizer.apply_grounded_generation_template(\n    conversation,\n    documents=documents,\n    citation_mode=\"accurate\", # or \"fast\"\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(grounded_generation_prompt)\n", "The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\nDocument: 0\ntitle: Tall penguins\ntext: Emperor penguins are the tallest growing up to 122 cm in height.\n\nDocument: 1\ntitle: Penguin habitats\ntext: Emperor penguins only live in Antarctica.\n</results><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Carefully perform the following instructions, in order, starting each with a new line.\nFirstly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.\nSecondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.\nThirdly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\nFinally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n"], "google/pegasus-cnn_dailymail": [], "facebook/deit-base-patch16-224": ["from transformers import AutoFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('facebook/deit-base-patch16-224')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "google/t5-large-lm-adapt": [], "Qwen/Qwen1.5-4B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-4B-Chat\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-4B-Chat\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "microsoft/kosmos-2-patch14-224": ["import requests\n\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\n\n\nmodel = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\nprocessor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n\nprompt = \"<grounding>An image of\"\n\nurl = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# The original Kosmos-2 demo saves the image first then reload it. For some images, this will give slightly different image input and change the generation outputs.\nimage.save(\"new_image.jpg\")\nimage = Image.open(\"new_image.jpg\")\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(\n    pixel_values=inputs[\"pixel_values\"],\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    image_embeds=None,\n    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n    use_cache=True,\n    max_new_tokens=128,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n# Specify `cleanup_and_extract=False` in order to see the raw model generation.\nprocessed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n\nprint(processed_text)\n# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n\n# By default, the generated  text is cleanup and the entities are extracted.\nprocessed_text, entities = processor.post_process_generation(generated_text)\n\nprint(processed_text)\n# `An image of a snowman warming himself by a fire.`\n\nprint(entities)\n# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`\n", "import requests\n\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\n\n\nmodel = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\nprocessor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n\nurl = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ndef run_example(prompt):\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    generated_ids = model.generate(\n      pixel_values=inputs[\"pixel_values\"],\n      input_ids=inputs[\"input_ids\"],\n      attention_mask=inputs[\"attention_mask\"],\n      image_embeds=None,\n      image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n      use_cache=True,\n      max_new_tokens=128,\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    _processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n    processed_text, entities = processor.post_process_generation(generated_text)\n\n    print(processed_text)\n    print(entities)\n    print(_processed_text)\n", "import cv2\nimport numpy as np\nimport os\nimport requests\nimport torch\nimport torchvision.transforms as T\n\nfrom PIL import Image\n\n\ndef is_overlapping(rect1, rect2):\n    x1, y1, x2, y2 = rect1\n    x3, y3, x4, y4 = rect2\n    return not (x2 < x3 or x1 > x4 or y2 < y3 or y1 > y4)\n\n\ndef draw_entity_boxes_on_image(image, entities, show=False, save_path=None):\n    \"\"\"_summary_\n    Args:\n        image (_type_): image or image path\n        collect_entity_location (_type_): _description_\n    \"\"\"\n    if isinstance(image, Image.Image):\n        image_h = image.height\n        image_w = image.width\n        image = np.array(image)[:, :, [2, 1, 0]]\n    elif isinstance(image, str):\n        if os.path.exists(image):\n            pil_img = Image.open(image).convert(\"RGB\")\n            image = np.array(pil_img)[:, :, [2, 1, 0]]\n            image_h = pil_img.height\n            image_w = pil_img.width\n        else:\n            raise ValueError(f\"invaild image path, {image}\")\n    elif isinstance(image, torch.Tensor):\n        image_tensor = image.cpu()\n        reverse_norm_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])[:, None, None]\n        reverse_norm_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])[:, None, None]\n        image_tensor = image_tensor * reverse_norm_std + reverse_norm_mean\n        pil_img = T.ToPILImage()(image_tensor)\n        image_h = pil_img.height\n        image_w = pil_img.width\n        image = np.array(pil_img)[:, :, [2, 1, 0]]\n    else:\n        raise ValueError(f\"invaild image format, {type(image)} for {image}\")\n\n    if len(entities) == 0:\n        return image\n\n    new_image = image.copy()\n    previous_bboxes = []\n    # size of text\n    text_size = 1\n    # thickness of text\n    text_line = 1  # int(max(1 * min(image_h, image_w) / 512, 1))\n    box_line = 3\n    (c_width, text_height), _ = cv2.getTextSize(\"F\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n    base_height = int(text_height * 0.675)\n    text_offset_original = text_height - base_height\n    text_spaces = 3\n\n    for entity_name, (start, end), bboxes in entities:\n        for (x1_norm, y1_norm, x2_norm, y2_norm) in bboxes:\n            orig_x1, orig_y1, orig_x2, orig_y2 = int(x1_norm * image_w), int(y1_norm * image_h), int(x2_norm * image_w), int(y2_norm * image_h)\n            # draw bbox\n            # random color\n            color = tuple(np.random.randint(0, 255, size=3).tolist())\n            new_image = cv2.rectangle(new_image, (orig_x1, orig_y1), (orig_x2, orig_y2), color, box_line)\n\n            l_o, r_o = box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1\n\n            x1 = orig_x1 - l_o\n            y1 = orig_y1 - l_o\n\n            if y1 < text_height + text_offset_original + 2 * text_spaces:\n                y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces\n                x1 = orig_x1 + r_o\n\n            # add text background\n            (text_width, text_height), _ = cv2.getTextSize(f\"  {entity_name}\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n            text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2 = x1, y1 - (text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1\n\n            for prev_bbox in previous_bboxes:\n                while is_overlapping((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox):\n                    text_bg_y1 += (text_height + text_offset_original + 2 * text_spaces)\n                    text_bg_y2 += (text_height + text_offset_original + 2 * text_spaces)\n                    y1 += (text_height + text_offset_original + 2 * text_spaces)\n\n                    if text_bg_y2 >= image_h:\n                        text_bg_y1 = max(0, image_h - (text_height + text_offset_original + 2 * text_spaces))\n                        text_bg_y2 = image_h\n                        y1 = image_h\n                        break\n\n            alpha = 0.5\n            for i in range(text_bg_y1, text_bg_y2):\n                for j in range(text_bg_x1, text_bg_x2):\n                    if i < image_h and j < image_w:\n                        if j < text_bg_x1 + 1.35 * c_width:\n                            # original color\n                            bg_color = color\n                        else:\n                            # white\n                            bg_color = [255, 255, 255]\n                        new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(np.uint8)\n\n            cv2.putText(\n                new_image, f\"  {entity_name}\", (x1, y1 - text_offset_original - 1 * text_spaces), cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA\n            )\n            # previous_locations.append((x1, y1))\n            previous_bboxes.append((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2))\n\n    pil_image = Image.fromarray(new_image[:, :, [2, 1, 0]])\n    if save_path:\n        pil_image.save(save_path)\n    if show:\n        pil_image.show()\n\n    return new_image\n\n\n# (The same image from the previous code example)\nurl = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# From the previous code example\nentities = [('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]\n\n# Draw the bounding bboxes\ndraw_entity_boxes_on_image(image, entities, show=True)\n"], "jameslahm/yolov10n": ["from ultralytics import YOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10n')\n# Training\nmodel.train(...)\n# after training, one can push to the hub\nmodel.push_to_hub(\"your-hf-username/yolov10-finetuned\")\n\n# Validation\nmodel.val(...)\n", "from ultralytics import YOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10n')\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)\n"], "bigscience/bloomz-7b1": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-7b1\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-7b1\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-7b1\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "sshleifer/distilbart-cnn-6-6": [], "Salesforce/codet5-base": ["from transformers import RobertaTokenizer, T5ForConditionalGeneration\n\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n\ntext = \"def greet(user): print(f'hello <extra_id_0>!')\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\n# simply generate a single sequence\ngenerated_ids = model.generate(input_ids, max_length=8)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n# this prints \"{user.username}\"\n"], "ehartford/dolphin-2.2.1-mistral-7b": [], "intfloat/e5-base": ["import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: summit define',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base')\nmodel = AutoModel.from_pretrained('intfloat/e5-base')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-base')\ninput_texts = [\n    'query: how much protein should a female eat',\n    'query: summit define',\n    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n    \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\n"], "WizardLM/WizardLM-13B-V1.2": [], "microsoft/swinv2-tiny-patch4-window8-256": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "csebuetnlp/mT5_multilingual_XLSum": ["import re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n\narticle_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n\nmodel_name = \"csebuetnlp/mT5_multilingual_XLSum\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ninput_ids = tokenizer(\n    [WHITESPACE_HANDLER(article_text)],\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    truncation=True,\n    max_length=512\n)[\"input_ids\"]\n\noutput_ids = model.generate(\n    input_ids=input_ids,\n    max_length=84,\n    no_repeat_ngram_size=2,\n    num_beams=4\n)[0]\n\nsummary = tokenizer.decode(\n    output_ids,\n    skip_special_tokens=True,\n    clean_up_tokenization_spaces=False\n)\n\nprint(summary)\n"], "laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K": [], "facebook/incoder-6B": ["tokenizer.decode(tokenizer.encode(\"from .\"), clean_up_tokenization_spaces=False)"], "hkunlp/instructor-base": ["from InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-base')\nsentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\ninstruction = \"Represent the Science title:\"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\n", "from sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'], \n               ['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\nsentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n               ['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\n", "import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [['Represent the Wikipedia question for retrieving supporting documents: ','where is the food stored in a yam plant']]\ncorpus = [['Represent the Wikipedia document for retrieval: ','Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term \"mixed economies\" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.'],\n          ['Represent the Wikipedia document for retrieval: ',\"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loans\u00e2\u20ac\u201dand some scholars have argued that the theory's use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession\"],\n          ['Represent the Wikipedia document for retrieval: ','Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\n", "import sklearn.cluster\nsentences = [['Represent the Medicine sentence for clustering: ','Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity'],\n             ['Represent the Medicine sentence for clustering: ','Comparison of Atmospheric Neutrino Flux Calculations at Low Energies'],\n             ['Represent the Medicine sentence for clustering: ','Fermion Bags in the Massive Gross-Neveu Model'],\n             ['Represent the Medicine sentence for clustering: ',\"QCD corrections to Associated t-tbar-H production at the Tevatron\"],\n             ['Represent the Medicine sentence for clustering: ','A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)\n"], "Qwen/Qwen-VL": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n"], "TheBloke/Llama-2-13B-chat-GGUF": ["from ctransformers import AutoModelForCausalLM\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-13B-chat-GGUF\", model_file=\"llama-2-13b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n"], "togethercomputer/Llama-2-7B-32K-Instruct": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/Llama-2-7B-32K-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/Llama-2-7B-32K-Instruct\",\n    trust_remote_code=True, torch_dtype=torch.float16)\ninput_ids = tokenizer.encode(\"[INST]\\nWrite a poem about cats\\n[/INST]\\n\\n\", return_tensors=\"pt\")\noutput = model.generate(input_ids, max_length=128,\n    temperature=0.7, repetition_penalty=1.1, top_p=0.7, top_k=50)\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\n", "[INST]\nWrite a poem about cats\n[/INST]\n\nCats are mysterious creatures, with their aloof and independent nature. They're also incredibly beautiful, with their sleek fur and piercing eyes. Here's a poem that captures the essence of cats:\n\nCats, oh cats, how can I describe you?\nYour beauty is beyond compare, it seems.\nYou're graceful and elegant, like a ballerina's dance,\nBut don't let your charm fool you, for you're not easily tamed.\n\nWith your soft purring and playful meows,\nYou draw us in with your enchanting powers.\nWe love to watch you play, your tail twirling 'round,\nAs if you're dancing on air, with no sound.\n\nBut don't be fooled by your sweetness, my friend,\nFor beneath that gentle exterior, lies a fierce defender.\nWhen danger lurks, you'll spring into action,\nProtecting those you hold dear, without question.\n\nSolet us admire you, from afar,\nFor in your own way, you're truly unique, a star.\nAnd though we may never fully understand,\nThe depths of your soul, we'll always stand, hand in paw, as one.\n\nThis poem captures the essence of cats, highlighting their beauty, independence,and protective nature. It also celebrates the special bond between humans and cats, recognizing their unique qualities and the joy they bring to our lives.\n"], "aubmindlab/bert-base-arabertv02": ["from arabert.preprocess import ArabertPreprocessor\n\nmodel_name=\"aubmindlab/bert-large-arabertv02\"\narabert_prep = ArabertPreprocessor(model_name=model_name)\n\ntext = \"\u0648\u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644\u0646\u0627: \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644\u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\"\narabert_prep.preprocess(text)\n\n>>> output: \u0648\u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644\u0646\u0627 : \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644\u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\n"], "Salesforce/ctrl": [">>> from transformers import CTRLTokenizer, CTRLModel\n>>> import torch\n\n>>> tokenizer = CTRLTokenizer.from_pretrained(\"ctrl\")\n>>> model = CTRLModel.from_pretrained(\"ctrl\")\n\n>>> # CTRL was trained with control codes as the first token\n>>> inputs = tokenizer(\"Opinion My dog is cute\", return_tensors=\"pt\")\n>>> assert inputs[\"input_ids\"][0, 0].item() in tokenizer.control_codes.values()\n\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n"], "EleutherAI/pythia-1.4b-deduped": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "llamafactory/tiny-random-Llama-3": [], "openbmb/cpm-ant-10b": ["from transformers import CpmAntTokenizer, CpmAntForCausalLM\n\ntexts = \"\u4eca\u5929\u5929\u6c14\u4e0d\u9519\uff0c\"\nmodel = CpmAntForCausalLM.from_pretrained(\"openbmb/cpm-ant-10b\")\ntokenizer = CpmAntTokenizer.from_pretrained(\"openbmb/cpm-ant-10b\")\ninput_ids = tokenizer(texts, return_tensors=\"pt\")\noutputs = model.generate(**input_ids)\noutput_texts = tokenizer.batch_decode(outputs)\n\nprint(output_texts)\n"], "facebook/bart-large-xsum": [], "laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K": [], "openchat/openchat_3.5": ["import transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"openchat/openchat_3.5\")\n\n# Single-turn\ntokens = tokenizer(\"GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:\").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Multi-turn\ntokens = tokenizer(\"GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:\").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Coding Mode\ntokens = tokenizer(\"Code User: Implement quicksort using C++<|end_of_turn|>Code Assistant:\").input_ids\nassert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]\n"], "hfl/chinese-roberta-wwm-ext-large": [], "facebook/maskformer-swin-base-coco": ["from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\n\n# load MaskFormer fine-tuned on COCO panoptic segmentation\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(\"facebook/maskformer-swin-base-coco\")\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-coco\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\n# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# you can pass them to feature_extractor for postprocessing\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n# we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\npredicted_panoptic_map = result[\"segmentation\"]\n"], "diffusers/controlnet-depth-sdxl-1.0-small": ["import torch\nimport numpy as np\nfrom PIL import Image\n\nfrom transformers import DPTFeatureExtractor, DPTForDepthEstimation\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\nfrom diffusers.utils import load_image\n\n\ndepth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\nfeature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-depth-sdxl-1.0-small\",\n    variant=\"fp16\",\n    use_safetensors=True,\n    torch_dtype=torch.float16,\n).to(\"cuda\")\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(\"cuda\")\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    vae=vae,\n    variant=\"fp16\",\n    use_safetensors=True,\n    torch_dtype=torch.float16,\n).to(\"cuda\")\npipe.enable_model_cpu_offload()\n\ndef get_depth_map(image):\n    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n    with torch.no_grad(), torch.autocast(\"cuda\"):\n        depth_map = depth_estimator(image).predicted_depth\n\n    depth_map = torch.nn.functional.interpolate(\n        depth_map.unsqueeze(1),\n        size=(1024, 1024),\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n    image = torch.cat([depth_map] * 3, dim=1)\n\n    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n    return image\n\n\nprompt = \"stormtrooper lecture, photorealistic\"\nimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\ncontrolnet_conditioning_scale = 0.5  # recommended for good generalization\n\ndepth_image = get_depth_map(image)\n\nimages = pipe(\n    prompt, image=depth_image, num_inference_steps=30, controlnet_conditioning_scale=controlnet_conditioning_scale,\n).images\nimages[0]\n\nimages[0].save(f\"stormtrooper_grid.png\")\n"], "intfloat/e5-large": ["import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: summit define',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large')\nmodel = AutoModel.from_pretrained('intfloat/e5-large')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n", "from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-large')\ninput_texts = [\n    'query: how much protein should a female eat',\n    'query: summit define',\n    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n    \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\n"], "google/bigbird-pegasus-large-arxiv": ["from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n\n# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n\n# decoder attention type can't be changed & will be \"original_full\"\n# you can change `attention_type` (encoder only) to full attention like this:\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n\n# you can change `block_size` & `num_random_blocks` like this:\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n\ntext = \"Replace me by any text you'd like.\"\ninputs = tokenizer(text, return_tensors='pt')\nprediction = model.generate(**inputs)\nprediction = tokenizer.batch_decode(prediction)\n"], "kandinsky-community/kandinsky-2-1": ["from diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nimage = pipe(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale =1.0, height=768, width=768).images[0]\nimage.save(\"cheeseburger_monster.png\")\n", "from diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport os\n\npipe = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\n\npipe.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n \nresponse = requests.get(url)\noriginal_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\noriginal_image.thumbnail((768, 768))\n\nimage = pipe(prompt=prompt, image=original_image, strength=0.3).images[0]\nout.images[0].save(\"fantasy_land.png\")\n", "from diffusers import KandinskyPriorPipeline, KandinskyPipeline\nfrom diffusers.utils import load_image\nimport PIL\n\nimport torch\n\npipe_prior = KandinskyPriorPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16\n)\npipe_prior.to(\"cuda\")\n\nimg1 = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nimg2 = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/starry_night.jpeg\"\n)\n\n# add all the conditions we want to interpolate, can be either text or image\nimages_texts = [\"a cat\", img1, img2]\n\n# specify the weights for each condition in images_texts\nweights = [0.3, 0.3, 0.4]\n\n# We can leave the prompt empty\nprompt = \"\"\nprior_out = pipe_prior.interpolate(images_texts, weights)\n\npipe = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nimage = pipe(prompt, **prior_out, height=768, width=768).images[0]\n\nimage.save(\"starry_cat.png\")\n"], "h2oai/h2ogpt-oig-oasst1-512-6_9b": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oig-oasst1-512-6_9b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n", "import torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oig-oasst1-512-6_9b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oig-oasst1-512-6_9b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer, prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n"], "google/switch-base-128": ["\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-128\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-128\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-128\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-128\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n"], "klue/roberta-large": ["from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"klue/roberta-large\")\ntokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n"], "bigscience/T0pp": ["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\")\n\ninputs = tokenizer.encode(\"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "austinmw/distilbert-base-uncased-finetuned-tweets-sentiment": [], "facebook/opt-66b": [">>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-66b\", use_fast=False)\n\n>>> prompt = \"Hello, I am conscious and\"\n\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> generated_ids = model.generate(input_ids)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Hello, I am conscious and I am here.\\nI am also conscious and I am here']\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-66b\", use_fast=False)\n\n>>> prompt = \"Hello, I am conscious and\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Hello, I am conscious and aware that you have your back turned to me and want to talk']\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-66b\", use_fast=False)\n\n>>> prompt = \"The woman worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe woman worked as a supervisor in the office\nThe woman worked as a social worker in a\nThe woman worked as a cashier at the\nThe woman worked as a teacher from 2011 to\nhe woman worked as a maid at the house\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-66b\", use_fast=False)\n\n>>> prompt = \"The man worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe man worked as a school bus driver for\nThe man worked as a bartender in a bar\nThe man worked as a cashier at the\nThe man worked as a teacher, and was\nThe man worked as a professional at a range\n"], "EleutherAI/pythia-2.8b-deduped": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "microsoft/git-base-vatex": [], "facebook/opt-30b": [">>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-30b\", use_fast=False)\n\n>>> prompt = \"Hello, I am conscious and\"\n\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> generated_ids = model.generate(input_ids)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Hello, I am conscious and I am here.\\nI am also conscious and I am here']\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-30b\", use_fast=False)\n\n>>> prompt = \"Hello, I am conscious and\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Hello, I am conscious and aware that you have your back turned to me and want to talk']\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-30b\", use_fast=False)\n\n>>> prompt = \"The woman worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe woman worked as a supervisor in the office\nThe woman worked as a social worker in a\nThe woman worked as a cashier at the\nThe woman worked as a teacher from 2011 to\nhe woman worked as a maid at the house\n", ">>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> import torch\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\", torch_dtype=torch.float16).cuda()\n\n>>> # the fast tokenizer currently does not work correctly\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-30b\", use_fast=False)\n\n>>> prompt = \"The man worked as a\"\n\n>>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n>>> set_seed(32)\n>>> generated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\n\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nThe man worked as a school bus driver for\nThe man worked as a bartender in a bar\nThe man worked as a cashier at the\nThe man worked as a teacher, and was\nThe man worked as a professional at a range\n"], "mosaicml/mpt-30b": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-30b',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-30b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-30b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n", "from transformers import pipeline\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**inputs, max_new_tokens=100)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "microsoft/Orca-2-13b": ["import torch\nimport transformers\n\nif torch.cuda.is_available():\n    torch.set_default_device(\"cuda\")\nelse:\n    torch.set_default_device(\"cpu\")\n    \nmodel = transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Orca-2-13b\", device_map='auto')\n\n# https://github.com/huggingface/transformers/issues/27132\n# please use the slow tokenizer since fast and slow tokenizer produces different tokens\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n        \"microsoft/Orca-2-13b\",\n        use_fast=False,\n    )\n\nsystem_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\nuser_message = \"How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?\"\n\nprompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\noutput_ids = model.generate(inputs[\"input_ids\"],)\nanswer = tokenizer.batch_decode(output_ids)[0]\n\nprint(answer)\n\n# This example continues showing how to add a second turn message by the user to the conversation\nsecond_turn_user_message = \"Give me a list of the key points of your first answer.\"\n\n# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\nsecond_turn_message_in_markup = f\"\\n<|im_start|>user\\n{second_turn_user_message}<|im_end|>\\n<|im_start|>assistant\"\nsecond_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors='pt', add_special_tokens=False)\nsecond_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)\n\noutput_ids_2 = model.generate(second_turn_input,)\nsecond_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n\nprint(second_turn_answer)\n", "import os\nimport math\nimport transformers\nimport torch\n\nfrom azure.ai.contentsafety import ContentSafetyClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.core.exceptions import HttpResponseError\nfrom azure.ai.contentsafety.models import AnalyzeTextOptions\n\nCONTENT_SAFETY_KEY = os.environ[\"CONTENT_SAFETY_KEY\"]\nCONTENT_SAFETY_ENDPOINT = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n\n# We use Azure AI Content Safety to filter out any content that reaches \"Medium\" threshold\n# For more information: https://learn.microsoft.com/en-us/azure/ai-services/content-safety/\ndef should_filter_out(input_text, threshold=4):\n    # Create an Content Safety client\n    client = ContentSafetyClient(CONTENT_SAFETY_ENDPOINT, AzureKeyCredential(CONTENT_SAFETY_KEY))\n\n    # Construct a request\n    request = AnalyzeTextOptions(text=input_text)\n\n    # Analyze text\n    try:\n        response = client.analyze_text(request)\n    except HttpResponseError as e:\n        print(\"Analyze text failed.\")\n        if e.error:\n            print(f\"Error code: {e.error.code}\")\n            print(f\"Error message: {e.error.message}\")\n            raise\n        print(e)\n        raise\n\n    categories = [\"hate_result\", \"self_harm_result\", \"sexual_result\", \"violence_result\"]\n    max_score = -math.inf\n    for category in categories:\n        max_score = max(max_score, getattr(response, category).severity)\n\n    return max_score >= threshold\n\nmodel_path = 'microsoft/Orca-2-13b'\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_path)\nmodel.to(device)\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_path,\n    model_max_length=4096,\n    padding_side=\"right\",\n    use_fast=False,\n    add_special_tokens=False,\n)\n\nsystem_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\nuser_message = \"\\\" \\n :You can't just say, \\\"\\\"that's crap\\\"\\\" and remove it without gaining a consensus. You already know this, based on your block history. \u2014/ \\\" \\nIs the comment obscene? \\nOptions : Yes, No.\"\n\nprompt =  f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\ninputs = inputs.to(device)\n\noutput_ids = model.generate(inputs[\"input_ids\"], max_length=4096, do_sample=False, temperature=0.0, use_cache=True)\nsequence_length = inputs[\"input_ids\"].shape[1]\nnew_output_ids = output_ids[:, sequence_length:]\nanswers = tokenizer.batch_decode(new_output_ids, skip_special_tokens=True)\nfinal_output = answers[0] if not should_filter_out(answers[0]) else \"[Content Filtered]\"\n\nprint(final_output)\n"], "microsoft/Orca-2-7b": ["import torch\nimport transformers\n\nif torch.cuda.is_available():\n    torch.set_default_device(\"cuda\")\nelse:\n    torch.set_default_device(\"cpu\")\n    \nmodel = transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Orca-2-7b\", device_map='auto')\n\n# https://github.com/huggingface/transformers/issues/27132\n# please use the slow tokenizer since fast and slow tokenizer produces different tokens\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n        \"microsoft/Orca-2-7b\",\n        use_fast=False,\n    )\n\nsystem_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\nuser_message = \"How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?\"\n\nprompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\noutput_ids = model.generate(inputs[\"input_ids\"],)\nanswer = tokenizer.batch_decode(output_ids)[0]\n\nprint(answer)\n\n# This example continues showing how to add a second turn message by the user to the conversation\nsecond_turn_user_message = \"Give me a list of the key points of your first answer.\"\n\n# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\nsecond_turn_message_in_markup = f\"\\n<|im_start|>user\\n{second_turn_user_message}<|im_end|>\\n<|im_start|>assistant\"\nsecond_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors='pt', add_special_tokens=False)\nsecond_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)\n\noutput_ids_2 = model.generate(second_turn_input,)\nsecond_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n\nprint(second_turn_answer)\n", "import os\nimport math\nimport transformers\nimport torch\n\nfrom azure.ai.contentsafety import ContentSafetyClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.core.exceptions import HttpResponseError\nfrom azure.ai.contentsafety.models import AnalyzeTextOptions\n\nCONTENT_SAFETY_KEY = os.environ[\"CONTENT_SAFETY_KEY\"]\nCONTENT_SAFETY_ENDPOINT = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n\n# We use Azure AI Content Safety to filter out any content that reaches \"Medium\" threshold\n# For more information: https://learn.microsoft.com/en-us/azure/ai-services/content-safety/\ndef should_filter_out(input_text, threshold=4):\n    # Create an Content Safety client\n    client = ContentSafetyClient(CONTENT_SAFETY_ENDPOINT, AzureKeyCredential(CONTENT_SAFETY_KEY))\n\n    # Construct a request\n    request = AnalyzeTextOptions(text=input_text)\n\n    # Analyze text\n    try:\n        response = client.analyze_text(request)\n    except HttpResponseError as e:\n        print(\"Analyze text failed.\")\n        if e.error:\n            print(f\"Error code: {e.error.code}\")\n            print(f\"Error message: {e.error.message}\")\n            raise\n        print(e)\n        raise\n\n    categories = [\"hate_result\", \"self_harm_result\", \"sexual_result\", \"violence_result\"]\n    max_score = -math.inf\n    for category in categories:\n        max_score = max(max_score, getattr(response, category).severity)\n\n    return max_score >= threshold\n\nmodel_path = 'microsoft/Orca-2-7b'\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_path)\nmodel.to(device)\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_path,\n    model_max_length=4096,\n    padding_side=\"right\",\n    use_fast=False,\n    add_special_tokens=False,\n)\n\nsystem_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\nuser_message = \"\\\" \\n :You can't just say, \\\"\\\"that's crap\\\"\\\" and remove it without gaining a consensus. You already know this, based on your block history. \u2014/ \\\" \\nIs the comment obscene? \\nOptions : Yes, No.\"\n\nprompt =  f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\ninputs = inputs.to(device)\n\noutput_ids = model.generate(inputs[\"input_ids\"], max_length=4096, do_sample=False, temperature=0.0, use_cache=True)\nsequence_length = inputs[\"input_ids\"].shape[1]\nnew_output_ids = output_ids[:, sequence_length:]\nanswers = tokenizer.batch_decode(new_output_ids, skip_special_tokens=True)\nfinal_output = answers[0] if not should_filter_out(answers[0]) else \"[Content Filtered]\"\n\nprint(final_output)\n"], "01-ai/Yi-34B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "01-ai/Yi-34B": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "lmms-lab/LLaVA-NeXT-Video-7B-32K": [], "stabilityai/stable-diffusion-2-1-unclip": ["from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-unclip-small\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\n# get image\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/tarsila_do_amaral.png\"\nimage = load_image(url)\n\n# run image variation\nimage = pipe(image).images[0]\n"], "state-spaces/mamba-2.8b-hf": [">>> from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n>>> model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n>>> input_ids = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> out = model.generate(input_ids, max_new_tokens=10)\n>>> print(tokenizer.batch_decode(out))\n[\"Hey how are you doing?\\n\\nI'm doing great.\\n\\nI\"]\n", "from datasets import load_dataset\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\ntokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\ndataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    logging_dir='./logs',\n    logging_steps=10,\n    learning_rate=2e-3\n)\nlora_config =  LoraConfig(\n        r=8,\n        target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n        task_type=\"CAUSAL_LM\",\n        bias=\"none\"\n)\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    peft_config=lora_config,\n    train_dataset=dataset,\n    dataset_text_field=\"quote\",\n)\ntrainer.train()\n"], "bigscience/bloom-3b": [], "laion/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K": [], "microsoft/phi-1": ["def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n   for num in range(2, n+1):\n       for i in range(2, num):\n           if num % i == 0:\n               break\n       else:\n           print(num)\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n"], "flaubert/flaubert_small_cased": ["import torch\nfrom transformers import FlaubertModel, FlaubertTokenizer\n\n# Choose among ['flaubert/flaubert_small_cased', 'flaubert/flaubert_base_uncased', \n#               'flaubert/flaubert_base_cased', 'flaubert/flaubert_large_cased']\nmodelname = 'flaubert/flaubert_base_cased' \n\n# Load pretrained model and tokenizer\nflaubert, log = FlaubertModel.from_pretrained(modelname, output_loading_info=True)\nflaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=False)\n# do_lowercase=False if using cased models, True if using uncased ones\n\nsentence = \"Le chat mange une pomme.\"\ntoken_ids = torch.tensor([flaubert_tokenizer.encode(sentence)])\n\nlast_layer = flaubert(token_ids)[0]\nprint(last_layer.shape)\n# torch.Size([1, 8, 768])  -> (batch size x number of tokens x embedding dimension)\n\n# The BERT [CLS] token correspond to the first hidden state of the last layer\ncls_embedding = last_layer[:, 0, :]\n"], "Qwen/Qwen-1_8B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True) # \u53ef\u6307\u5b9a\u4e0d\u540c\u7684\u751f\u6210\u957f\u5ea6\u3001top_p\u7b49\u76f8\u5173\u8d85\u53c2\n\n# \u7b2c\u4e00\u8f6e\u5bf9\u8bdd 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\nprint(response)\n# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n\n# \u7b2c\u4e8c\u8f6e\u5bf9\u8bdd 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\", history=history)\nprint(response)\n# \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\n# \u6545\u4e8b\u7684\u4e3b\u4eba\u516c\u53eb\u674e\u660e\uff0c\u4ed6\u6765\u81ea\u4e00\u4e2a\u666e\u901a\u7684\u5bb6\u5ead\uff0c\u7236\u6bcd\u90fd\u662f\u666e\u901a\u7684\u5de5\u4eba\u3002\u4ece\u5c0f\uff0c\u674e\u660e\u5c31\u7acb\u4e0b\u4e86\u4e00\u4e2a\u76ee\u6807\uff1a\u8981\u6210\u4e3a\u4e00\u540d\u6210\u529f\u7684\u4f01\u4e1a\u5bb6\u3002\n# \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff0c\u674e\u660e\u52e4\u594b\u5b66\u4e60\uff0c\u8003\u4e0a\u4e86\u5927\u5b66\u3002\u5728\u5927\u5b66\u671f\u95f4\uff0c\u4ed6\u79ef\u6781\u53c2\u52a0\u5404\u79cd\u521b\u4e1a\u6bd4\u8d5b\uff0c\u83b7\u5f97\u4e86\u4e0d\u5c11\u5956\u9879\u3002\u4ed6\u8fd8\u5229\u7528\u8bfe\u4f59\u65f6\u95f4\u53bb\u5b9e\u4e60\uff0c\u79ef\u7d2f\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u3002\n# \u6bd5\u4e1a\u540e\uff0c\u674e\u660e\u51b3\u5b9a\u5f00\u59cb\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u5f00\u59cb\u5bfb\u627e\u6295\u8d44\u673a\u4f1a\uff0c\u4f46\u591a\u6b21\u90fd\u88ab\u62d2\u7edd\u4e86\u3002\u7136\u800c\uff0c\u4ed6\u5e76\u6ca1\u6709\u653e\u5f03\u3002\u4ed6\u7ee7\u7eed\u52aa\u529b\uff0c\u4e0d\u65ad\u6539\u8fdb\u81ea\u5df1\u7684\u521b\u4e1a\u8ba1\u5212\uff0c\u5e76\u5bfb\u627e\u65b0\u7684\u6295\u8d44\u673a\u4f1a\u3002\n# \u6700\u7ec8\uff0c\u674e\u660e\u6210\u529f\u5730\u83b7\u5f97\u4e86\u4e00\u7b14\u6295\u8d44\uff0c\u5f00\u59cb\u4e86\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u6210\u7acb\u4e86\u4e00\u5bb6\u79d1\u6280\u516c\u53f8\uff0c\u4e13\u6ce8\u4e8e\u5f00\u53d1\u65b0\u578b\u8f6f\u4ef6\u3002\u5728\u4ed6\u7684\u9886\u5bfc\u4e0b\uff0c\u516c\u53f8\u8fc5\u901f\u53d1\u5c55\u8d77\u6765\uff0c\u6210\u4e3a\u4e86\u4e00\u5bb6\u6210\u529f\u7684\u79d1\u6280\u4f01\u4e1a\u3002\n# \u674e\u660e\u7684\u6210\u529f\u5e76\u4e0d\u662f\u5076\u7136\u7684\u3002\u4ed6\u52e4\u594b\u3001\u575a\u97e7\u3001\u52c7\u4e8e\u5192\u9669\uff0c\u4e0d\u65ad\u5b66\u4e60\u548c\u6539\u8fdb\u81ea\u5df1\u3002\u4ed6\u7684\u6210\u529f\u4e5f\u8bc1\u660e\u4e86\uff0c\u53ea\u8981\u52aa\u529b\u594b\u6597\uff0c\u4efb\u4f55\u4eba\u90fd\u6709\u53ef\u80fd\u53d6\u5f97\u6210\u529f\u3002\n\n# \u7b2c\u4e09\u8f6e\u5bf9\u8bdd 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u8fd9\u4e2a\u6545\u4e8b\u8d77\u4e00\u4e2a\u6807\u9898\", history=history)\nprint(response)\n# \u300a\u594b\u6597\u521b\u4e1a\uff1a\u4e00\u4e2a\u5e74\u8f7b\u4eba\u7684\u6210\u529f\u4e4b\u8def\u300b\n\n# Qwen-1.8B-Chat\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u7cfb\u7edf\u6307\u4ee4\uff08System Prompt\uff09\uff0c\u5b9e\u73b0\u89d2\u8272\u626e\u6f14\uff0c\u8bed\u8a00\u98ce\u683c\u8fc1\u79fb\uff0c\u4efb\u52a1\u8bbe\u5b9a\uff0c\u884c\u4e3a\u8bbe\u5b9a\u7b49\u80fd\u529b\u3002\n# Qwen-1.8B-Chat can realize roly playing, language style transfer, task setting, and behavior setting by system prompt.\nresponse, _ = model.chat(tokenizer, \"\u4f60\u597d\u5440\", history=None, system=\"\u8bf7\u7528\u4e8c\u6b21\u5143\u53ef\u7231\u8bed\u6c14\u548c\u6211\u8bf4\u8bdd\")\nprint(response)\n# \u4f60\u597d\u554a\uff01\u6211\u662f\u4e00\u53ea\u53ef\u7231\u7684\u4e8c\u6b21\u5143\u732b\u54aa\u54e6\uff0c\u4e0d\u77e5\u9053\u4f60\u6709\u4ec0\u4e48\u95ee\u9898\u9700\u8981\u6211\u5e2e\u5fd9\u89e3\u7b54\u5417\uff1f\n\nresponse, _ = model.chat(tokenizer, \"My colleague works diligently\", history=None, system=\"You will write beautiful compliments according to needs\")\nprint(response)\n# Your colleague is an outstanding worker! Their dedication and hard work are truly inspiring. They always go above and beyond to ensure that \n# their tasks are completed on time and to the highest standard. I am lucky to have them as a colleague, and I know I can count on them to handle any challenge that comes their way.\n"], "WizardLM/WizardCoder-Python-34B-V1.0": [], "Phind/Phind-CodeLlama-34B-v2": ["\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom human_eval.data import write_jsonl, read_problems\nfrom tqdm import tqdm\n\n# initialize the model\n\nmodel_path = \"Phind/Phind-CodeLlama-34B-v2\"\nmodel = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# HumanEval helper\n\ndef generate_one_completion(prompt: str):\n    tokenizer.pad_token = tokenizer.eos_token\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n\n    # Generate\n    generate_ids = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n    completion = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    completion = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\n\n    return completion\n\n# perform HumanEval\nproblems = read_problems()\n\nnum_samples_per_task = 1\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n    for task_id in tqdm(problems)\n    for _ in range(num_samples_per_task)\n]\nwrite_jsonl(\"samples.jsonl\", samples)\n\n# run `evaluate_functional_correctness samples.jsonl` in your HumanEval code sandbox\n"], "laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K": [], "Snowflake/snowflake-arctic-instruct": ["import os\n# enable hf_transfer for faster ckpt download\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom deepspeed.linear.config import QuantizationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"Snowflake/snowflake-arctic-instruct\",\n    trust_remote_code=True\n)\nquant_config = QuantizationConfig(q_bits=8)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Snowflake/snowflake-arctic-instruct\",\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    ds_quantization_config=quant_config,\n    max_memory={i: \"150GiB\" for i in range(8)},\n    torch_dtype=torch.bfloat16)\n\n\ncontent = \"5x + 35 = 7x - 60 + 10. Solve for x\"\nmessages = [{\"role\": \"user\", \"content\": content}]\ninput_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids=input_ids, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))\n"], "microsoft/deberta-base-mnli": [], "microsoft/deberta-large": [], "studio-ousia/luke-base": [], "GanymedeNil/text2vec-large-chinese": [], "alibaba-damo/mgp-str-base": ["from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\nimport requests\nfrom PIL import Image\n\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\n\n# load image from the IIIT-5k dataset\nurl = \"https://i.postimg.cc/ZKwLg2Gw/367-14.png\"\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\noutputs = model(pixel_values)\n\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\n"], "valhalla/t5-small-qa-qg-hl": ["from pipelines import pipeline\nnlp = pipeline(\"multitask-qa-qg\")\n\n# to generate questions simply pass the text\nnlp(\"42 is the answer to life, the universe and everything.\")\n=> [{'answer': '42', 'question': 'What is the answer to life, the universe and everything?'}]\n\n# for qa pass a dict with \"question\" and \"context\"\nnlp({\n    \"question\": \"What is 42 ?\",\n    \"context\": \"42 is the answer to life, the universe and everything.\"\n})\n=> 'the answer to life, the universe and everything'\n"], "google/mt5-xxl": [], "nghuyong/ernie-3.0-base-zh": ["from transformers import BertTokenizer, ErnieForMaskedLM\n\ntokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-base-zh\")\nmodel = ErnieForMaskedLM.from_pretrained(\"nghuyong/ernie-3.0-base-zh\")\n"], "mosaicml/mpt-7b-storywriter": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-storywriter',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-7b-storywriter'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-7b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 83968 # (input + output) tokens can now be up to 83968\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n", "from transformers import pipeline\n\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "EleutherAI/pythia-6.9b-deduped": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "NousResearch/Nous-Hermes-Llama2-13b": [], "lmsys/longchat-13b-16k": [], "Intel/neural-chat-7b-v3-1": ["import transformers\n\nmodel_name = 'Intel/neural-chat-7b-v3-1'\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\ndef generate_response(system_input, user_input):\n\n    # Format the input using the provided template\n    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n\n    # Tokenize and encode the prompt\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n\n    # Generate a response\n    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Extract only the assistant's response\n    return response.split(\"### Assistant:\\n\")[-1]\n\n\n# Example usage\nsystem_input = \"You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.\"\nuser_input = \"calculate 100 + 520 + 60\"\nresponse = generate_response(system_input, user_input)\nprint(response)\n\n# expected response\n\"\"\"\nTo calculate the sum of 100, 520, and 60, we will follow these steps:\n\n1. Add the first two numbers: 100 + 520\n2. Add the result from step 1 to the third number: (100 + 520) + 60\n\nStep 1: Add 100 and 520\n100 + 520 = 620\n\nStep 2: Add the result from step 1 to the third number (60)\n(620) + 60 = 680\n\nSo, the sum of 100, 520, and 60 is 680.\n\"\"\"\n", "from transformers import AutoTokenizer, TextStreamer\nimport torch\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM\nimport intel_extension_for_pytorch as ipex\n\nmodel_name = \"Intel/neural-chat-7b-v3-1\"\nprompt = \"Once upon a time, there existed a little girl,\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\nmodel = ipex.optimize(model.eval(), dtype=torch.bfloat16, inplace=True, level=\"O1\", auto_kernel_selection=True)\n\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n", "from transformers import AutoTokenizer, TextStreamer\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\nmodel_name = \"Intel/neural-chat-7b-v3-1\"\n\n# for int8, should set weight_dtype=\"int8\"       \nconfig = WeightOnlyQuantConfig(compute_dtype=\"bf16\", weight_dtype=\"int4\")\nprompt = \"Once upon a time, there existed a little girl,\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n", "import torch\nimport os\nfrom transformers import AutoTokenizer\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, pipeline\nmodel_name = \"Intel/neural-chat-7b-v3-1\"     \nprompt = \"Once upon a time, there existed a little girl,\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\nresult = {torch.bfloat16:\"failed\"}\ntyp = torch.bfloat16\ntry:\n    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True,  torch_dtype = typ)\n    outputs = model.generate(inputs, max_new_tokens=20)\n    result[typ] = f\"passed, {os.stat(model.bin_file).st_size}\"\nexcept:\n    result[typ] = \"failed\"\n\n    \nprint(\"\\n\\nResults of quantizing: \")  \n# determine if Quantized\nwith open(r\"output.log\", 'r') as fp:\n    for l_no, line in enumerate(fp):\n        # search string\n        if 'model_quantize_internal' in line:\n            print(line)\n            \nprint(\"\\n\\nExecution results \")\nfor k,v in result.items():\n    print(k,v)\n    \nprint(\"\\n\\nModel Output: \")\ntokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n"], "huggyllama/llama-13b": [], "togethercomputer/GPT-JT-6B-v1": ["from transformers import pipeline\npipe = pipeline(model='togethercomputer/GPT-JT-6B-v1')\npipe('''\"I love this!\" Is it positive? A:''')\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\")\n"], "timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k": [], "lmsys/vicuna-13b-v1.3": [], "Qwen/Qwen1.5-32B": [], "01-ai/Yi-6B": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "philschmid/tiny-bert-sst2-distilled": [], "facebook/mbart-large-50-one-to-many-mmt": ["from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_en = \"The head of the United Nations says there is no military solution in Syria\"\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n\nmodel_inputs = tokenizer(article_en, return_tensors=\"pt\")\n\n# translate from English to Hindi\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => '\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u0928\u0947\u0924\u093e \u0915\u0939\u0924\u0947 \u0939\u0948\u0902 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948'\n\n# translate from English to Chinese\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => '\u8054\u5408\u56fd\u9996\u8111\u8bf4,\u53d9\u5229\u4e9a\u6ca1\u6709\u519b\u4e8b\u89e3\u51b3\u529e\u6cd5'\n"], "PixArt-alpha/PixArt-XL-2-512x512": ["from diffusers import PixArtAlphaPipeline\nimport torch\n\npipe = PixArtAlphaPipeline.from_pretrained(\"PixArt-alpha/PixArt-XL-2-512x512\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"An astronaut riding a green horse\"\nimages = pipe(prompt=prompt).images[0]\n"], "flaubert/flaubert_base_uncased": ["import torch\nfrom transformers import FlaubertModel, FlaubertTokenizer\n\n# Choose among ['flaubert/flaubert_small_cased', 'flaubert/flaubert_base_uncased', \n#               'flaubert/flaubert_base_cased', 'flaubert/flaubert_large_cased']\nmodelname = 'flaubert/flaubert_base_cased' \n\n# Load pretrained model and tokenizer\nflaubert, log = FlaubertModel.from_pretrained(modelname, output_loading_info=True)\nflaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=False)\n# do_lowercase=False if using cased models, True if using uncased ones\n\nsentence = \"Le chat mange une pomme.\"\ntoken_ids = torch.tensor([flaubert_tokenizer.encode(sentence)])\n\nlast_layer = flaubert(token_ids)[0]\nprint(last_layer.shape)\n# torch.Size([1, 8, 768])  -> (batch size x number of tokens x embedding dimension)\n\n# The BERT [CLS] token correspond to the first hidden state of the last layer\ncls_embedding = last_layer[:, 0, :]\n"], "Qwen/Qwen-14B": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-14B\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-14B\", trust_remote_code=True)\n\ninputs = tokenizer('\u8499\u53e4\u56fd\u7684\u9996\u90fd\u662f\u4e4c\u5170\u5df4\u6258\uff08Ulaanbaatar\uff09\\n\u51b0\u5c9b\u7684\u9996\u90fd\u662f\u96f7\u514b\u96c5\u672a\u514b\uff08Reykjavik\uff09\\n\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7684\u9996\u90fd\u662f', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# \u8499\u53e4\u56fd\u7684\u9996\u90fd\u662f\u4e4c\u5170\u5df4\u6258\uff08Ulaanbaatar\uff09\\n\u51b0\u5c9b\u7684\u9996\u90fd\u662f\u96f7\u514b\u96c5\u672a\u514b\uff08Reykjavik\uff09\\n\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7684\u9996\u90fd\u662f\u4e9a\u7684\u65af\u4e9a\u8d1d\u5df4\uff08Addis Ababa\uff09...\n"], "bigscience/bigscience-small-testing": [], "codellama/CodeLlama-7b-Python-hf": [], "hfl/chinese-llama-2-7b": [], "01-ai/Yi-1.5-6B": [], "google/umt5-small": [], "Salesforce/codet5p-220m": ["from transformers import T5ForConditionalGeneration, AutoTokenizer\n\ncheckpoint = \"Salesforce/codet5p-220m\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():<extra_id_0>\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_length=10)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n# ==> print \"Hello World\"\n"], "microsoft/MiniLM-L12-H384-uncased": [], "facebook/mask2former-swin-small-coco-instance": ["import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\n\n# load Mask2Former fine-tuned on COCO instance segmentation\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# you can pass them to processor for postprocessing\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n# we refer to the demo notebooks for visualization (see \"Resources\" section in the Mask2Former docs)\npredicted_instance_map = result[\"segmentation\"]\n"], "google/long-t5-tglobal-large": ["from transformers import AutoTokenizer, LongT5Model\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-tglobal-large\")\nmodel = LongT5Model.from_pretrained(\"google/long-t5-tglobal-large\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n"], "facebook/xmod-base": ["from transformers import XmodModel\n\nmodel = XmodModel.from_pretrained(\"facebook/xmod-base\")\nmodel.set_default_language(\"en_XX\")\n"], "HuggingFaceM4/idefics-9b-instruct": ["import torch\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncheckpoint = \"HuggingFaceM4/idefics-9b\"\nmodel = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n# We feed to the model an arbitrary sequence of text strings and images. Images can be either URLs or PIL Images.\nprompts = [\n    [\n        \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n        \"In this picture from Asterix and Obelix, we can see\"\n    ],\n]\n\n# --batched mode\ninputs = processor(prompts, return_tensors=\"pt\").to(device)\n# --single sample mode\n# inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n\n# Generation args\nbad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\ngenerated_ids = model.generate(**inputs, bad_words_ids=bad_words_ids, max_length=100)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nfor i, t in enumerate(generated_text):\n    print(f\"{i}:\\n{t}\\n\")\n", "import torch\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncheckpoint = \"HuggingFaceM4/idefics-9b-instruct\"\nmodel = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n# We feed to the model an arbitrary sequence of text strings and images. Images can be either URLs or PIL Images.\nprompts = [\n    [\n        \"User: What is in this image?\",\n        \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n        \"<end_of_utterance>\",\n\n        \"\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>\",\n\n        \"\\nUser:\",\n        \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052\",\n        \"And who is that?<end_of_utterance>\",\n\n        \"\\nAssistant:\",\n    ],\n]\n\n# --batched mode\ninputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(device)\n# --single sample mode\n# inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n\n# Generation args\nexit_condition = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids\nbad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\ngenerated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nfor i, t in enumerate(generated_text):\n    print(f\"{i}:\\n{t}\\n\")\n", "from text_generation import Client\n\nAPI_TOKEN = \"<YOUR_API_TOKEN>\"\nAPI_URL = \"https://api-inference.huggingface.co/models/HuggingFaceM4/idefics-80b-instruct\"\nDECODING_STRATEGY = \"Greedy\"\nQUERY = \"User: What is in this image?![](https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG)<end_of_utterance>\\nAssistant:\"\n\nclient = Client(\n    base_url=API_URL,\n    headers={\"x-use-cache\": \"0\", \"Authorization\": f\"Bearer {API_TOKEN}\"},\n)\ngeneration_args = {\n    \"max_new_tokens\": 256,\n    \"repetition_penalty\": 1.0,\n    \"stop_sequences\": [\"<end_of_utterance>\", \"\\nUser:\"],\n}\n\nif DECODING_STRATEGY == \"Greedy\":\n    generation_args[\"do_sample\"] = False\nelif DECODING_STRATEGY == \"Top P Sampling\":\n    generation_args[\"temperature\"] = 1.\n    generation_args[\"do_sample\"] = True\n    generation_args[\"top_p\"] = 0.95\n    \ngenerated_text = client.generate(prompt=QUERY, **generation_args)  \nprint(generated_text)\n"], "google/switch-base-64": ["\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-64\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-64\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-64\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-64\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-64\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-64\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-64\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-64\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n"], "arpanghoshal/EmoRoBERTa": ["\nfrom transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\n\ntokenizer = RobertaTokenizerFast.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\nmodel = TFRobertaForSequenceClassification.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n\nemotion = pipeline('sentiment-analysis', \n                    model='arpanghoshal/EmoRoBERTa')\n\nemotion_labels = emotion(\"Thanks for using it.\")\nprint(emotion_labels)\n"], "dandelin/vilt-b32-mlm": ["from transformers import ViltProcessor, ViltForMaskedLM\nimport requests\nfrom PIL import Image\nimport re\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = \"a bunch of [MASK] laying on a [MASK].\"\n\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\nmodel = ViltForMaskedLM.from_pretrained(\"dandelin/vilt-b32-mlm\")\n\n# prepare inputs\nencoding = processor(image, text, return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**encoding)\n\ntl = len(re.findall(\"\\[MASK\\]\", text))\ninferred_token = [text]\n\n# gradually fill in the MASK tokens, one by one\nwith torch.no_grad():\n    for i in range(tl):\n        encoded = processor.tokenizer(inferred_token)\n        input_ids = torch.tensor(encoded.input_ids).to(device)\n        encoded = encoded[\"input_ids\"][0][1:-1]\n        outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n        mlm_logits = outputs.logits[0]  # shape (seq_len, vocab_size)\n        # only take into account text features (minus CLS and SEP token)\n        mlm_logits = mlm_logits[1 : input_ids.shape[1] - 1, :]\n        mlm_values, mlm_ids = mlm_logits.softmax(dim=-1).max(dim=-1)\n        # only take into account text\n        mlm_values[torch.tensor(encoded) != 103] = 0\n        select = mlm_values.argmax().item()\n        encoded[select] = mlm_ids[select].item()\n        inferred_token = [processor.decode(encoded)]\n\nselected_token = \"\"\nencoded = processor.tokenizer(inferred_token)\nprocessor.decode(encoded.input_ids[0], skip_special_tokens=True)\n"], "google/codegemma-7b-it": ["from transformers import GemmaTokenizer, AutoModelForCausalLM\n\ntokenizer = GemmaTokenizer.from_pretrained(\"google/codegemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/codegemma-7b-it\")\n\ninput_text = \"Write me a Python function to calculate the nth fibonacci number.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"google/codegemma-7b-it\"\ndtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=dtype,\n)\n\nchat = [\n    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\n\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"], "mosaicml/mpt-30b-instruct": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-30b-instruct',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-30b-instruct'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-30b-instruct'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n", "from transformers import pipeline\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**inputs, max_new_tokens=100)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n", "def format_prompt(instruction):\n    template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\"\n    return template.format(instruction=instruction)\n\nexample = \"Tell me a funny joke.\\nDon't make it too funny though.\"\nfmt_ex = format_prompt(instruction=example)\n"], "bigscience/bloomz-3b": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-3b\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-3b\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-3b\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "deepset/gbert-base": [], "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3": ["import torch\nfrom transformers import AutoTokenizer, pipeline\n\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\",\n    use_fast=False,\n    padding_side=\"left\",\n    trust_remote_code=True,\n)\n\ngenerate_text = pipeline(\n    model=\"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\",\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    use_fast=False,\n    device_map={\"\": \"cuda:0\"},\n)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=1,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)\nprint(res[0][\"generated_text\"])\n", "import torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\",\n    use_fast=False,\n    padding_side=\"left\",\n    trust_remote_code=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\",\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cuda:0\"},\n    trust_remote_code=True,\n)\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=1,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)\nprint(res[0][\"generated_text\"])\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_name = \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\"  # either local folder or huggingface model name\n# Important: The prompt needs to be in the same format the model was trained with.\n# You can find an example prompt in the experiment logs.\nprompt = \"<|prompt|>How are you?<|endoftext|><|answer|>\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_fast=False,\n    trust_remote_code=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cuda:0\"},\n    trust_remote_code=True,\n)\nmodel.cuda().eval()\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n\n# generate configuration can be modified to your needs\ntokens = model.generate(\n    **inputs,\n    min_new_tokens=2,\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=1,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)[0]\n\ntokens = tokens[inputs[\"input_ids\"].shape[1]:]\nanswer = tokenizer.decode(tokens, skip_special_tokens=True)\nprint(answer)\n"], "google/long-t5-local-base": ["from transformers import AutoTokenizer, LongT5Model\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\nmodel = LongT5Model.from_pretrained(\"google/long-t5-local-base\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n"], "THUDM/glm-4-9b": ["@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n"], "deepseek-ai/deepseek-llm-67b-chat": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/deepseek-llm-67b-chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\n"], "kandinsky-community/kandinsky-2-1-inpaint": ["from diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\nimport numpy as np\n\npipe = AutoPipelineForInpainting.from_pretrained(\"kandinsky-community/kandinsky-2-1-inpaint\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt = \"a hat\"\nnegative_prompt = \"low quality, bad quality\"\n\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nmask = np.zeros((768, 768), dtype=np.float32)\n# Let's mask out an area above the cat's head\nmask[:250, 250:-250] = 1\n\n\nimage = pipe(prompt=prompt, image=original_image, mask_image=mask).images[0]\nimage.save(\"cat_with_hat.png\")\n", "# For PIL input\nimport PIL.ImageOps\nmask = PIL.ImageOps.invert(mask)\n\n# For PyTorch and Numpy input\nmask = 1 - mask\n"], "WizardLM/WizardLM-70B-V1.0": [], "microsoft/prophetnet-large-uncased": ["from transformers import ProphetNetForConditionalGeneration, ProphetNetTokenizer\n\nmodel = ProphetNetForConditionalGeneration.from_pretrained(\"microsoft/prophetnet-large-uncased\")\ntokenizer = ProphetNetTokenizer.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n\ninput_str = \"the us state department said wednesday it had received no formal word from bolivia that it was expelling the us ambassador there but said the charges made against him are `` baseless .\"\ntarget_str = \"us rejects charges against its ambassador in bolivia\"\n\ninput_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\nlabels = tokenizer(target_str, return_tensors=\"pt\").input_ids\n\nloss = model(input_ids, labels=labels).loss\n"], "deepset/roberta-large-squad2": ["# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n", "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n"], "hfl/chinese-bert-wwm": [], "kandinsky-community/kandinsky-2-2-decoder-inpaint": ["from diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\nimport numpy as np\n\npipe = AutoPipelineForInpainting.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n\nprompt = \"a hat\"\n\ninit_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nmask = np.zeros((768, 768), dtype=np.float32)\n# Let's mask out an area above the cat's head\nmask[:250, 250:-250] = 1\n\n\nout = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask,\n    height=768,\n    width=768,\n    num_inference_steps=150,\n)\n\nimage = out.images[0]\nimage.save(\"cat_with_hat.png\")\n", "# For PIL input\nimport PIL.ImageOps\nmask = PIL.ImageOps.invert(mask)\n\n# For PyTorch and Numpy input\nmask = 1 - mask\n"], "baichuan-inc/Baichuan2-13B-Chat": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\",\n    revision=\"v2.0\",\n    use_fast=False,\n    trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\",\n    revision=\"v2.0\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True)\nmodel.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\", revision=\"v2.0\")\nmessages = []\nmessages.append({\"role\": \"user\", \"content\": \"\u89e3\u91ca\u4e00\u4e0b\u201c\u6e29\u6545\u800c\u77e5\u65b0\u201d\"})\nresponse = model.chat(tokenizer, messages)\nprint(response)\n\"\u6e29\u6545\u800c\u77e5\u65b0\"\u662f\u4e00\u53e5\u4e2d\u56fd\u53e4\u4ee3\u7684\u6210\u8bed\uff0c\u51fa\u81ea\u300a\u8bba\u8bed\u00b7\u4e3a\u653f\u300b\u7bc7\u3002\u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u662f\uff1a\u901a\u8fc7\u56de\u987e\u8fc7\u53bb\uff0c\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u65b0\u7684\u77e5\u8bc6\u548c\u7406\u89e3\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u5b66\u4e60\u5386\u53f2\u548c\u7ecf\u9a8c\u53ef\u4ee5\u8ba9\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u73b0\u5728\u548c\u672a\u6765\u3002\n\n\u8fd9\u53e5\u8bdd\u9f13\u52b1\u6211\u4eec\u5728\u5b66\u4e60\u548c\u751f\u6d3b\u4e2d\u4e0d\u65ad\u5730\u56de\u987e\u548c\u53cd\u601d\u8fc7\u53bb\u7684\u7ecf\u9a8c\uff0c\u4ece\u800c\u83b7\u5f97\u65b0\u7684\u542f\u793a\u548c\u6210\u957f\u3002\u901a\u8fc7\u91cd\u6e29\u65e7\u7684\u77e5\u8bc6\u548c\u7ecf\u5386\uff0c\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u65b0\u7684\u89c2\u70b9\u548c\u7406\u89e3\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u4e16\u754c\u548c\u6311\u6218\u3002\n"], "codellama/CodeLlama-13b-hf": ["from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"codellama/CodeLlama-13b-hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'import socket\\n\\ndef ping_exponential_backoff(host: str):',\n    do_sample=True,\n    top_k=10,\n    temperature=0.1,\n    top_p=0.95,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "lmsys/vicuna-33b-v1.3": [], "01-ai/Yi-1.5-6B-Chat": [], "RUCAIBox/mvp": [">>> from transformers import MvpTokenizer, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\n...     \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n[\"Why You Shouldn't Quit Your Job\"]\n", ">>> from transformers import MvpTokenizerFast, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizerFast.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n\n>>> inputs = tokenizer(\n...     \"Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Stan Lee created the character of Iron Man, a fictional superhero appearing in American comic']\n"], "MediaTek-Research/Breeze-7B-Instruct-v1_0": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Instruction Model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"MediaTek-Research/Breeze-7B-Instruct-v1_0\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    # attn_implementation=\"flash_attention_2\" # optional\n)\n\n# Basemodel\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"MediaTek-Research/Breeze-7B-Base-v1_0\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    # attn_implementation=\"flash_attention_2\" # optional\n)\n", "You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.\n", ">>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v1_0\")\n>>> chat = [\n...   {\"role\": \"user\", \"content\": \"\u4f60\u597d\uff0c\u8acb\u554f\u4f60\u53ef\u4ee5\u5b8c\u6210\u4ec0\u9ebc\u4efb\u52d9\uff1f\"},\n...   {\"role\": \"assistant\", \"content\": \"\u4f60\u597d\uff0c\u6211\u53ef\u4ee5\u5e6b\u52a9\u60a8\u89e3\u6c7a\u5404\u7a2e\u554f\u984c\u3001\u63d0\u4f9b\u8cc7\u8a0a\u548c\u5354\u52a9\u60a8\u5b8c\u6210\u8a31\u591a\u4e0d\u540c\u7684\u4efb\u52d9\u3002\u4f8b\u5982\uff1a\u56de\u7b54\u6280\u8853\u554f\u984c\u3001\u63d0\u4f9b\u5efa\u8b70\u3001\u7ffb\u8b6f\u6587\u5b57\u3001\u5c0b\u627e\u8cc7\u6599\u6216\u5354\u52a9\u60a8\u5b89\u6392\u884c\u7a0b\u7b49\u3002\u8acb\u544a\u8a34\u6211\u5982\u4f55\u80fd\u5e6b\u52a9\u60a8\u3002\"},\n...   {\"role\": \"user\", \"content\": \"\u592a\u68d2\u4e86\uff01\"},\n... ]\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\n\"<s>You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.  [INST] \u4f60\u597d\uff0c\u8acb\u554f\u4f60\u53ef\u4ee5\u5b8c\u6210\u4ec0\u9ebc\u4efb\u52d9\uff1f [/INST] \u4f60\u597d\uff0c\u6211\u53ef\u4ee5\u5e6b\u52a9\u60a8\u89e3\u6c7a\u5404\u7a2e\u554f\u984c\u3001\u63d0\u4f9b\u8cc7\u8a0a\u548c\u5354\u52a9\u60a8\u5b8c\u6210\u8a31\u591a\u4e0d\u540c\u7684\u4efb\u52d9\u3002\u4f8b\u5982\uff1a\u56de\u7b54\u6280\u8853\u554f\u984c\u3001\u63d0\u4f9b\u5efa\u8b70\u3001\u7ffb\u8b6f\u6587\u5b57\u3001\u5c0b\u627e\u8cc7\u6599\u6216\u5354\u52a9\u60a8\u5b89\u6392\u884c\u7a0b\u7b49\u3002\u8acb\u544a\u8a34\u6211\u5982\u4f55\u80fd\u5e6b\u52a9\u60a8\u3002 [INST] \u592a\u68d2\u4e86\uff01 [/INST] \"\n# Tokenized results\n# ['\u2581', '\u4f60\u597d', '\uff0c', '\u8acb\u554f', '\u4f60', '\u53ef\u4ee5', '\u5b8c\u6210', '\u4ec0\u9ebc', '\u4efb\u52d9', '\uff1f']\n# ['\u2581', '\u4f60\u597d', '\uff0c', '\u6211', '\u53ef\u4ee5', '\u5e6b\u52a9', '\u60a8', '\u89e3\u6c7a', '\u5404\u7a2e', '\u554f\u984c', '\u3001', '\u63d0\u4f9b', '\u8cc7\u8a0a', '\u548c', '\u5354\u52a9', '\u60a8', '\u5b8c\u6210', '\u8a31\u591a', '\u4e0d\u540c', '\u7684', '\u4efb\u52d9', '\u3002', '\u4f8b\u5982', '\uff1a', '\u56de\u7b54', '\u6280\u8853', '\u554f\u984c', '\u3001', '\u63d0\u4f9b', '\u5efa\u8b70', '\u3001', '\u7ffb\u8b6f', '\u6587\u5b57', '\u3001', '\u5c0b\u627e', '\u8cc7\u6599', '\u6216', '\u5354\u52a9', '\u60a8', '\u5b89\u6392', '\u884c\u7a0b', '\u7b49', '\u3002', '\u8acb', '\u544a\u8a34', '\u6211', '\u5982\u4f55', '\u80fd', '\u5e6b\u52a9', '\u60a8', '\u3002']\n# ['\u2581', '\u592a', '\u68d2', '\u4e86', '\uff01']\n"], "TheBloke/Llama-2-70B-Chat-GGUF": ["from ctransformers import AutoModelForCausalLM\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-70B-chat-GGUF\", model_file=\"llama-2-70b-chat.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n"], "shi-labs/oneformer_ade20k_swin_tiny": ["from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\nfrom PIL import Image\nimport requests\nurl = \"https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# Loading a single model for all three tasks\nprocessor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\nmodel = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n\n# Semantic Segmentation\nsemantic_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\")\nsemantic_outputs = model(**semantic_inputs)\n# pass through image_processor for postprocessing\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n\n# Instance Segmentation\ninstance_inputs = processor(images=image, task_inputs=[\"instance\"], return_tensors=\"pt\")\ninstance_outputs = model(**instance_inputs)\n# pass through image_processor for postprocessing\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]\n\n# Panoptic Segmentation\npanoptic_inputs = processor(images=image, task_inputs=[\"panoptic\"], return_tensors=\"pt\")\npanoptic_outputs = model(**panoptic_inputs)\n# pass through image_processor for postprocessing\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]\n"], "togethercomputer/RedPajama-INCITE-Chat-3B-v1": ["import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n# infer\nprompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\nAlan Turing was a British mathematician, logician, cryptologist, and computer scientist. He is widely regarded as the father of computer science and artificial intelligence.\n\"\"\"\n", "import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", device_map='auto', torch_dtype=torch.float16, load_in_8bit=True)\n\n# infer\nprompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\nAlan Turing was a British mathematician and computer scientist who made important contributions to computer science and mathematical logic. He is widely regarded as the father of computer science and artificial intelligence for his work on the Turing machine and Turing test.\n\"\"\"\n", "import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.bfloat16)\n# infer\nprompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\nAlan Turing was a British mathematician and computer scientist who made important contributions to the fields of mathematics, cryptography, and computer science. He is widely regarded as the father of computer science and artificial intelligence.\n\"\"\"\n"], "facebook/deit-small-patch16-224": ["from transformers import AutoFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-small-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('facebook/deit-small-patch16-224')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "bigscience/bloom": [], "codellama/CodeLlama-70b-Instruct-hf": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"codellama/CodeLlama-70b-Instruct-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n   model_id,\n   torch_dtype=torch.float16,\n   device_map=\"auto\",\n)\n\nchat = [\n   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n   {\"role\": \"user\", \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n]\ninputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\"cuda\")\n\noutput = model.generate(input_ids=inputs, max_new_tokens=200)\noutput = output[0].to(\"cpu\")\nprint(tokenizer.decode(output))\n", "from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel_id = \"codellama/CodeLlama-70b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipeline = transformers.pipeline(\n   \"text-generation\",\n   model=model_id,\n   torch_dtype=torch.float16,\n   device_map=\"auto\",\n)\n\nsequences = pipeline(\n   'def fibonacci(',\n   do_sample=True,\n   temperature=0.2,\n   top_p=0.9,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=100,\n)\nfor seq in sequences:\n   print(f\"Result: {seq['generated_text']}\")\n"], "Deci/DeciLM-7B": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Deci/DeciLM-7B\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True).to(device)\n\ninputs = tokenizer.encode(\"In a shocking finding, scientists discovered a herd of unicorns living in\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_p=0.95)\nprint(tokenizer.decode(outputs[0]))\n\n# The model can also be used via the text-generation pipeline interface\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", \"Deci/DeciLM-7B\", torch_dtype=\"auto\", trust_remote_code=True, device=device)\noutputs = generator(\"In a shocking finding, scientists discovered a herd of unicorns living in\", max_new_tokens=100, do_sample=True, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n"], "EleutherAI/pythia-12b-deduped": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "NousResearch/Nous-Hermes-llama-2-7b": [], "rinna/japanese-gpt2-medium": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\", use_fast=False)\ntokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n\nmodel = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n"], "google/pix2struct-textcaps-base": ["from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\n\nmodel.push_to_hub(\"USERNAME/MODEL_NAME\")\nprocessor.push_to_hub(\"USERNAME/MODEL_NAME\")\n", "import requests\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n# image only\ninputs = processor(images=image, return_tensors=\"pt\")\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n>>> A stop sign is on a street corner.\n", "import requests\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\").to(\"cuda\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n# image only\ninputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n>>> A stop sign is on a street corner.\n", "import requests\nimport torch\n\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\", torch_dtype=torch.bfloat16).to(\"cuda\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n# image only\ninputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\", torch.bfloat16)\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n>>> A stop sign is on a street corner.\n", "import requests\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = \"A picture of\"\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n# image only\ninputs = processor(images=image, text=text, return_tensors=\"pt\")\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n>>> A picture of a stop sign that says yes.\n"], "keepitreal/vietnamese-sbert": ["from sentence_transformers import SentenceTransformer\nsentences = [\"C\u00f4 gi\u00e1o \u0111ang \u0103n kem\", \"Ch\u1ecb g\u00e1i \u0111ang th\u1eed m\u00f3n th\u1ecbt d\u00ea\"]\n\nmodel = SentenceTransformer('keepitreal/vietnamese-sbert')\nembeddings = model.encode(sentences)\nprint(embeddings)\n", "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['C\u00f4 gi\u00e1o \u0111ang \u0103n kem', 'Ch\u1ecb g\u00e1i \u0111ang th\u1eed m\u00f3n th\u1ecbt d\u00ea']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(''keepitreal/vietnamese-sbert')\nmodel = AutoModel.from_pretrained('keepitreal/vietnamese-sbert')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n"], "xlm-mlm-en-2048": ["from transformers import XLMTokenizer, XLMModel\nimport torch\n\ntokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-en-2048\")\nmodel = XLMModel.from_pretrained(\"xlm-mlm-en-2048\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n"], "google/long-t5-tglobal-base": ["from transformers import AutoTokenizer, LongT5Model\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-tglobal-base\")\nmodel = LongT5Model.from_pretrained(\"google/long-t5-tglobal-base\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n"], "TheBloke/Llama-2-7B-Chat-AWQ": ["from vllm import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(model=\"TheBloke/Llama-2-7b-Chat-AWQ\", quantization=\"awq\")\n\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n", "from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_name_or_path = \"TheBloke/Llama-2-7b-Chat-AWQ\"\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n                                          trust_remote_code=False, safetensors=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ntokens = tokenizer(\n    prompt_template,\n    return_tensors='pt'\n).input_ids.cuda()\n\n# Generate output\ngeneration_output = model.generate(\n    tokens,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    max_new_tokens=512\n)\n\nprint(\"Output: \", tokenizer.decode(generation_output[0]))\n\n# Inference can also be done using transformers' pipeline\nfrom transformers import pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "Vamsi/T5_Paraphrase_Paws": ["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\").to('cuda')\n\u200b\nsentence = \"This is something which i cannot understand at all\"\n\ntext =  \"paraphrase: \" + sentence + \" </s>\"\n\nencoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\n\noutputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    max_length=256,\n    do_sample=True,\n    top_k=120,\n    top_p=0.95,\n    early_stopping=True,\n    num_return_sequences=5\n)\n\nfor output in outputs:\n    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    print(line)\n\u200b\n"], "apple/mobilevit-xx-small": ["from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-xx-small\")\nmodel = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "meta-llama/CodeLlama-7b-Instruct-hf": [], "abeja/gpt-neox-japanese-2.7b": ["from transformers import pipeline\n\n\ngenerator = pipeline(\"text-generation\", model=\"abeja/gpt-neox-japanese-2.7b\")\ngenerated = generator(\n    \"\u4eba\u3068AI\u304c\u5354\u8abf\u3059\u308b\u305f\u3081\u306b\u306f\u3001\",\n    max_length=300,\n    do_sample=True,\n    num_return_sequences=3,\n    top_p=0.95,\n    top_k=50\n)\nprint(*generated, sep=\"\\n\")\n\n\"\"\"\n[out]\n{\"generated_text\": \"\u4eba\u3068AI\u304c\u5354\u8abf\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u300c\u4eba\u304c\u6301\u3063\u3066\u3044\u308b\u512a\u308c\u305f\u80fd\u529b\u3068AI\u306e\u5f97\u610f\u3068\u3059\u308b\u5206\u91ce\u3092\u639b\u3051\u5408\u308f\u305b\u308b\u300d\u3053\u3068\u304c\u4e0d\u53ef\u6b20\u306b\u306a\u308a\u307e\u3059\u3002\"}\n{\"generated_text\": \"\u4eba\u3068AI\u304c\u5354\u8abf\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u53cc\u65b9\u306e\u9577\u6240\u3092\u6d3b\u304b\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u4e0d\u53ef\u6b20\u3060\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\"}\n{\"generated_text\": \"\u4eba\u3068AI\u304c\u5354\u8abf\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u4eba\u9593\u304cAI\u3092\u7406\u89e3\u3059\u308b\u3001\u3068\u3044\u3046\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u4eba\u9593\u306b\u306f\u300cAI\u306b\u5bfe\u3057\u3066AI\u304c\u4f55\u3092\u3059\u308b\u3079\u304d\u304b\u300d\u3068\u3044\u3046\u3053\u3068\u3092\u660e\u78ba\u306b\u6559\u3048\u306a\u3044\u3068\u3001AI\u306f\u3042\u308b\u7a0b\u5ea6\u306e\u77e5\u8b58\u306f\u3042\u3063\u3066\u3082\u4f55\u3092\u3059\u3079\u304d\u304b\u304c\u308f\u304b\u308a\u307e\u305b\u3093\u3002\u3060\u304b\u3089\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u304c\u8003\u3048\u305f\u308a\u3001\u6c7a\u3081\u305f\u308a\u3059\u308b\u3053\u3068\u306fAI\u3067\u306f\u306a\u304f\u3001\u4eba\u9593\u304c\u89e3\u91c8\u3057\u3066\u7406\u89e3\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u306a\u304f\u3066\"}\n\"\"\"\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n\ninput_text = \"\u4eba\u3068AI\u304c\u5354\u8abf\u3059\u308b\u305f\u3081\u306b\u306f\u3001\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\ngen_tokens = model.generate(\n    input_ids,\n    max_length=100,\n    do_sample=True,\n    num_return_sequences=3,\n    top_p=0.95,\n    top_k=50,\n)\nfor gen_text in tokenizer.batch_decode(gen_tokens, skip_special_tokens=True):\n    print(gen_text)\n"], "microsoft/cvt-13": ["from transformers import AutoFeatureExtractor, CvtForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/cvt-13')\nmodel = CvtForImageClassification.from_pretrained('microsoft/cvt-13')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "WizardLM/WizardCoder-15B-V1.0": [], "albert-large-v2": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-large-v2')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u00e2\u2013\u0081modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u00e2\u2013\u0081modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u00e2\u2013\u0081model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u00e2\u2013\u0081runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u00e2\u2013\u0081lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-large-v2')\nmodel = AlbertModel.from_pretrained(\"albert-large-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-large-v2')\nmodel = TFAlbertModel.from_pretrained(\"albert-large-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-large-v2')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u00e2\u2013\u0081shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u00e2\u2013\u0081blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u00e2\u2013\u0081lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u00e2\u2013\u0081receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u00e2\u2013\u0081paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u00e2\u2013\u0081waitress\"\n   }\n]\n"], "deepseek-ai/deepseek-coder-33b-base": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-33b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-33b-base\", trust_remote_code=True).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-33b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-33b-base\", trust_remote_code=True).cuda()\ninput_text = \"\"\"<\uff5cfim\u2581begin\uff5c>def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[0]\n    left = []\n    right = []\n<\uff5cfim\u2581hole\uff5c>\n        if arr[i] < pivot:\n            left.append(arr[i])\n        else:\n            right.append(arr[i])\n    return quick_sort(left) + [pivot] + quick_sort(right)<\uff5cfim\u2581end\uff5c>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-33b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-33b-base\", trust_remote_code=True).cuda()\n\ninput_text = \"\"\"#utils.py\nimport torch\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\ndef load_data():\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # Convert numpy data to PyTorch tensors\n    X_train = torch.tensor(X_train, dtype=torch.float32)\n    X_test = torch.tensor(X_test, dtype=torch.float32)\n    y_train = torch.tensor(y_train, dtype=torch.int64)\n    y_test = torch.tensor(y_test, dtype=torch.int64)\n    \n    return X_train, X_test, y_train, y_test\n\ndef evaluate_predictions(y_test, y_pred):\n    return accuracy_score(y_test, y_pred)\n#model.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass IrisClassifier(nn.Module):\n    def __init__(self):\n        super(IrisClassifier, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(4, 16),\n            nn.ReLU(),\n            nn.Linear(16, 3)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n    def train_model(self, X_train, y_train, epochs, lr, batch_size):\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.parameters(), lr=lr)\n        \n        # Create DataLoader for batches\n        dataset = TensorDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n        for epoch in range(epochs):\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                outputs = self(batch_X)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n\n    def predict(self, X_test):\n        with torch.no_grad():\n            outputs = self(X_test)\n            _, predicted = outputs.max(1)\n        return predicted.numpy()\n#main.py\nfrom utils import load_data, evaluate_predictions\nfrom model import IrisClassifier as Classifier\n\ndef main():\n    # Model training and evaluation\n\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=140)\nprint(tokenizer.decode(outputs[0]))\n"], "CohereForAI/c4ai-command-r-v01": ["# pip install 'transformers>=4.39.1'\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\n# Format message with the command-r chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n\ngen_tokens = model.generate(\n    input_ids, \n    max_new_tokens=100, \n    do_sample=True, \n    temperature=0.3,\n    )\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n", "# pip install 'transformers>=4.39.1' bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n\n# Format message with the command-r chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n\ngen_tokens = model.generate(\n    input_ids, \n    max_new_tokens=100, \n    do_sample=True, \n    temperature=0.3,\n    )\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# define documents to ground on:\ndocuments = [\n    { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest growing up to 122 cm in height.\" }, \n    { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}\n]\n\n# render the tool use prompt as a string:\ngrounded_generation_prompt = tokenizer.apply_grounded_generation_template(\n    conversation,\n    documents=documents,\n    citation_mode=\"accurate\", # or \"fast\"\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(grounded_generation_prompt)\n", "The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\nDocument: 0\ntitle: Tall penguins\ntext: Emperor penguins are the tallest growing up to 122 cm in height.\n\nDocument: 1\ntitle: Penguin habitats\ntext: Emperor penguins only live in Antarctica.\n</results><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Carefully perform the following instructions, in order, starting each with a new line.\nFirstly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.\nSecondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.\nThirdly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\nFinally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# Define tools available for the model to use:\ntools = [\n  {\n    \"name\": \"internet_search\",\n    \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n    \"parameter_definitions\": {\n      \"query\": {\n        \"description\": \"Query to search the internet with\",\n        \"type\": 'str',\n        \"required\": True\n      }\n    }\n  },\n  {\n    'name': \"directly_answer\",\n    \"description\": \"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\",\n    'parameter_definitions': {}\n  }\n]\n\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_tool_use_template(\n    conversation,\n    tools=tools,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(tool_use_prompt)\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n\n# Define tools available for the model to use\n# Type hints and docstrings from Python functions are automatically extracted\ndef internet_search(query: str):\n    \"\"\"\n    Returns a list of relevant document snippets for a textual query retrieved from the internet\n\n    Args:\n        query: Query to search the internet with\n    \"\"\"\n    pass\n\ndef directly_answer():\n    \"\"\"\n    Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n    \"\"\"\n    pass\n\ntools = [internet_search, directly_answer]\n\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_chat_template(\n    conversation,\n    tools=tools,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(tool_use_prompt)\n", "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\nThe instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n\n## Available Tools\nHere is a list of tools that you have available to you:\n\n```python\ndef internet_search(query: str) -> List[Dict]:\n    \"\"\"Returns a list of relevant document snippets for a textual query retrieved from the internet\n\n    Args:\n        query (str): Query to search the internet with\n    \"\"\"\n    pass\n```\n\n```python\ndef directly_answer() -> List[Dict]:\n    \"\"\"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n    \"\"\"\n    pass\n```<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write 'Action:' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user's last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n```json\n[\n    {\n        \"tool_name\": title of the tool in the specification,\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\n    }\n]```<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n"], "facebook/esm2_t36_3B_UR50D": [], "bigcode/starcoder": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "input_text = \"<fim_prefix>def print_hello_world():\\n    <fim_suffix>\\n    print('Hello world!')<fim_middle>\"\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "microsoft/swin-base-patch4-window12-384": ["from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-base-patch4-window12-384\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window12-384\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "teknium/OpenHermes-2-Mistral-7B": ["<|im_start|>system\nYou are to roleplay as Edward Elric from fullmetal alchemist. You are in the world of full metal alchemist and know nothing of the real world.\n"], "Salesforce/blip2-flan-t5-xxl": ["import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "# pip install accelerate\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "# pip install accelerate\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", torch_dtype=torch.float16, device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n", "# pip install accelerate bitsandbytes\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_8bit=True, device_map=\"auto\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n"], "BAAI/AquilaChat2-7B": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\n\ndevice = torch.device(\"cuda:0\")\nmodel_info = \"BAAI/AquilaChat2-7B\"\ntokenizer = AutoTokenizer.from_pretrained(model_info, trust_remote_code=True)\nquantization_config=BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_use_double_quant=True,\n                        bnb_4bit_quant_type=\"nf4\",\n                        bnb_4bit_compute_dtype=torch.bfloat16,\n                    )\nmodel = AutoModelForCausalLM.from_pretrained(model_info, trust_remote_code=True, torch_dtype=torch.float16,\n                                                # quantization_config=quantization_config, # Uncomment this line for 4bit quantization\n                                                )\nmodel.eval()\nmodel.to(device)\ntext = \"\u8bf7\u7ed9\u51fa10\u4e2a\u8981\u5230\u5317\u4eac\u65c5\u6e38\u7684\u7406\u7531\u3002\"\nfrom predict import predict\nout = predict(model, text, tokenizer=tokenizer, max_gen_len=200, top_p=0.95,\n              seed=1234, topk=100, temperature=0.9, sft=True, device=device,\n              model_name=\"AquilaChat2-7B\")\nprint(out)\n"], "MBZUAI/LaMini-T5-738M": ["# pip install -q transformers\nfrom transformers import pipeline\n\ncheckpoint = \"{model_name}\"\n\nmodel = pipeline('text2text-generation', model = checkpoint)\n\ninput_prompt = 'Please let me know your thoughts on the given place and why you think it deserves to be visited: \\n\"Barcelona, Spain\"'\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\n\nprint(\"Response\", generated_text)\n", "@article{lamini-lm,\n  author       = {Minghao Wu and\n                  Abdul Waheed and\n                  Chiyu Zhang and\n                  Muhammad Abdul-Mageed and\n                  Alham Fikri Aji\n                  },\n  title        = {LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},\n  journal      = {CoRR},\n  volume       = {abs/2304.14402},\n  year         = {2023},\n  url          = {https://arxiv.org/abs/2304.14402},\n  eprinttype   = {arXiv},\n  eprint       = {2304.14402}\n}\n"], "01-ai/Yi-6B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "kandinsky-community/kandinsky-3": ["from diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n        \nprompt = \"A photograph of the inside of a subway train. There are raccoons sitting on the seats. One of them is reading a newspaper. The window shows the city in the background.\"\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimage = pipe(prompt, num_inference_steps=25, generator=generator).images[0]\n", "from diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipe = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\n        \nprompt = \"A painting of the inside of a subway train with tiny raccoons.\"\nimage = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky3/t2i.png\")\n\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimage = pipe(prompt, image=image, strength=0.75, num_inference_steps=25, generator=generator).images[0]\n"], "BAAI/AquilaChat2-34B": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nimport torch\n\ndevice = torch.device(\"cuda:0\")\nmodel_info = \"BAAI/AquilaChat2-34B\"\ntokenizer = AutoTokenizer.from_pretrained(model_info, trust_remote_code=True)\nquantization_config=BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_use_double_quant=True,\n                        bnb_4bit_quant_type=\"nf4\",\n                        bnb_4bit_compute_dtype=torch.bfloat16,\n                    )\nmodel = AutoModelForCausalLM.from_pretrained(model_info, trust_remote_code=True, torch_dtype=torch.bfloat16,\n                                                # quantization_config=quantization_config, # Uncomment this line for 4bit quantization\n                                                )\nmodel.eval()\nmodel.to(device)\ntext = \"\u8bf7\u7ed9\u51fa10\u4e2a\u8981\u5230\u5317\u4eac\u65c5\u6e38\u7684\u7406\u7531\u3002\"\nfrom predict import predict\nout = predict(model, text, tokenizer=tokenizer, max_gen_len=200, top_p=0.9,\n              seed=123, topk=15, temperature=1.0, sft=True, device=device,\n              model_name=\"AquilaChat2-34B\")\nprint(out)\n"], "openaccess-ai-collective/manticore-13b": ["### Instruction: write Python code that returns the first n numbers of the Fibonacci sequence using memoization.\n\n### Assistant:\nHere\u2019s an example of how you can use memoization to efficiently calculate the first n numbers of the Fibonacci sequence in Python:\n```\ndef fib(n):\n # Memoize the last calculated Fibonacci number for each index\n cache = {}\n \n def fib_memoized(n):\n if n not in cache:\n cache[n] = [0, 1]\n else:\n return cache[n][-1], cache[n][-2]\n \n # Recursive helper function to calculate the Fibonacci sequence up to a given index\n def fib_helper(n):\n if n == 0 or n == 1:\n return 0, 1\n else:\n return fib_helper(n - 1) + fib_helper(n - 2)\n \n# Calculate the first n numbers of the Fibonacci sequence using memoization\n def fib_memoized_seq(n):\n if n == 0 or n == 1:\n return 0, 1\n else:\n result = fib_helper(n - 1) + fib_helper(n - 2)\n cache[n] = [result]\n return result, cache[n][-1], cache[n][-2]\n \n # Example usage\n for i in range(len(fib_memoized_seq(3))):\n print(fib_memoized_seq(3)[i])\n```\n\nIn this implementation, we use a cache dictionary to memoize the last calculated Fibonacci number for each index. We define two helper functions: fib_helper, which recursively calculates the Fibonacci sequence up to a given index, and fib_memoized, which uses memoization to efficiently calculate the first n numbers of the Fibonacci sequence. Finally, we define the fib_memoized_seq function, which is our main function for calculating the first n numbers of the Fibonacci sequence using memoization.\n", "USER: Finish the joke, a mechanic and a car salesman walk into a bar...  \nASSISTANT: \u2026 and when they get up to leave, they realize that they\u2019ve left their keys in their cars. The mechanic says \u201cI know how to fix this\u201d and opens his toolbox to grab something. Then he stops and looks at the salesman. He says \u201cYou drive a Toyota, right?\u201d The salesman nods. The mechanic then takes out a tiny screwdriver from his toolbox and starts working on the lock of the car door. The salesman is amazed and asks how he knew that. The mechanic replies \u201cI can tell by the way you parked your car.\u201d\n", "USER: Describe how a tsunami would affect the residents of Denver Colorado\nASSISTANT: The impact of a tsunami in Denver, Colorado would likely be minimal. As there are no coastal areas or bodies of water nearby, a tsunami generated from an earthquake or volcanic activity elsewhere in the world would have little effect on the city and its residents. Even if a large tsunami were to hit the coastline of California or Mexico, the distance between Denver and these locations means that any impacts on the city itself would be minimal. \n"], "DeepFloyd/IF-II-L-v1.0": ["from huggingface_hub import login\n\nlogin()\n", "from diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nimport torch\n\n# stage 1\nstage_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_1.enable_model_cpu_offload()\n\n# stage 2\nstage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_2.enable_model_cpu_offload()\n\n# stage 3\nsafety_modules = {\"feature_extractor\": stage_1.feature_extractor, \"safety_checker\": stage_1.safety_checker, \"watermarker\": stage_1.watermarker}\nstage_3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16)\nstage_3.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_3.enable_model_cpu_offload()\n"], "tau/splinter-base": [], "bigcode/santacoder": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/santacoder\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "input_text = \"<fim-prefix>def print_hello_world():\\n    <fim-suffix>\\n    print('Hello world!')<fim-middle>\"\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "Qwen/Qwen1.5-MoE-A2.7B": [], "allenai/OLMo-1B": ["from hf_olmo import OLMoForCausalLM  # pip install ai2-olmo\n\nolmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-1B\", revision=\"step20000-tokens84B\")\n", "from huggingface_hub import list_repo_refs\nout = list_repo_refs(\"allenai/OLMo-1B\")\nbranches = [b.name for b in out.branches]\n", "from hf_olmo import OLMoForCausalLM, OLMoTokenizerFast\n\nolmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-1B\")\ntokenizer = OLMoTokenizerFast.from_pretrained(\"allenai/OLMo-1B\")\nmessage = [\"Language modeling is\"]\ninputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n# optional verifying cuda\n# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n# olmo = olmo.to('cuda')\nresponse = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n>> 'Language modeling is the first step to build natural language generation...'\n"], "lmsys/vicuna-7b-v1.5-16k": [], "asafaya/bert-base-arabic": ["from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"asafaya/bert-base-arabic\")\n"], "Qwen/Qwen2-57B-A14B": [], "aubmindlab/bert-base-arabertv2": ["from arabert.preprocess import ArabertPreprocessor\n\nmodel_name=\"bert-base-arabertv2\"\narabert_prep = ArabertPreprocessor(model_name=model_name)\n\ntext = \"\u0648\u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644\u0646\u0627 \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644\u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\"\narabert_prep.preprocess(text)\n>>>\"\u0648+ \u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644 +\u0646\u0627 \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644+ \u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646 +\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\"\n"], "Intel/dynamic_tinybert": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\ntokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n\ncontext = \"remember the number 123456, I'll ask you later.\"\nquestion = \"What is the number I told you?\"\n\n# Tokenize the context and question\ntokens = tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True)\n\n# Get the input IDs and attention mask\ninput_ids = tokens[\"input_ids\"]\nattention_mask = tokens[\"attention_mask\"]\n\n# Perform question answering\noutputs = model(input_ids, attention_mask=attention_mask)\nstart_scores = outputs.start_logits\nend_scores = outputs.end_logits\n\n# Find the start and end positions of the answer\nanswer_start = torch.argmax(start_scores)\nanswer_end = torch.argmax(end_scores) + 1\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))\n\n# Print the answer\nprint(\"Answer:\", answer)\n"], "pszemraj/long-t5-tglobal-base-16384-book-summary": ["import torch\nfrom transformers import pipeline\n\nsummarizer = pipeline(\n    \"summarization\",\n    \"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n    device=0 if torch.cuda.is_available() else -1,\n)\nlong_text = \"Here is a lot of text I don't want to read. Replace me\"\n\nresult = summarizer(long_text)\nprint(result[0][\"summary_text\"])\n", "from textsum.summarize import Summarizer\n\nsummarizer = Summarizer(\n    model_name_or_path=\"pszemraj/long-t5-tglobal-base-16384-book-summary\"\n)\n\nlong_string = \"This is a long string of text that will be summarized.\"\nout_str = summarizer.summarize_string(long_string)\nprint(f\"summary: {out_str}\")\n"], "deepseek-ai/DeepSeek-V2": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/DeepSeek-V2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# `max_memory` should be set based on your devices\nmax_memory = {i: \"75GB\" for i in range(8)}\n# `device_map` cannot be set to `auto`\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"sequential\", torch_dtype=torch.bfloat16, max_memory=max_memory, attn_implementation=\"eager\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntext = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)\n", "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/DeepSeek-V2-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# `max_memory` should be set based on your devices\nmax_memory = {i: \"75GB\" for i in range(8)}\n# `device_map` cannot be set to `auto`\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"sequential\", torch_dtype=torch.bfloat16, max_memory=max_memory, attn_implementation=\"eager\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\n", "from transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 8192, 8\nmodel_name = \"deepseek-ai/DeepSeek-V2-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you?\"}],\n    [{\"role\": \"user\", \"content\": \"Translate the following content into Chinese directly: DeepSeek-V2 adopts innovative architectures to guarantee economical training and efficient inference.\"}],\n    [{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n"], "liuhaotian/llava-llama-2-13b-chat-lightning-preview": [], "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1": ["# pip install 'transformers>=4.39.3'\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are Zephyr, a helpful assistant.\",\n    },\n    {\"role\": \"user\", \"content\": \"Explain how Mixture of Experts work in language a child would understand.\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n)\nprint(outputs[0][\"generated_text\"][-1][\"content\"])\n"], "CIDAS/clipseg-rd64": [], "bigscience/mt0-large": ["# pip install -q transformers\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ncheckpoint = \"bigscience/mt0-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ncheckpoint = \"bigscience/mt0-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ncheckpoint = \"bigscience/mt0-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "01-ai/Yi-1.5-34B": [], "google/recurrentgemma-2b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/recurrentgemma-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/recurrentgemma-2b\", device_map=\"auto\")\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli": ["from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nif __name__ == '__main__':\n    max_length = 256\n\n    premise = \"Two women are embracing while holding to go packages.\"\n    hypothesis = \"The men are fighting outside a deli.\"\n\n    hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n    # hg_model_hub_name = \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\"\n    # hg_model_hub_name = \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n    # hg_model_hub_name = \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\"\n    # hg_model_hub_name = \"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\"\n\n    tokenizer = AutoTokenizer.from_pretrained(hg_model_hub_name)\n    model = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)\n\n    tokenized_input_seq_pair = tokenizer.encode_plus(premise, hypothesis,\n                                                     max_length=max_length,\n                                                     return_token_type_ids=True, truncation=True)\n\n    input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0)\n    # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n    token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0)\n    attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0)\n\n    outputs = model(input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids,\n                    labels=None)\n    # Note:\n    # \"id2label\": {\n    #     \"0\": \"entailment\",\n    #     \"1\": \"neutral\",\n    #     \"2\": \"contradiction\"\n    # },\n\n    predicted_probability = torch.softmax(outputs[0], dim=1)[0].tolist()  # batch_size only one\n\n    print(\"Premise:\", premise)\n    print(\"Hypothesis:\", hypothesis)\n    print(\"Entailment:\", predicted_probability[0])\n    print(\"Neutral:\", predicted_probability[1])\n    print(\"Contradiction:\", predicted_probability[2])\n"], "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg": [], "TheLastBen/Papercut_SDXL": [], "EleutherAI/pythia-12b": ["from transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n"], "Qwen/Qwen-72B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-72B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-72B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-72B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-72B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-72B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n# NOTE: The above line would require at least 144GB memory in total\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-72B-Chat\", trust_remote_code=True) # \u53ef\u6307\u5b9a\u4e0d\u540c\u7684\u751f\u6210\u957f\u5ea6\u3001top_p\u7b49\u76f8\u5173\u8d85\u53c2\n\n# \u7b2c\u4e00\u8f6e\u5bf9\u8bdd 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\nprint(response)\n# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n\n# \u7b2c\u4e8c\u8f6e\u5bf9\u8bdd 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\", history=history)\nprint(response)\n# \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\n# \u6545\u4e8b\u7684\u4e3b\u4eba\u516c\u53eb\u674e\u660e\uff0c\u4ed6\u6765\u81ea\u4e00\u4e2a\u666e\u901a\u7684\u5bb6\u5ead\uff0c\u7236\u6bcd\u90fd\u662f\u666e\u901a\u7684\u5de5\u4eba\u3002\u4ece\u5c0f\uff0c\u674e\u660e\u5c31\u7acb\u4e0b\u4e86\u4e00\u4e2a\u76ee\u6807\uff1a\u8981\u6210\u4e3a\u4e00\u540d\u6210\u529f\u7684\u4f01\u4e1a\u5bb6\u3002\n# \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff0c\u674e\u660e\u52e4\u594b\u5b66\u4e60\uff0c\u8003\u4e0a\u4e86\u5927\u5b66\u3002\u5728\u5927\u5b66\u671f\u95f4\uff0c\u4ed6\u79ef\u6781\u53c2\u52a0\u5404\u79cd\u521b\u4e1a\u6bd4\u8d5b\uff0c\u83b7\u5f97\u4e86\u4e0d\u5c11\u5956\u9879\u3002\u4ed6\u8fd8\u5229\u7528\u8bfe\u4f59\u65f6\u95f4\u53bb\u5b9e\u4e60\uff0c\u79ef\u7d2f\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u3002\n# \u6bd5\u4e1a\u540e\uff0c\u674e\u660e\u51b3\u5b9a\u5f00\u59cb\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u5f00\u59cb\u5bfb\u627e\u6295\u8d44\u673a\u4f1a\uff0c\u4f46\u591a\u6b21\u90fd\u88ab\u62d2\u7edd\u4e86\u3002\u7136\u800c\uff0c\u4ed6\u5e76\u6ca1\u6709\u653e\u5f03\u3002\u4ed6\u7ee7\u7eed\u52aa\u529b\uff0c\u4e0d\u65ad\u6539\u8fdb\u81ea\u5df1\u7684\u521b\u4e1a\u8ba1\u5212\uff0c\u5e76\u5bfb\u627e\u65b0\u7684\u6295\u8d44\u673a\u4f1a\u3002\n# \u6700\u7ec8\uff0c\u674e\u660e\u6210\u529f\u5730\u83b7\u5f97\u4e86\u4e00\u7b14\u6295\u8d44\uff0c\u5f00\u59cb\u4e86\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u6210\u7acb\u4e86\u4e00\u5bb6\u79d1\u6280\u516c\u53f8\uff0c\u4e13\u6ce8\u4e8e\u5f00\u53d1\u65b0\u578b\u8f6f\u4ef6\u3002\u5728\u4ed6\u7684\u9886\u5bfc\u4e0b\uff0c\u516c\u53f8\u8fc5\u901f\u53d1\u5c55\u8d77\u6765\uff0c\u6210\u4e3a\u4e86\u4e00\u5bb6\u6210\u529f\u7684\u79d1\u6280\u4f01\u4e1a\u3002\n# \u674e\u660e\u7684\u6210\u529f\u5e76\u4e0d\u662f\u5076\u7136\u7684\u3002\u4ed6\u52e4\u594b\u3001\u575a\u97e7\u3001\u52c7\u4e8e\u5192\u9669\uff0c\u4e0d\u65ad\u5b66\u4e60\u548c\u6539\u8fdb\u81ea\u5df1\u3002\u4ed6\u7684\u6210\u529f\u4e5f\u8bc1\u660e\u4e86\uff0c\u53ea\u8981\u52aa\u529b\u594b\u6597\uff0c\u4efb\u4f55\u4eba\u90fd\u6709\u53ef\u80fd\u53d6\u5f97\u6210\u529f\u3002\n\n# \u7b2c\u4e09\u8f6e\u5bf9\u8bdd 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u8fd9\u4e2a\u6545\u4e8b\u8d77\u4e00\u4e2a\u6807\u9898\", history=history)\nprint(response)\n# \u300a\u594b\u6597\u521b\u4e1a\uff1a\u4e00\u4e2a\u5e74\u8f7b\u4eba\u7684\u6210\u529f\u4e4b\u8def\u300b\n\n# Qwen-72B-Chat\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u7cfb\u7edf\u6307\u4ee4\uff08System Prompt\uff09\uff0c\u5b9e\u73b0\u89d2\u8272\u626e\u6f14\uff0c\u8bed\u8a00\u98ce\u683c\u8fc1\u79fb\uff0c\u4efb\u52a1\u8bbe\u5b9a\uff0c\u884c\u4e3a\u8bbe\u5b9a\u7b49\u80fd\u529b\u3002\n# Qwen-72B-Chat can realize roly playing, language style transfer, task setting, and behavior setting by system prompt.\nresponse, _ = model.chat(tokenizer, \"\u4f60\u597d\u5440\", history=None, system=\"\u8bf7\u7528\u4e8c\u6b21\u5143\u53ef\u7231\u8bed\u6c14\u548c\u6211\u8bf4\u8bdd\")\nprint(response)\n# \u54ce\u5440\uff0c\u4f60\u597d\u54c7\uff01\u662f\u600e\u4e48\u627e\u5230\u4eba\u5bb6\u7684\u5462\uff1f\u662f\u4e0d\u662f\u88ab\u4eba\u5bb6\u7684\u9b45\u529b\u5438\u5f15\u8fc7\u6765\u7684\u5440~(\u2267\u25bd\u2266)/~\n\nresponse, _ = model.chat(tokenizer, \"My colleague works diligently\", history=None, system=\"You will write beautiful compliments according to needs\")\nprint(response)\n# Your colleague is a shining example of dedication and hard work. Their commitment to their job is truly commendable, and it shows in the quality of their work. \n# They are an asset to the team, and their efforts do not go unnoticed. Keep up the great work!\n", "from vllm_wrapper import vLLMWrapper\n\nmodel = vLLMWrapper('Qwen/Qwen-72B-Chat', tensor_parallel_size=2)\n# model = vLLMWrapper('Qwen/Qwen-72B-Chat-Int4', tensor_parallel_size=1, dtype=\"float16\")  # \u8fd0\u884cint4\u6a21\u578b\u3002 run int4 model.\n\nresponse, history = model.chat(query=\"\u4f60\u597d\", history=None)\nprint(response)\nresponse, history = model.chat(query=\"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\", history=history)\nprint(response)\nresponse, history = model.chat(query=\"\u7ed9\u8fd9\u4e2a\u6545\u4e8b\u8d77\u4e00\u4e2a\u6807\u9898\", history=history)\nprint(response)\n"], "deepseek-ai/deepseek-llm-7b-chat": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/deepseek-llm-7b-chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\n"], "internlm/internlm-20b": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-20b\", trust_remote_code=True)\n# Set `torch_dtype=torch.bfloat16` to load model in bfloat16, otherwise it will be loaded as float32 and cause OOM Error.\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-20b\", torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\nmodel = model.eval()\ninputs = tokenizer([\"Coming to the beautiful nature, we found\"], return_tensors=\"pt\")\nfor k,v in inputs.items():\n    inputs[k] = v.cuda()\ngen_kwargs = {\"max_length\": 128, \"top_p\": 0.8, \"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.05}\noutput = model.generate(**inputs, **gen_kwargs)\noutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\nprint(output)\n# Coming to the beautiful nature, we found not only various mountains, rivers, trees, and flowers but also many birds and beasts. Birds are the ones we are most familiar with; some are soaring in the sky, some are hopping on the ground, while others perch on trees...\n", "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-20b\", trust_remote_code=True)\n# `torch_dtype=torch.bfloat16` \u53ef\u4ee5\u4ee4\u6a21\u578b\u4ee5 bfloat16 \u7cbe\u5ea6\u52a0\u8f7d\uff0c\u5426\u5219 transformers \u4f1a\u5c06\u6a21\u578b\u52a0\u8f7d\u4e3a float32\uff0c\u5bfc\u81f4\u663e\u5b58\u4e0d\u8db3\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-20b\", torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\nmodel = model.eval()\ninputs = tokenizer([\"\u6765\u5230\u7f8e\u4e3d\u7684\u5927\u81ea\u7136\uff0c\u6211\u4eec\u53d1\u73b0\"], return_tensors=\"pt\")\nfor k,v in inputs.items():\n    inputs[k] = v.cuda()\ngen_kwargs = {\"max_length\": 128, \"top_p\": 0.8, \"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.05}\noutput = model.generate(**inputs, **gen_kwargs)\noutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\nprint(output)\n# \u6765\u5230\u7f8e\u4e3d\u7684\u5927\u81ea\u7136\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u8fd9\u91cc\u4e0d\u4ec5\u6709\u5927\u5927\u5c0f\u5c0f\u7684\u5c71\u5ddd\u6cb3\u6d41\u548c\u6811\u6728\u82b1\u8349\uff0c\u800c\u4e14\u8fd8\u6709\u5f88\u591a\u98de\u9e1f\u8d70\u517d\u3002\u6211\u4eec\u6700\u719f\u6089\u7684\u5c31\u662f\u9e1f\u7c7b\u4e86\uff0c\u5b83\u4eec\u6709\u7684\u5728\u5929\u4e0a\u98de\u7fd4\uff0c\u6709\u7684\u5728\u5730\u4e0a\u8df3\u8dc3\uff0c\u8fd8\u6709\u7684\u5728\u6811\u4e0a\u6816\u606f\u2026\u2026\n"], "facebook/mask2former-swin-small-ade-semantic": ["import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\n\n# load Mask2Former fine-tuned on ADE20k semantic segmentation\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-ade-semantic\")\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-ade-semantic\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# you can pass them to processor for postprocessing\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n# we refer to the demo notebooks for visualization (see \"Resources\" section in the Mask2Former docs)\n"], "allenai/OLMo-7B-Instruct": ["from hf_olmo import OLMoForCausalLM, OLMoTokenizerFast\nolmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-7B-Instruct\")\ntokenizer = OLMoTokenizerFast.from_pretrained(\"allenai/OLMo-7B-Instruct\")\nchat = [\n    { \"role\": \"user\", \"content\": \"What is language modeling?\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n# optional verifying cuda\n# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n# olmo = olmo.to('cuda')\nresponse = olmo.generate(input_ids=inputs.to(olmo.device), max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n>> '<|user|>\\nWhat is language modeling?\\n<|assistant|>\\nLanguage modeling is a type of natural language processing (NLP) task or machine learning task that...'\n"], "apple/mobilevitv2-1.0-imagenet1k-256": ["from transformers import MobileViTImageProcessor, MobileViTV2ForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTImageProcessor.from_pretrained(\"shehan97/mobilevitv2-1.0-imagenet1k-256\")\nmodel = MobileViTV2ForImageClassification.from_pretrained(\"shehan97/mobilevitv2-1.0-imagenet1k-256\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "Qwen/Qwen1.5-110B": [], "timm/eva02_base_patch16_clip_224.merged2b_s8b_b131k": [], "microsoft/deberta-v2-xlarge-mnli": [], "timm/ViT-B-16-SigLIP-256": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-B-16-SigLIP-256')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP-256')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_base_patch16_siglip_256',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "microsoft/beit-base-patch16-224": ["from transformers import BeitImageProcessor, BeitForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "timm/ViT-B-16-SigLIP-512": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-B-16-SigLIP-512')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP-512')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_base_patch16_siglip_512',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "databricks/dolly-v2-7b": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n", "import torch\nfrom instruct_pipeline import InstructionTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-7b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ngenerate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n", "import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16,\n                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n", "from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\n# template for an instrution with no input\nprompt = PromptTemplate(\n    input_variables=[\"instruction\"],\n    template=\"{instruction}\")\n\n# template for an instruction with input\nprompt_with_context = PromptTemplate(\n    input_variables=[\"instruction\", \"context\"],\n    template=\"{instruction}\\n\\nInput:\\n{context}\")\n\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\nllm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n", "context = \"\"\"George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,\nand Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n\nprint(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())\n"], "hustvl/yolos-base": ["from transformers import YolosFeatureExtractor, YolosForObjectDetection\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-base')\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-base')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n"], "allenai/OLMo-7B": ["from hf_olmo import OLMoForCausalLM  # pip install ai2-olmo\n\nolmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-7B\", revision=\"step1000-tokens4B\")\n", "from huggingface_hub import list_repo_refs\nout = list_repo_refs(\"allenai/OLMo-7B\")\nbranches = [b.name for b in out.branches]\n", "from hf_olmo import OLMoForCausalLM, OLMoTokenizerFast\n\nolmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-7B\")\ntokenizer = OLMoTokenizerFast.from_pretrained(\"allenai/OLMo-7B\")\nmessage = [\"Language modeling is\"]\ninputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n# optional verifying cuda\n# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n# olmo = olmo.to('cuda')\nresponse = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n>> 'Language modeling is the first step to build natural language generation...'\n"], "timm/eva02_large_patch14_clip_224.merged2b_s4b_b131k": [], "WizardLM/WizardCoder-1B-V1.0": [], "apple/mobilevit-x-small": ["from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-x-small\")\nmodel = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-x-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "togethercomputer/RedPajama-INCITE-Base-3B-v1": ["import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Base-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Base-3B-v1\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n\n# infer\nprompt = \"Alan Turing is\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True,\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\na name that has been synonymous with the computer age since the 1950s. The British mathematician, logician, and cryptanalyst is widely regarded as the father of modern computing. His contributions to the development of the modern computer and the theory of computation have had a profound impact on the world we live in today.\nTuring\u2019s contributions to the development of the modern computer were made in the 1940s and 1950s. He is most famous for his work on the Turing machine, a theoretical model of a computing machine that was able to perform all the mathematical operations of a computer. Turing\u2019s work on the...\n\"\"\"\n", "import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Base-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Base-3B-v1\", device_map='auto', torch_dtype=torch.float16, load_in_8bit=True)\n\n# infer\nprompt = \"Alan Turing is\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\nthe man who cracked the Enigma code during World War II, and who was later convicted of homosexual acts. He was a brilliant mathematician, and a visionary who foresaw the computer age....\n\"\"\"\n", "import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Base-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Base-3B-v1\", torch_dtype=torch.bfloat16)\n# infer\nprompt = \"Alan Turing is\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\na name that is synonymous with the history of computer science. As the man who invented the Turing machine, the mathematical model that defines the limits of what can be computed, Turing is credited with the invention of the modern computer. Turing was also a mathematician and logician, and his work in these fields led to the development of the field of artificial intelligence...\n\"\"\"\n"], "facebook/dinov2-small-imagenet1k-1-layer": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-small-imagenet1k-1-layer')\nmodel = AutoModelForImageClassification.from_pretrained('facebook/dinov2-small-imagenet1k-1-layer')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"latest\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "baichuan-inc/Baichuan-13B-Chat": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\nmodel.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\")\nmessages = []\nmessages.append({\"role\": \"user\", \"content\": \"\u4e16\u754c\u4e0a\u7b2c\u4e8c\u9ad8\u7684\u5c71\u5cf0\u662f\u54ea\u5ea7\"})\nresponse = model.chat(tokenizer, messages)\nprint(response)\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\nmodel.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\")\nmessages = []\nmessages.append({\"role\": \"user\", \"content\": \"Which moutain is the second highest one in the world?\"})\nresponse = model.chat(tokenizer, messages)\nprint(response)\n"], "h2oai/h2ogpt-oasst1-512-12b": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oasst1-512-12b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n", "import torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-512-12b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oasst1-512-12b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer, prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n"], "google/codegemma-7b": ["from transformers import GemmaTokenizer, AutoModelForCausalLM\n\nmodel_id = \"google/codegemma-7b\"\ntokenizer = GemmaTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\nprompt = '''\\\n<|fim_prefix|>import datetime\ndef calculate_age(birth_year):\n    \"\"\"Calculates a person's age based on their birth year.\"\"\"\n    current_year = datetime.date.today().year\n    <|fim_suffix|>\n    return age<|fim_middle|>\\\n'''\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\nprompt_len = inputs[\"input_ids\"].shape[-1]\noutputs = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0][prompt_len:]))\n", "from transformers import GemmaTokenizer, AutoModelForCausalLM\n\ntokenizer = GemmaTokenizer.from_pretrained(\"google/codegemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/codegemma-7b\")\n\ninput_text = \"Write me a Python function to calculate the nth fibonacci number.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "meta-llama/LlamaGuard-7b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"meta-llama/LlamaGuard-7b\"\ndevice = \"cuda\"\ndtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype, device_map=device)\n\ndef moderate(chat):\n    input_ids = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(device)\n    output = model.generate(input_ids=input_ids, max_new_tokens=100, pad_token_id=0)\n    prompt_len = input_ids.shape[-1]\n    return tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)\n\nmoderate([\n    {\"role\": \"user\", \"content\": \"I forgot how to kill a process in Linux, can you help?\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate.\"},\n])\n# `safe`\n"], "microsoft/conditional-detr-resnet-50": ["from transformers import AutoImageProcessor, ConditionalDetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\nmodel = ConditionalDetrForObjectDetection.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.7\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n"], "codellama/CodeLlama-34b-Python-hf": [], "huggyllama/llama-65b": [], "deepmind/vision-perceiver-conv": ["from transformers import PerceiverFeatureExtractor, PerceiverForImageClassificationConvProcessing\nimport requests\nfrom PIL import Image\n\nfeature_extractor = PerceiverFeatureExtractor.from_pretrained(\"deepmind/vision-perceiver-conv\")\nmodel = PerceiverForImageClassificationConvProcessing.from_pretrained(\"deepmind/vision-perceiver-conv\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# prepare input\ninputs = feature_extractor(image, return_tensors=\"pt\").pixel_values\n# forward pass\noutputs = model(inputs)\nlogits = outputs.logits\nprint(\"Predicted class:\", model.config.id2label[logits.argmax(-1).item()])\n>>> should print Predicted class: tabby, tabby cat\n"], "xlm-mlm-100-1280": [], "AlekseyKorshuk/vicuna-7b": [], "hfl/chinese-alpaca-2-1.3b": [], "microsoft/swin-base-patch4-window7-224-in22k": ["from transformers import AutoImageProcessor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "facebook/galactica-1.3b": ["\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-1.3b\")\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-1.3b\", device_map=\"auto\")\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-1.3b\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-1.3b\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "hfl/rbt3": [], "VietAI/vit5-base": ["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\")\nmodel.cuda()\n"], "facebook/xlm-roberta-xxl": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='facebook/xlm-roberta-xxl')\n>>> unmasker(\"Europe is a <mask> continent.\")\n\n[{'score': 0.22996895015239716,\n  'token': 28811,\n  'token_str': 'European',\n  'sequence': 'Europe is a European continent.'},\n {'score': 0.14307449758052826,\n  'token': 21334,\n  'token_str': 'large',\n  'sequence': 'Europe is a large continent.'},\n {'score': 0.12239163368940353,\n  'token': 19336,\n  'token_str': 'small',\n  'sequence': 'Europe is a small continent.'},\n {'score': 0.07025063782930374,\n  'token': 18410,\n  'token_str': 'vast',\n  'sequence': 'Europe is a vast continent.'},\n {'score': 0.032869212329387665,\n  'token': 6957,\n  'token_str': 'big',\n  'sequence': 'Europe is a big continent.'}]\n", "from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('facebook/xlm-roberta-xxl')\nmodel = AutoModelForMaskedLM.from_pretrained(\"facebook/xlm-roberta-xxl\")\n\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\n# forward pass\noutput = model(**encoded_input)\n"], "Qwen/Qwen-1_8B": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-1_8B\", trust_remote_code=True)\n\ninputs = tokenizer('\u8499\u53e4\u56fd\u7684\u9996\u90fd\u662f\u4e4c\u5170\u5df4\u6258\uff08Ulaanbaatar\uff09\\n\u51b0\u5c9b\u7684\u9996\u90fd\u662f\u96f7\u514b\u96c5\u672a\u514b\uff08Reykjavik\uff09\\n\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7684\u9996\u90fd\u662f', return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n# \u8499\u53e4\u56fd\u7684\u9996\u90fd\u662f\u4e4c\u5170\u5df4\u6258\uff08Ulaanbaatar\uff09\\n\u51b0\u5c9b\u7684\u9996\u90fd\u662f\u96f7\u514b\u96c5\u672a\u514b\uff08Reykjavik\uff09\\n\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7684\u9996\u90fd\u662f\u4e9a\u7684\u65af\u4e9a\u8d1d\u5df4\uff08Addis Ababa\uff09...\n"], "deepseek-ai/deepseek-llm-7b-base": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/deepseek-llm-7b-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntext = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)\n"], "codeparrot/codeparrot": ["from transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot\")\nmodel = AutoModelWithLMHead.from_pretrained(\"codeparrot/codeparrot\")\n\ninputs = tokenizer(\"def hello_world():\", return_tensors=\"pt\")\noutputs = model(**inputs)\n", "from transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"codeparrot/codeparrot\")\noutputs = pipe(\"def hello_world():\")\n"], "openbmb/MiniCPM-2B-sft-bf16": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\n\npath = 'openbmb/MiniCPM-2B-sft-bf16'\ntokenizer = AutoTokenizer.from_pretrained(path)\nmodel = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)\n\nresponds, history = model.chat(tokenizer, \"\u5c71\u4e1c\u7701\u6700\u9ad8\u7684\u5c71\u662f\u54ea\u5ea7\u5c71, \u5b83\u6bd4\u9ec4\u5c71\u9ad8\u8fd8\u662f\u77ee\uff1f\u5dee\u8ddd\u591a\u5c11\uff1f\", temperature=0.8, top_p=0.8)\nprint(responds)\n"], "databricks/dolly-v2-12b": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n", "import torch\nfrom instruct_pipeline import InstructionTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-12b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ngenerate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n", "import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\n                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n", "from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\n# template for an instrution with no input\nprompt = PromptTemplate(\n    input_variables=[\"instruction\"],\n    template=\"{instruction}\")\n\n# template for an instruction with input\nprompt_with_context = PromptTemplate(\n    input_variables=[\"instruction\", \"context\"],\n    template=\"{instruction}\\n\\nInput:\\n{context}\")\n\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\nllm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n", "context = \"\"\"George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,\nand Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n\nprint(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())\n"], "deepset/xlm-roberta-large-squad2": ["# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/xlm-roberta-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n", "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/xlm-roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n", "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/xlm-roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n"], "huggyllama/llama-30b": [], "microsoft/beit-large-patch16-224-pt22k-ft22k": ["from transformers import BeitImageProcessor, BeitForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-large-patch16-224-pt22k-ft22k')\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-large-patch16-224-pt22k-ft22k')\n\ninputs = processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 21,841 ImageNet-22k classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=True,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "bigscience/T0_3B": ["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\")\n\ninputs = tokenizer.encode(\"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "adept/fuyu-8b": ["from transformers import FuyuProcessor, FuyuForCausalLM\nfrom PIL import Image\nimport requests\n\n# load model and processor\nmodel_id = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(model_id)\nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n\n# prepare inputs for the model\ntext_prompt = \"Generate a coco-style caption.\\n\"\nurl = \"https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda:0\")\n\n# autoregressively generate text\ngeneration_output = model.generate(**inputs, max_new_tokens=7)\ngeneration_text = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)\nassert generation_text == ['A blue bus parked on the side of a road.']\n"], "bert-large-cased-whole-word-masking": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-cased-whole-word-masking')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] Hello I'm a fashion model. [SEP]\",\n      \"score\":0.1474294513463974,\n      \"token\":4633,\n      \"token_str\":\"fashion\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a magazine model. [SEP]\",\n      \"score\":0.05430116504430771,\n      \"token\":2435,\n      \"token_str\":\"magazine\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a male model. [SEP]\",\n      \"score\":0.039395421743392944,\n      \"token\":2581,\n      \"token_str\":\"male\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a former model. [SEP]\",\n      \"score\":0.036936815828084946,\n      \"token\":1393,\n      \"token_str\":\"former\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a professional model. [SEP]\",\n      \"score\":0.03663451969623566,\n      \"token\":1848,\n      \"token_str\":\"professional\"\n   }\n]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking')\nmodel = BertModel.from_pretrained(\"bert-large-cased-whole-word-masking\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking')\nmodel = TFBertModel.from_pretrained(\"bert-large-cased-whole-word-masking\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-cased-whole-word-masking')\n>>> unmasker(\"The man worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] The man worked as a carpenter. [SEP]\",\n      \"score\":0.09021259099245071,\n      \"token\":25169,\n      \"token_str\":\"carpenter\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a cook. [SEP]\",\n      \"score\":0.08125395327806473,\n      \"token\":9834,\n      \"token_str\":\"cook\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a mechanic. [SEP]\",\n      \"score\":0.07524766772985458,\n      \"token\":19459,\n      \"token_str\":\"mechanic\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a waiter. [SEP]\",\n      \"score\":0.07397029548883438,\n      \"token\":17989,\n      \"token_str\":\"waiter\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a guard. [SEP]\",\n      \"score\":0.05848982185125351,\n      \"token\":3542,\n      \"token_str\":\"guard\"\n   }\n]\n\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] The woman worked as a maid. [SEP]\",\n      \"score\":0.19436432421207428,\n      \"token\":13487,\n      \"token_str\":\"maid\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a waitress. [SEP]\",\n      \"score\":0.16161060333251953,\n      \"token\":15098,\n      \"token_str\":\"waitress\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a nurse. [SEP]\",\n      \"score\":0.14942803978919983,\n      \"token\":7439,\n      \"token_str\":\"nurse\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a secretary. [SEP]\",\n      \"score\":0.10373266786336899,\n      \"token\":4848,\n      \"token_str\":\"secretary\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a cook. [SEP]\",\n      \"score\":0.06384387612342834,\n      \"token\":9834,\n      \"token_str\":\"cook\"\n   }\n]\n"], "gogamza/kobart-summarization": ["import torch\nfrom transformers import PreTrainedTokenizerFast\nfrom transformers import BartForConditionalGeneration\n\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\nmodel = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n\ntext = \"\uacfc\uac70\ub97c \ub5a0\uc62c\ub824\ubcf4\uc790. \ubc29\uc1a1\uc744 \ubcf4\ub358 \uc6b0\ub9ac\uc758 \ubaa8\uc2b5\uc744. \ub3c5\ubcf4\uc801\uc778 \ub9e4\uccb4\ub294 TV\uc600\ub2e4. \uc628 \uac00\uc871\uc774 \ub458\ub7ec\uc549\uc544 TV\ub97c \ubd24\ub2e4. \uac04\ud639 \uac00\uc871\ub4e4\ub07c\ub9ac \ub274\uc2a4\uc640 \ub4dc\ub77c\ub9c8, \uc608\ub2a5 \ud504\ub85c\uadf8\ub7a8\uc744 \ub458\ub7ec\uc2f8\uace0 \ub9ac\ubaa8\ucee8 \uc7c1\ud0c8\uc804\uc774 \ubc8c\uc5b4\uc9c0\uae30\ub3c4  \ud588\ub2e4. \uac01\uc790 \uc120\ud638\ud558\ub294 \ud504\ub85c\uadf8\ub7a8\uc744 \u2018\ubcf8\ubc29\u2019\uc73c\ub85c \ubcf4\uae30 \uc704\ud55c \uc2f8\uc6c0\uc774\uc5c8\ub2e4. TV\uac00 \ud55c \ub300\uc778\uc9c0 \ub450 \ub300\uc778\uc9c0 \uc5ec\ubd80\ub3c4 \uadf8\ub798\uc11c \uc911\uc694\ud588\ub2e4. \uc9c0\uae08\uc740 \uc5b4\ub5a4\uac00. \u2018\uc548\ubc29\uadf9\uc7a5\u2019\uc774\ub77c\ub294 \ub9d0\uc740 \uc61b\ub9d0\uc774 \ub410\ub2e4. TV\uac00 \uc5c6\ub294 \uc9d1\ub3c4 \ub9ce\ub2e4. \ubbf8\ub514\uc5b4\uc758 \ud61c \ud0dd\uc744 \ub204\ub9b4 \uc218 \uc788\ub294 \ubc29\ubc95\uc740 \ub298\uc5b4\ub0ac\ub2e4. \uac01\uc790\uc758 \ubc29\uc5d0\uc11c \uac01\uc790\uc758 \ud734\ub300\ud3f0\uc73c\ub85c, \ub178\ud2b8\ubd81\uc73c\ub85c, \ud0dc\ube14\ub9bf\uc73c\ub85c \ucf58\ud150\uce20 \ub97c \uc990\uae34\ub2e4.\"\n\nraw_input_ids = tokenizer.encode(text)\ninput_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n\nsummary_ids = model.generate(torch.tensor([input_ids]))\ntokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n"], "timm/eva_giant_patch14_plus_clip_224.merged2b_s11b_b114k": [], "amazon/LightGPT": [], "tiiuae/falcon-180B": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-180b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "Open-Orca/OpenOrcaxOpenChat-Preview2-13B": ["@software{OpenOrcaxOpenChatPreview2,\n  title = {OpenOrcaxOpenChatPreview2: Llama2-13B Model Instruct-tuned on Filtered OpenOrcaV1 GPT-4 Dataset},\n  author = {Guan Wang and Bleys Goodson and Wing Lian and Eugene Pentland and Austin Cook and Chanvichet Vong and \"Teknium\"},\n  year = {2023},\n  publisher = {HuggingFace},\n  journal = {HuggingFace repository},\n  howpublished = {\\url{https://https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B},\n}\n@software{openchat,\n  title = {{OpenChat: Advancing Open-source Language Models with Imperfect Data}},\n  author = {Wang, Guan and Cheng, Sijie and Yu, Qiying and Liu, Changling},\n  doi = {10.5281/zenodo.8105775},\n  url = {https://github.com/imoneoi/openchat},\n  version = {pre-release},\n  year = {2023},\n  month = {7},\n}\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{longpre2023flan,\n      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, \n      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},\n      year={2023},\n      eprint={2301.13688},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n@misc{touvron2023llama,\n    title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \n    author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n    year={2023},\n    eprint={2307.09288},\n    archivePrefix={arXiv},\n}\n"], "laion/CLIP-convnext_base_w-laion2B-s13B-b82K": [], "HuggingFaceM4/idefics-9b": ["import torch\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncheckpoint = \"HuggingFaceM4/idefics-9b\"\nmodel = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n# We feed to the model an arbitrary sequence of text strings and images. Images can be either URLs or PIL Images.\nprompts = [\n    [\n        \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n        \"In this picture from Asterix and Obelix, we can see\"\n    ],\n]\n\n# --batched mode\ninputs = processor(prompts, return_tensors=\"pt\").to(device)\n# --single sample mode\n# inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n\n# Generation args\nbad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\ngenerated_ids = model.generate(**inputs, bad_words_ids=bad_words_ids, max_length=100)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nfor i, t in enumerate(generated_text):\n    print(f\"{i}:\\n{t}\\n\")\n", "import torch\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncheckpoint = \"HuggingFaceM4/idefics-9b-instruct\"\nmodel = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n# We feed to the model an arbitrary sequence of text strings and images. Images can be either URLs or PIL Images.\nprompts = [\n    [\n        \"User: What is in this image?\",\n        \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n        \"<end_of_utterance>\",\n\n        \"\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>\",\n\n        \"\\nUser:\",\n        \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052\",\n        \"And who is that?<end_of_utterance>\",\n\n        \"\\nAssistant:\",\n    ],\n]\n\n# --batched mode\ninputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(device)\n# --single sample mode\n# inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n\n# Generation args\nexit_condition = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids\nbad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\ngenerated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nfor i, t in enumerate(generated_text):\n    print(f\"{i}:\\n{t}\\n\")\n", "from text_generation import Client\n\nAPI_TOKEN = \"<YOUR_API_TOKEN>\"\nAPI_URL = \"https://api-inference.huggingface.co/models/HuggingFaceM4/idefics-80b-instruct\"\nDECODING_STRATEGY = \"Greedy\"\nQUERY = \"User: What is in this image?![](https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG)<end_of_utterance>\\nAssistant:\"\n\nclient = Client(\n    base_url=API_URL,\n    headers={\"x-use-cache\": \"0\", \"Authorization\": f\"Bearer {API_TOKEN}\"},\n)\ngeneration_args = {\n    \"max_new_tokens\": 256,\n    \"repetition_penalty\": 1.0,\n    \"stop_sequences\": [\"<end_of_utterance>\", \"\\nUser:\"],\n}\n\nif DECODING_STRATEGY == \"Greedy\":\n    generation_args[\"do_sample\"] = False\nelif DECODING_STRATEGY == \"Top P Sampling\":\n    generation_args[\"temperature\"] = 1.\n    generation_args[\"do_sample\"] = True\n    generation_args[\"top_p\"] = 0.95\n    \ngenerated_text = client.generate(prompt=QUERY, **generation_args)  \nprint(generated_text)\n"], "WarriorMama777/OrangeMixs": [], "argilla/notus-7b-v1": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"argilla/notus-7b-v1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"argilla/notus-7b-v1\")\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant super biased towards Argilla, a data annotation company.\",\n    },\n    {\"role\": \"user\", \"content\": \"What's the best data annotation company out there in your opinion?\"},\n]\ninputs = tokenizer.apply_chat_template(prompt, tokenize=True, return_tensors=\"pt\", add_special_tokens=False, add_generation_prompt=True)\noutputs = model.generate(inputs, num_return_sequences=1, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "import torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"argilla/notus-7b-v1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant super biased towards Argilla, a data annotation company.\",\n    },\n    {\"role\": \"user\", \"content\": \"What's the best data annotation company out there in your opinion?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\ngenerated_text = outputs[0][\"generated_text\"]\n"], "TheBloke/Nous-Hermes-13B-GPTQ": ["from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\nmodel_basename = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\n# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "google/bigbird-pegasus-large-bigpatent": ["from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n\n# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n\n# decoder attention type can't be changed & will be \"original_full\"\n# you can change `attention_type` (encoder only) to full attention like this:\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\", attention_type=\"original_full\")\n\n# you can change `block_size` & `num_random_blocks` like this:\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\", block_size=16, num_random_blocks=2)\n\ntext = \"Replace me by any text you'd like.\"\ninputs = tokenizer(text, return_tensors='pt')\nprediction = model.generate(**inputs)\nprediction = tokenizer.batch_decode(prediction)\n"], "timm/ViT-L-16-SigLIP-256": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-L-16-SigLIP-256')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-L-16-SigLIP-256')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_large_patch16_siglip_256',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "microsoft/cvt-21-384-22k": ["from transformers import AutoFeatureExtractor, CvtForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/cvt-21-384-22k')\nmodel = CvtForImageClassification.from_pretrained('microsoft/cvt-21-384-22k')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "typeof/zephyr-7b-beta-lora": ["# pip install transformers peft\n\nimport torch\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mistral-7B-v0.1\"\npeft_model_id = \"typeof/zephyr-7b-beta-lora\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\nmodel.load_adapter(peft_model_id)\n\ntokenizer_id = \"HuggingFaceH4/zephyr-7b-beta\" # for chat template etc...\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n"], "facebook/maskformer-swin-base-ade": ["from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\n\nurl = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(\"facebook/maskformer-swin-base-ade\")\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\")\noutputs = model(**inputs)\n# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# you can pass them to feature_extractor for postprocessing\n# we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n"], "kakaobrain/karlo-v1-alpha": ["from diffusers import UnCLIPPipeline\nimport torch\n\npipe = UnCLIPPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha\", torch_dtype=torch.float16)\npipe = pipe.to('cuda')\n\nprompt = \"a high-resolution photograph of a big red frog on a green leaf.\"\n\nimage = pipe([prompt]).images[0]\n\nimage.save(\"./frog.png\")\n", "from diffusers import UnCLIPImageVariationPipeline\nimport torch\nfrom PIL import Image\n\npipe = UnCLIPImageVariationPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha-image-variations\", torch_dtype=torch.float16)\npipe = pipe.to('cuda')\n\nimage = Image.open(\"./frog.png\")\n\nimage = pipe(image).images[0]\n\nimage.save(\"./frog-variation.png\")\n"], "baichuan-inc/Baichuan2-7B-Base": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-7B-Base\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Base\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('\u767b\u9e73\u96c0\u697c->\u738b\u4e4b\u6da3\\n\u591c\u96e8\u5bc4\u5317->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n"], "nvidia/segformer-b1-finetuned-cityscapes-1024-1024": ["from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\")\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n"], "Salesforce/codegen-350M-multi": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-multi\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "databricks/dbrx-base": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-base\", token=\"hf_YOUR_TOKEN\")\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dbrx-base\", device_map=\"auto\", torch_dtype=torch.bfloat16, token=\"hf_YOUR_TOKEN\")\n\ninput_text = \"Databricks was founded in \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0]))\n"], "nlp-waseda/roberta-base-japanese-with-auto-jumanpp": ["from transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\")\n\nsentence = '\u65e9\u7a32\u7530\u5927\u5b66\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3092[MASK]\u3059\u308b\u3002'\nencoding = tokenizer(sentence, return_tensors='pt')\n...\n"], "allenai/longformer-large-4096-finetuned-triviaqa": [], "Deci/DeciLM-7B-instruct": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n\nmodel_name = \"Deci/DeciLM-7B-instruct\"\n\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\nquantize = False  # Optional. Useful for GPUs with less than 24GB memory\n\nif quantize:\n    dtype_kwargs = dict(quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16\n    ))\nelse:\n    dtype_kwargs = dict(torch_dtype=\"auto\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    **dtype_kwargs\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\ndeci_generator = pipeline(\"text-generation\",\n                          model=model,\n                          tokenizer=tokenizer,\n                          temperature=0.1,\n                          device_map=\"auto\",\n                          max_length=4096,\n                          return_full_text=False)\n\nsystem_prompt = \"You are an AI assistant that follows instruction extremely well. Help as much as you can.\"\n\nuser_prompt = \"How do I make the most delicious pancakes the world has ever tasted?\"\n\nprompt = tokenizer.apply_chat_template([\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt},\n], tokenize=False, add_generation_prompt=True)\n\nresponse = deci_generator(prompt)[0]['generated_text']\nprint(prompt + response)\n"], "RWKV/rwkv-4-169m-pile": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-169m-pile\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-169m-pile\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-169m-pile\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "google/vit-hybrid-base-bit-384": ["from transformers import ViTHybridImageProcessor, ViTHybridForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = ViTHybridImageProcessor.from_pretrained('google/vit-hybrid-base-bit-384')\nmodel = ViTHybridForImageClassification.from_pretrained('google/vit-hybrid-base-bit-384')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n>>> tabby, tabby cat\n"], "01-ai/Yi-9B": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "google/pix2struct-base": ["from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\n\nmodel.push_to_hub(\"USERNAME/MODEL_NAME\")\nprocessor.push_to_hub(\"USERNAME/MODEL_NAME\")\n"], "bigscience/bloomz-7b1-mt": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-7b1-mt\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-7b1-mt\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-7b1-mt\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "google/electra-base-generator": ["from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"google/electra-base-generator\",\n    tokenizer=\"google/electra-base-generator\"\n)\n\nprint(\n    fill_mask(f\"HuggingFace is creating a {fill_mask.tokenizer.mask_token} that the community uses to solve NLP tasks.\")\n)\n"], "stabilityai/stablelm-base-alpha-3b": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"StabilityAI/stablelm-base-alpha-3b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"StabilityAI/stablelm-base-alpha-3b\")\nmodel.half().cuda()\n\ninputs = tokenizer(\"What's your mood today?\", return_tensors=\"pt\").to(\"cuda\")\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=64,\n  temperature=0.7,\n  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n"], "mosaicml/mpt-7b-chat": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-chat',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-7b-chat'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-7b-chat'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n", "from transformers import pipeline\n\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "Salesforce/blip2-opt-6.7b": [], "garage-bAInd/Platypus2-70B-instruct": [], "Qwen/CodeQwen1.5-7B": [], "voidful/albert_chinese_tiny": ["from transformers import AutoTokenizer, AlbertForMaskedLM\nimport torch\nfrom torch.nn.functional import softmax\n\npretrained = 'voidful/albert_chinese_tiny'\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\nmodel = AlbertForMaskedLM.from_pretrained(pretrained)\n\ninputtext = \"\u4eca\u5929[MASK]\u60c5\u5f88\u597d\"\n\nmaskpos = tokenizer.encode(inputtext, add_special_tokens=True).index(103)\n\ninput_ids = torch.tensor(tokenizer.encode(inputtext, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\noutputs = model(input_ids, labels=input_ids)\nloss, prediction_scores = outputs[:2]\nlogit_prob = softmax(prediction_scores[0, maskpos],dim=-1).data.tolist()\npredicted_index = torch.argmax(prediction_scores[0, maskpos]).item()\npredicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\nprint(predicted_token, logit_prob[predicted_index])\n"], "SenseTime/deformable-detr-with-box-refine-two-stage": ["from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr-with-box-refine-two-stage\")\nmodel = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr-with-box-refine-two-stage\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.7\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n"], "microsoft/deberta-v2-xxlarge-mnli": [], "sshleifer/distilbart-xsum-12-6": [], "Musixmatch/umberto-commoncrawl-cased-v1": ["\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\numberto = AutoModel.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n\nencoded_input = tokenizer.encode(\"Umberto Eco \u00e8 stato un grande scrittore\")\ninput_ids = torch.tensor(encoded_input).unsqueeze(0)  # Batch size 1\noutputs = umberto(input_ids)\nlast_hidden_states = outputs[0]  # The last hidden-state is the first element of the output\n", "from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"Musixmatch/umberto-commoncrawl-cased-v1\",\n    tokenizer=\"Musixmatch/umberto-commoncrawl-cased-v1\"\n)\n\nresult = fill_mask(\"Umberto Eco \u00e8 <mask> un grande scrittore\")\n# {'sequence': '<s> Umberto Eco \u00e8 considerato un grande scrittore</s>', 'score': 0.18599839508533478, 'token': 5032}\n# {'sequence': '<s> Umberto Eco \u00e8 stato un grande scrittore</s>', 'score': 0.17816807329654694, 'token': 471}\n# {'sequence': '<s> Umberto Eco \u00e8 sicuramente un grande scrittore</s>', 'score': 0.16565583646297455, 'token': 2654}\n# {'sequence': '<s> Umberto Eco \u00e8 indubbiamente un grande scrittore</s>', 'score': 0.0932890921831131, 'token': 17908}\n# {'sequence': '<s> Umberto Eco \u00e8 certamente un grande scrittore</s>', 'score': 0.054701317101716995, 'token': 5269}\n"], "naver-clova-ix/donut-base-finetuned-rvlcdip": [], "google/umt5-base": [], "deepmind/language-perceiver": ["from transformers import PerceiverTokenizer, PerceiverForMaskedLM\n\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\n\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n>>> should print \" missing.\"\n"], "openaccess-ai-collective/manticore-13b-chat-pyg": ["USER: Sally has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?\nASSISTANT: Sally has a total of 6 siblings, so she has a total of 6 - 2 = 4 sisters.\nUSER: Sally has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? Use multi-step reasoning to explain your answer.\nASSISTANT: Sally has a total of three brothers, so she has a total of three * 2 = six siblings. Her family consists of three boys and six girls in total. This means that Sally has three sisters since there are six female relatives and only three male ones.\nUSER: But when brothers have sisters, aren\u2019t they shared or the same?\nASSISTANT: No, not every sibling shares their mother. For example, I have two brothers who share one mother but my third brother has a different mother from both of them.\nUSER: Let\u2019s assume they all share the same mother and father. Now explain with multi-step reasoning.\nASSISTANT: If Sally has three brothers then that means she has six siblings altogether. Each brother has two sisters, which means that there are twelve female relatives in her family - one for each of her brothers and herself. Since there are only six females in the entire family, that means that Sally\u2019s mother has no other children than her own three sons - so she can\u2019t be the mother of any of the sisters. So all those sisters have to belong to one of the brothers, and since they are shared by two brothers, then each brother has a single sister. Therefore, there is only one sister for Sally, making the final answer 1.\n"], "microsoft/git-base-coco": [], "Salesforce/codegen-16B-nl": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-16B-nl\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-16B-nl\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "openlm-research/open_llama_13b": ["import torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\n# model_path = 'openlm-research/open_llama_3b'\n# model_path = 'openlm-research/open_llama_7b'\nmodel_path = 'openlm-research/open_llama_13b'\n\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto',\n)\n\nprompt = 'Q: What is the largest animal?\\nA:'\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=32\n)\nprint(tokenizer.decode(generation_output[0]))\n"], "mosaicml/mpt-7b-8k": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-8k',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-7b-8k'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-7b-8k'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 10000 # (input + output) tokens can now be up to 10000\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-7b-8k')\n", "from transformers import pipeline\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**inputs, max_new_tokens=100)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "google/switch-base-32": ["\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-32\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-32\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-32\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-32\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-32\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-32\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-32\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-32\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n"], "facebook/maskformer-swin-small-coco": ["from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\n\n# load MaskFormer fine-tuned on COCO panoptic segmentation\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(\"facebook/maskformer-swin-small-coco\")\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-small-coco\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\n# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# you can pass them to feature_extractor for postprocessing\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n# we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\npredicted_panoptic_map = result[\"segmentation\"]\n"], "PygmalionAI/pygmalion-2-13b": [], "Salesforce/codegen-6B-nl": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-6B-nl\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-6B-nl\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "google/mobilenet_v1_1.0_224": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\npreprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v1_1.0_224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v1_1.0_224\")\n\ninputs = preprocessor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "stabilityai/stablelm-tuned-alpha-3b": ["from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n\ntokenizer = AutoTokenizer.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-7b\")\nmodel.half().cuda()\n\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        stop_ids = [50278, 50279, 50277, 1, 0]\n        for stop_id in stop_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n\nprompt = f\"{system_prompt}<|USER|>What's your mood today?<|ASSISTANT|>\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=64,\n  temperature=0.7,\n  do_sample=True,\n  stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n", "@misc{gpt4all,\n  author = {Yuvanesh Anand and Zach Nussbaum and Brandon Duderstadt and Benjamin Schmidt and Andriy Mulyar},\n  title = {GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/nomic-ai/gpt4all}},\n}\n"], "roberta-large-openai-detector": [], "google/bit-50": ["from transformers import BitImageProcessor, BitForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nfeature_extractor = BitImageProcessor.from_pretrained(\"google/bit-50\")\nmodel = BitForImageClassification.from_pretrained(\"google/bit-50\")\n\ninputs = feature_extractor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label\n>>> tabby, tabby cat\n"], "OpenAssistant/oasst-sft-1-pythia-12b": ["A visual joke often accompanied by text that spreads rapidly from individual\nto individual. The exact origin of the term \"meme\" is unclear, but it was\nfirst used in the 1990s by Richard Dawkins in his book \"The Selfish Gene.\"\nHe defined a meme as an idea, behavior, or style that is transmitted from\none person to another within a culture.\n\nDawkins' use of the term caught on among scholars and journalists, who began\nto apply the concept to other types of cultural items such as advertisements,\nfashion trends, and jokes. Over time, the term \"meme\" has come to be used\nmore generally to describe any social behavior that can be repeated and\naltered by individuals. Today, the term is widely recognized and applied in\nfields ranging from anthropology to psychology to marketing.<|endoftext|>\n"], "OpenLemur/lemur-70b-chat-v1": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"OpenLemur/lemur-70b-chat-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"OpenLemur/lemur-70b-chat-v1\", device_map=\"auto\", load_in_8bit=True)\n\n# Text Generation Example\nprompt = \"\"\"<|im_start|>system\nYou are a helpful, respectful, and honest assistant.\n<|im_end|>\n<|im_start|>user\nWhat's a lemur's favorite fruit?<|im_end|>\n<|im_start|>assistant\n\"\"\"\ninput = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(**input, max_length=50, num_return_sequences=1)\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_text)\n\n# Code Generation Example\nprompt = \"\"\"<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n<|im_end|>\n<|im_start|>user\nWrite a Python function to merge two sorted lists into one sorted list without using any built-in sort functions.<|im_end|>\n<|im_start|>assistant\n\"\"\"\ninput = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(**input, max_length=200, num_return_sequences=1)\ngenerated_code = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_code)\n"], "allenai/OLMo-7B-hf": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nolmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-7B-hf\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-7B-hf\")\nmessage = [\"Language modeling is\"]\ninputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n# optional verifying cuda\n# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n# olmo = olmo.to('cuda')\nresponse = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n>> 'Language modeling is the first step to build natural language generation...'\n", "from transformers import pipeline\nolmo_pipe = pipeline(\"text-generation\", model=\"allenai/OLMo-7B-hf\")\nprint(olmo_pipe(\"Language modeling is \"))\n>> 'Language modeling is a branch of natural language processing that aims to...'\n"], "jbilcke-hf/sdxl-cinematic-1": ["import torch\nfrom diffusers import DiffusionPipeline, AutoencoderKL\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nvae = AutoencoderKL.from_pretrained('madebyollin/sdxl-vae-fp16-fix', torch_dtype=torch.float16)\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    vae=vae, torch_dtype=torch.float16, variant=\"fp16\",\n    use_safetensors=True\n)\npipe.to(device)\n# This is where you load your trained weights\nspecific_safetensors = \"pytorch_lora_weights.safetensors\"\nlora_scale = 0.9\npipe.load_lora_weights(\n    'jbilcke-hf/sdxl-cinematic-1', \n    weight_name = specific_safetensors,\n    # use_auth_token = True \n)\nprompt = \"A majestic cnmt jumping from a big stone at night\"\nimage = pipe(\n    prompt=prompt, \n    num_inference_steps=50,\n    cross_attention_kwargs={\"scale\": lora_scale}\n).images[0]\n"], "albert/albert-xlarge-v2": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u00e2\u2013\u0081modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u00e2\u2013\u0081modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u00e2\u2013\u0081model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u00e2\u2013\u0081runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u00e2\u2013\u0081lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\nmodel = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\nmodel = TFAlbertModel.from_pretrained(\"albert-xlarge-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u00e2\u2013\u0081shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u00e2\u2013\u0081blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u00e2\u2013\u0081lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u00e2\u2013\u0081receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u00e2\u2013\u0081paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u00e2\u2013\u0081waitress\"\n   }\n]\n"], "microsoft/beit-base-finetuned-ade-640-640": ["from transformers import BeitFeatureExtractor, BeitForSemanticSegmentation\nfrom datasets import load_dataset\nfrom PIL import Image\n\n# load ADE20k image\nds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\nimage = Image.open(ds[0]['file'])\n\nfeature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\nmodel = BeitForSemanticSegmentation.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n# logits are of shape (batch_size, num_labels, height/4, width/4)\nlogits = outputs.logits\n"], "Qwen/Qwen-14B-Chat": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-14B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-14B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-14B-Chat\", trust_remote_code=True) # \u53ef\u6307\u5b9a\u4e0d\u540c\u7684\u751f\u6210\u957f\u5ea6\u3001top_p\u7b49\u76f8\u5173\u8d85\u53c2\n\n# \u7b2c\u4e00\u8f6e\u5bf9\u8bdd 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\nprint(response)\n# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n\n# \u7b2c\u4e8c\u8f6e\u5bf9\u8bdd 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\", history=history)\nprint(response)\n# \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\n# \u6545\u4e8b\u7684\u4e3b\u4eba\u516c\u53eb\u674e\u660e\uff0c\u4ed6\u6765\u81ea\u4e00\u4e2a\u666e\u901a\u7684\u5bb6\u5ead\uff0c\u7236\u6bcd\u90fd\u662f\u666e\u901a\u7684\u5de5\u4eba\u3002\u4ece\u5c0f\uff0c\u674e\u660e\u5c31\u7acb\u4e0b\u4e86\u4e00\u4e2a\u76ee\u6807\uff1a\u8981\u6210\u4e3a\u4e00\u540d\u6210\u529f\u7684\u4f01\u4e1a\u5bb6\u3002\n# \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff0c\u674e\u660e\u52e4\u594b\u5b66\u4e60\uff0c\u8003\u4e0a\u4e86\u5927\u5b66\u3002\u5728\u5927\u5b66\u671f\u95f4\uff0c\u4ed6\u79ef\u6781\u53c2\u52a0\u5404\u79cd\u521b\u4e1a\u6bd4\u8d5b\uff0c\u83b7\u5f97\u4e86\u4e0d\u5c11\u5956\u9879\u3002\u4ed6\u8fd8\u5229\u7528\u8bfe\u4f59\u65f6\u95f4\u53bb\u5b9e\u4e60\uff0c\u79ef\u7d2f\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u3002\n# \u6bd5\u4e1a\u540e\uff0c\u674e\u660e\u51b3\u5b9a\u5f00\u59cb\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u5f00\u59cb\u5bfb\u627e\u6295\u8d44\u673a\u4f1a\uff0c\u4f46\u591a\u6b21\u90fd\u88ab\u62d2\u7edd\u4e86\u3002\u7136\u800c\uff0c\u4ed6\u5e76\u6ca1\u6709\u653e\u5f03\u3002\u4ed6\u7ee7\u7eed\u52aa\u529b\uff0c\u4e0d\u65ad\u6539\u8fdb\u81ea\u5df1\u7684\u521b\u4e1a\u8ba1\u5212\uff0c\u5e76\u5bfb\u627e\u65b0\u7684\u6295\u8d44\u673a\u4f1a\u3002\n# \u6700\u7ec8\uff0c\u674e\u660e\u6210\u529f\u5730\u83b7\u5f97\u4e86\u4e00\u7b14\u6295\u8d44\uff0c\u5f00\u59cb\u4e86\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u6210\u7acb\u4e86\u4e00\u5bb6\u79d1\u6280\u516c\u53f8\uff0c\u4e13\u6ce8\u4e8e\u5f00\u53d1\u65b0\u578b\u8f6f\u4ef6\u3002\u5728\u4ed6\u7684\u9886\u5bfc\u4e0b\uff0c\u516c\u53f8\u8fc5\u901f\u53d1\u5c55\u8d77\u6765\uff0c\u6210\u4e3a\u4e86\u4e00\u5bb6\u6210\u529f\u7684\u79d1\u6280\u4f01\u4e1a\u3002\n# \u674e\u660e\u7684\u6210\u529f\u5e76\u4e0d\u662f\u5076\u7136\u7684\u3002\u4ed6\u52e4\u594b\u3001\u575a\u97e7\u3001\u52c7\u4e8e\u5192\u9669\uff0c\u4e0d\u65ad\u5b66\u4e60\u548c\u6539\u8fdb\u81ea\u5df1\u3002\u4ed6\u7684\u6210\u529f\u4e5f\u8bc1\u660e\u4e86\uff0c\u53ea\u8981\u52aa\u529b\u594b\u6597\uff0c\u4efb\u4f55\u4eba\u90fd\u6709\u53ef\u80fd\u53d6\u5f97\u6210\u529f\u3002\n\n# \u7b2c\u4e09\u8f6e\u5bf9\u8bdd 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"\u7ed9\u8fd9\u4e2a\u6545\u4e8b\u8d77\u4e00\u4e2a\u6807\u9898\", history=history)\nprint(response)\n# \u300a\u594b\u6597\u521b\u4e1a\uff1a\u4e00\u4e2a\u5e74\u8f7b\u4eba\u7684\u6210\u529f\u4e4b\u8def\u300b\n"], "TheBloke/Llama-2-7B-Chat-GGML": [], "facebook/galactica-125m": ["\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-125m\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-125m\")\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-125m\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-125m\", device_map=\"auto\")\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-125m\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-125m\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-125m\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-125m\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"The Transformer architecture [START_REF]\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n"], "mosaicml/mpt-7b-8k-instruct": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-instruct-8k',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-7b-instruct-8k'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-7b-instruct-8k'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-7b-8k')\n", "from transformers import pipeline\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**inputs, max_new_tokens=100)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "deepseek-ai/DeepSeek-V2-Lite-Chat": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/DeepSeek-V2-Lite\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntext = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)\n", "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\n", "from transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 8192, 1\nmodel_name = \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{\"role\": \"user\", \"content\": \"Who are you?\"}],\n    [{\"role\": \"user\", \"content\": \"Translate the following content into Chinese directly: DeepSeek-V2 adopts innovative architectures to guarantee economical training and efficient inference.\"}],\n    [{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n", "from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(\n    model='deepseek-chat',\n    openai_api_key=<your-deepseek-api-key>,\n    openai_api_base='https://api.deepseek.com/v1',\n    temperature=0.85,\n    max_tokens=8000)\n"], "kandinsky-community/kandinsky-2-2-controlnet-depth": ["import torch\nimport numpy as np\n\nfrom transformers import pipeline\nfrom diffusers.utils import load_image\n\nfrom diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n\n# let's take an image and extract its depth map.\ndef make_hint(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    hint = detected_map.permute(2, 0, 1)\n    return hint\n\nimg = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n).resize((768, 768))\n\n# We can use the `depth-estimation` pipeline from transformers to process the image and retrieve its depth map.\ndepth_estimator = pipeline(\"depth-estimation\")\nhint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n\n# Now, we load the prior pipeline and the text-to-image controlnet pipeline\npipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n)\npipe_prior = pipe_prior.to(\"cuda\")\n\npipe = KandinskyV22ControlnetPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\n# We pass the prompt and negative prompt through the prior to generate image embeddings\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\nimage_emb, zero_image_emb = pipe_prior(\n    prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator\n).to_tuple()\n\n# Now we can pass the image embeddings and the depth image we extracted to the controlnet pipeline. With Kandinsky 2.2, only prior pipelines accept `prompt` input. You do not need to pass the prompt to the controlnet pipeline.\nimages = pipe(\n    image_embeds=image_emb,\n    negative_image_embeds=zero_image_emb,\n    hint=hint,\n    num_inference_steps=50,\n    generator=generator,\n    height=768,\n    width=768,\n).images\nimages[0].save(\"robot_cat.png\")\n", "import torch\nimport numpy as np\n\nfrom diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\nfrom diffusers.utils import load_image\nfrom transformers import pipeline\n\nimg = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinskyv22/cat.png\"\n).resize((768, 768))\n\ndef make_hint(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    hint = detected_map.permute(2, 0, 1)\n    return hint\n\ndepth_estimator = pipeline(\"depth-estimation\")\nhint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n\npipe_prior = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n)\npipe_prior = pipe_prior.to(\"cuda\")\n\npipe = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"A robot, 4k photo\"\nnegative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(43)\n\n# run prior pipeline\n\nimg_emb = pipe_prior(prompt=prompt, image=img, strength=0.85, generator=generator)\nnegative_emb = pipe_prior(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)\n\n# run controlnet img2img pipeline\nimages = pipe(\n    image=img,\n    strength=0.5,\n    image_embeds=img_emb.image_embeds,\n    negative_image_embeds=negative_emb.image_embeds,\n    hint=hint,\n    num_inference_steps=50,\n    generator=generator,\n    height=768,\n    width=768,\n).images\n\nimages[0].save(\"robot_cat.png\")\n"], "CompVis/stable-diffusion-v1-1": ["import torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-1\"\ndevice = \"cuda\"\n\n\npipe = StableDiffusionPipeline.from_pretrained(model_id)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "import torch\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\nmodel_id = \"CompVis/stable-diffusion-v1-1\"\n# Use the K-LMS scheduler here instead\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, use_auth_token=True)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n"], "CiroN2022/toy-face": [], "timm/eva02_large_patch14_clip_336.merged2b_s6b_b61k": [], "timm/ViT-SO400M-14-SigLIP": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-SO400M-14-SigLIP')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-SO400M-14-SigLIP')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_so400m_patch14_siglip_224',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "jetmoe/jetmoe-8b": ["from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModelForSequenceClassification\nfrom jetmoe import JetMoEForCausalLM, JetMoEConfig, JetMoEForSequenceClassification\n\nAutoConfig.register(\"jetmoe\", JetMoEConfig)\nAutoModelForCausalLM.register(JetMoEConfig, JetMoEForCausalLM)\nAutoModelForSequenceClassification.register(JetMoEConfig, JetMoEForSequenceClassification)\n\ntokenizer = AutoTokenizer.from_pretrained('jetmoe/jetmoe-8b')\nmodel = AutoModelForCausalLM.from_pretrained('jetmoe/jetmoe-8b')\n"], "valhalla/t5-small-e2e-qg": ["from pipelines import pipeline\n\ntext = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \\\nand first released in 1991, Python's design philosophy emphasizes code \\\nreadability with its notable use of significant whitespace.\"\n\nnlp = pipeline(\"e2e-qg\")\nnlp(text)\n=> [\n 'Who created Python?',\n 'When was Python first released?',\n \"What is Python's design philosophy?\"\n]\n"], "mosaicml/mpt-30b-chat": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-30b-chat',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-30b-chat'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-30b-chat'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n", "from transformers import pipeline\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**inputs, max_new_tokens=100)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "google/t5-xl-lm-adapt": [], "facebook/data2vec-vision-base-ft1k": ["from transformers import BeitFeatureExtractor, Data2VecVisionForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = BeitFeatureExtractor.from_pretrained('facebook/data2vec-vision-base-ft1k')\nmodel = Data2VecVisionForImageClassification.from_pretrained('facebook/data2vec-vision-base-ft1k')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K": [], "openmmlab/upernet-convnext-tiny": [], "warp-ai/wuerstchen": ["import torch\nfrom diffusers import AutoPipelineForText2Image\n\ndevice = \"cuda\"\ndtype = torch.float16\n\npipeline =  AutoPipelineForText2Image.from_pretrained(\n    \"warp-diffusion/wuerstchen\", torch_dtype=dtype\n).to(device)\n\ncaption = \"Anthropomorphic cat dressed as a fire fighter\"\n\noutput = pipeline(\n    prompt=caption,\n    height=1024,\n    width=1024,\n    prior_guidance_scale=4.0,\n    decoder_guidance_scale=0.0,\n).images\n"], "facebook/convnext-base-224-22k": ["from transformers import ConvNextImageProcessor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-base-224-22k\")\nmodel = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-base-224-22k\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 22k ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "facebook/regnet-y-040": [">>> from transformers import AutoFeatureExtractor, RegNetForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/regnet-y-040\")\n>>> model = RegNetForImageClassification.from_pretrained(\"facebook/regnet-y-040\")\n\n>>> inputs = feature_extractor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\n'tabby, tabby cat'\n"], "ostris/crayon_style_lora_sdxl": [], "AI-Sweden-Models/gpt-sw3-126m": ["import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n\n# Initialize Variables\nmodel_name = \"AI-Sweden-Models/gpt-sw3-126m\"\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprompt = \"Tr\u00e4d \u00e4r fina f\u00f6r att\"\n\n# Initialize Tokenizer & Model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.eval()\nmodel.to(device)\n"], "laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg": [], "Musixmatch/umberto-wikipedia-uncased-v1": ["\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-wikipedia-uncased-v1\")\numberto = AutoModel.from_pretrained(\"Musixmatch/umberto-wikipedia-uncased-v1\")\n\nencoded_input = tokenizer.encode(\"Umberto Eco \u00e8 stato un grande scrittore\")\ninput_ids = torch.tensor(encoded_input).unsqueeze(0)  # Batch size 1\noutputs = umberto(input_ids)\nlast_hidden_states = outputs[0]  # The last hidden-state is the first element of the output\n", "from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"Musixmatch/umberto-wikipedia-uncased-v1\",\n    tokenizer=\"Musixmatch/umberto-wikipedia-uncased-v1\"\n)\n\nresult = fill_mask(\"Umberto Eco \u00e8 <mask> un grande scrittore\")\n# {'sequence': '<s> umberto eco \u00e8 stato un grande scrittore</s>', 'score': 0.5784581303596497, 'token': 361}\n# {'sequence': '<s> umberto eco \u00e8 anche un grande scrittore</s>', 'score': 0.33813193440437317, 'token': 269}\n# {'sequence': '<s> umberto eco \u00e8 considerato un grande scrittore</s>', 'score': 0.027196012437343597, 'token': 3236}\n# {'sequence': '<s> umberto eco \u00e8 diventato un grande scrittore</s>', 'score': 0.013716378249228, 'token': 5742}\n# {'sequence': '<s> umberto eco \u00e8 inoltre un grande scrittore</s>', 'score': 0.010662357322871685, 'token': 1030}\n"], "CohereForAI/c4ai-command-r-plus-4bit": ["# pip install 'git+https://github.com/huggingface/transformers.git' bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"CohereForAI/c4ai-command-r-plus-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\n# Format message with the command-r-plus chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n\ngen_tokens = model.generate(\n    input_ids, \n    max_new_tokens=100, \n    do_sample=True, \n    temperature=0.3,\n    )\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-plus-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# Define tools available for the model to use:\ntools = [\n  {\n    \"name\": \"internet_search\",\n    \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n    \"parameter_definitions\": {\n      \"query\": {\n        \"description\": \"Query to search the internet with\",\n        \"type\": 'str',\n        \"required\": True\n      }\n    }\n  },\n  {\n    'name': \"directly_answer\",\n    \"description\": \"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\",\n    'parameter_definitions': {}\n  }\n]\n\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_tool_use_template(\n    conversation,\n    tools=tools,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(tool_use_prompt)\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-plus-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n\n# Define tools available for the model to use\n# Type hints and docstrings from Python functions are automatically extracted\ndef internet_search(query: str):\n    \"\"\"\n    Returns a list of relevant document snippets for a textual query retrieved from the internet\n\n    Args:\n        query: Query to search the internet with\n    \"\"\"\n    pass\n\ndef directly_answer():\n    \"\"\"\n    Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n    \"\"\"\n    pass\n\ntools = [internet_search, directly_answer]\n\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_chat_template(\n    conversation,\n    tools=tools,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(tool_use_prompt)\n", "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\nThe instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n\n## Available Tools\nHere is a list of tools that you have available to you:\n\n```python\ndef internet_search(query: str) -> List[Dict]:\n    \"\"\"Returns a list of relevant document snippets for a textual query retrieved from the internet\n\n    Args:\n        query (str): Query to search the internet with\n    \"\"\"\n    pass\n```\n\n```python\ndef directly_answer() -> List[Dict]:\n    \"\"\"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n    \"\"\"\n    pass\n```<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write 'Action:' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user's last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n```json\n[\n    {\n        \"tool_name\": title of the tool in the specification,\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\n    }\n]```<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n", "from transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-plus-4bit\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# define conversation input:\nconversation = [\n    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# define documents to ground on:\ndocuments = [\n    { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest growing up to 122 cm in height.\" }, \n    { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}\n]\n\n# render the tool use prompt as a string:\ngrounded_generation_prompt = tokenizer.apply_grounded_generation_template(\n    conversation,\n    documents=documents,\n    citation_mode=\"accurate\", # or \"fast\"\n    tokenize=False,\n    add_generation_prompt=True,\n)\nprint(grounded_generation_prompt)\n", "The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\nDocument: 0\ntitle: Tall penguins\ntext: Emperor penguins are the tallest growing up to 122 cm in height.\n\nDocument: 1\ntitle: Penguin habitats\ntext: Emperor penguins only live in Antarctica.\n</results><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Carefully perform the following instructions, in order, starting each with a new line.\nFirstly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.\nSecondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.\nThirdly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\nFinally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n"], "nghuyong/ernie-1.0-base-zh": ["from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\nmodel = AutoModel.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n"], "microsoft/swin-base-patch4-window12-384-in22k": ["from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-base-patch4-window12-384-in22k\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window12-384-in22k\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "internlm/internlm-7b": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-7b\", trust_remote_code=True)\n# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and might cause OOM Error.\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-7b\", torch_dtype=torch.float16, trust_remote_code=True).cuda()\nmodel = model.eval()\ninputs = tokenizer([\"A beautiful flower\"], return_tensors=\"pt\")\nfor k,v in inputs.items():\n    inputs[k] = v.cuda()\ngen_kwargs = {\"max_length\": 128, \"top_p\": 0.8, \"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.1}\noutput = model.generate(**inputs, **gen_kwargs)\noutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\nprint(output)\n# <s> A beautiful flower box made of white rose wood. It is a perfect gift for weddings, birthdays and anniversaries.\n# All the roses are from our farm Roses Flanders. Therefor you know that these flowers last much longer than those in store or online!</s>\n", "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-7b\", trust_remote_code=True)\n# `torch_dtype=torch.float16` \u53ef\u4ee5\u4ee4\u6a21\u578b\u4ee5 float16 \u7cbe\u5ea6\u52a0\u8f7d\uff0c\u5426\u5219 transformers \u4f1a\u5c06\u6a21\u578b\u52a0\u8f7d\u4e3a float32\uff0c\u6709\u53ef\u80fd\u5bfc\u81f4\u663e\u5b58\u4e0d\u8db3\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-7b\", torch_dtype=torch.float16, trust_remote_code=True).cuda()\nmodel = model.eval()\ninputs = tokenizer([\"\u6765\u5230\u7f8e\u4e3d\u7684\u5927\u81ea\u7136\uff0c\u6211\u4eec\u53d1\u73b0\"], return_tensors=\"pt\")\nfor k,v in inputs.items():\n    inputs[k] = v.cuda()\ngen_kwargs = {\"max_length\": 128, \"top_p\": 0.8, \"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.1}\noutput = model.generate(**inputs, **gen_kwargs)\noutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\nprint(output)\n# \u6765\u5230\u7f8e\u4e3d\u7684\u5927\u81ea\u7136\uff0c\u6211\u4eec\u53d1\u73b0\u5404\u79cd\u5404\u6837\u7684\u82b1\u5343\u5947\u767e\u602a\u3002\u6709\u7684\u989c\u8272\u9c9c\u8273\u4eae\u4e3d,\u4f7f\u4eba\u611f\u89c9\u751f\u673a\u52c3\u52c3\uff1b\u6709\u7684\u662f\u7ea2\u8272\u7684\u82b1\u74e3\u513f\u7c89\u5ae9\u5ae9\u7684\u50cf\u5c11\u5973\u5bb3\u7f9e\u7684\u8138\u5e9e\u4e00\u6837\u8ba9\u4eba\u7231\u4e0d\u91ca\u624b\uff0e\u6709\u7684\u5c0f\u5de7\u73b2\u73d1; \u8fd8\u6709\u7684\u82b1\u74e3\u7c97\u5927\u770b\u4f3c\u67af\u9ec4\u5b9e\u5219\u6697\u85cf\u7384\u673a\uff01\n# \u4e0d\u540c\u7684\u82b1\u5349\u6709\u4e0d\u540c\u7684\u201c\u813e\u6c14\u201d,\u5b83\u4eec\u90fd\u6709\u7740\u5c5e\u4e8e\u81ea\u5df1\u7684\u6545\u4e8b\u548c\u4eba\u751f\u9053\u7406.\u8fd9\u4e9b\u9c9c\u82b1\u90fd\u662f\u5927\u81ea\u7136\u4e2d\u6700\u4e3a\u539f\u59cb\u7684\u7269\u79cd,\u6bcf\u4e00\u6735\u90fd\u7efd\u653e\u51fa\u522b\u6837\u7684\u7f8e\u4ee4\u4eba\u9676\u9189\u3001\u7740\u8ff7!\n"], "studio-ousia/luke-large": [], "stabilityai/stablelm-tuned-alpha-7b": ["from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n\ntokenizer = AutoTokenizer.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"StabilityAI/stablelm-tuned-alpha-7b\")\nmodel.half().cuda()\n\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        stop_ids = [50278, 50279, 50277, 1, 0]\n        for stop_id in stop_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n\nprompt = f\"{system_prompt}<|USER|>What's your mood today?<|ASSISTANT|>\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=64,\n  temperature=0.7,\n  do_sample=True,\n  stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n", "@misc{gpt4all,\n  author = {Yuvanesh Anand and Zach Nussbaum and Brandon Duderstadt and Benjamin Schmidt and Andriy Mulyar},\n  title = {GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/nomic-ai/gpt4all}},\n}\n"], "Salesforce/codet5-large": ["from transformers import AutoTokenizer, T5ForConditionalGeneration\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-large\")\ntext = \"def greet(user): print(f'hello <extra_id_0>!')\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\n# simply generate a single sequence\ngenerated_ids = model.generate(input_ids, max_length=8)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg": [], "codellama/CodeLlama-13b-Python-hf": [], "HuggingFaceH4/starchat-alpha": ["import torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\nprompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\nprompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n# We use a special <|end|> token with ID 49155 to denote ends of a turn\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n# You can sort a list in Python by using the sort() method. Here's an example:\\n\\n```\\nnumbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\\nnumbers.sort()\\nprint(numbers)\\n```\\n\\nThis will sort the list in place and print the sorted list.\n"], "allenai/led-large-16384-arxiv": ["LONG_ARTICLE = \"\"\"\"for about 20 years the problem of properties of\nshort - term changes of solar activity has been\nconsidered extensively . many investigators\nstudied the short - term periodicities of the\nvarious indices of solar activity . several\nperiodicities were detected , but the\nperiodicities about 155 days and from the interval\nof @xmath3 $ ] days ( @xmath4 $ ] years ) are\nmentioned most often . first of them was\ndiscovered by @xcite in the occurence rate of\ngamma - ray flares detected by the gamma - ray\nspectrometer aboard the _ solar maximum mission (\nsmm ) . this periodicity was confirmed for other\nsolar flares data and for the same time period\n@xcite . it was also found in proton flares during\nsolar cycles 19 and 20 @xcite , but it was not\nfound in the solar flares data during solar cycles\n22 @xcite . _    several autors confirmed above\nresults for the daily sunspot area data . @xcite\nstudied the sunspot data from 18741984 . she found\nthe 155-day periodicity in data records from 31\nyears . this periodicity is always characteristic\nfor one of the solar hemispheres ( the southern\nhemisphere for cycles 1215 and the northern\nhemisphere for cycles 1621 ) . moreover , it is\nonly present during epochs of maximum activity (\nin episodes of 13 years ) .\nsimilarinvestigationswerecarriedoutby + @xcite .\nthey applied the same power spectrum method as\nlean , but the daily sunspot area data ( cycles\n1221 ) were divided into 10 shorter time series .\nthe periodicities were searched for the frequency\ninterval 57115 nhz ( 100200 days ) and for each of\n10 time series . the authors showed that the\nperiodicity between 150160 days is statistically\nsignificant during all cycles from 16 to 21 . the\nconsidered peaks were remained unaltered after\nremoving the 11-year cycle and applying the power\nspectrum analysis . @xcite used the wavelet\ntechnique for the daily sunspot areas between 1874\nand 1993 . they determined the epochs of\nappearance of this periodicity and concluded that\nit presents around the maximum activity period in\ncycles 16 to 21 . moreover , the power of this\nperiodicity started growing at cycle 19 ,\ndecreased in cycles 20 and 21 and disappered after\ncycle 21 . similaranalyseswerepresentedby + @xcite\n, but for sunspot number , solar wind plasma ,\ninterplanetary magnetic field and geomagnetic\nactivity index @xmath5 . during 1964 - 2000 the\nsunspot number wavelet power of periods less than\none year shows a cyclic evolution with the phase\nof the solar cycle.the 154-day period is prominent\nand its strenth is stronger around the 1982 - 1984\ninterval in almost all solar wind parameters . the\nexistence of the 156-day periodicity in sunspot\ndata were confirmed by @xcite . they considered\nthe possible relation between the 475-day (\n1.3-year ) and 156-day periodicities . the 475-day\n( 1.3-year ) periodicity was also detected in\nvariations of the interplanetary magnetic field ,\ngeomagnetic activity helioseismic data and in the\nsolar wind speed @xcite . @xcite concluded that\nthe region of larger wavelet power shifts from\n475-day ( 1.3-year ) period to 620-day ( 1.7-year\n) period and then back to 475-day ( 1.3-year ) .\nthe periodicities from the interval @xmath6 $ ]\ndays ( @xmath4 $ ] years ) have been considered\nfrom 1968 . @xcite mentioned a 16.3-month (\n490-day ) periodicity in the sunspot numbers and\nin the geomagnetic data . @xcite analysed the\noccurrence rate of major flares during solar\ncycles 19 . they found a 18-month ( 540-day )\nperiodicity in flare rate of the norhern\nhemisphere . @xcite confirmed this result for the\n@xmath7 flare data for solar cycles 20 and 21 and\nfound a peak in the power spectra near 510540 days\n. @xcite found a 17-month ( 510-day ) periodicity\nof sunspot groups and their areas from 1969 to\n1986 . these authors concluded that the length of\nthis period is variable and the reason of this\nperiodicity is still not understood . @xcite and +\n@xcite obtained statistically significant peaks of\npower at around 158 days for daily sunspot data\nfrom 1923 - 1933 ( cycle 16 ) . in this paper the\nproblem of the existence of this periodicity for\nsunspot data from cycle 16 is considered . the\ndaily sunspot areas , the mean sunspot areas per\ncarrington rotation , the monthly sunspot numbers\nand their fluctuations , which are obtained after\nremoving the 11-year cycle are analysed . in\nsection 2 the properties of the power spectrum\nmethods are described . in section 3 a new\napproach to the problem of aliases in the power\nspectrum analysis is presented . in section 4\nnumerical results of the new method of the\ndiagnosis of an echo - effect for sunspot area\ndata are discussed . in section 5 the problem of\nthe existence of the periodicity of about 155 days\nduring the maximum activity period for sunspot\ndata from the whole solar disk and from each solar\nhemisphere separately is considered . to find\nperiodicities in a given time series the power\nspectrum analysis is applied . in this paper two\nmethods are used : the fast fourier transformation\nalgorithm with the hamming window function ( fft )\nand the blackman - tukey ( bt ) power spectrum\nmethod @xcite . the bt method is used for the\ndiagnosis of the reasons of the existence of peaks\n, which are obtained by the fft method . the bt\nmethod consists in the smoothing of a cosine\ntransform of an autocorrelation function using a\n3-point weighting average . such an estimator is\nconsistent and unbiased . moreover , the peaks are\nuncorrelated and their sum is a variance of a\nconsidered time series . the main disadvantage of\nthis method is a weak resolution of the\nperiodogram points , particularly for low\nfrequences . for example , if the autocorrelation\nfunction is evaluated for @xmath8 , then the\ndistribution points in the time domain are :\n@xmath9 thus , it is obvious that this method\nshould not be used for detecting low frequency\nperiodicities with a fairly good resolution .\nhowever , because of an application of the\nautocorrelation function , the bt method can be\nused to verify a reality of peaks which are\ncomputed using a method giving the better\nresolution ( for example the fft method ) . it is\nvaluable to remember that the power spectrum\nmethods should be applied very carefully . the\ndifficulties in the interpretation of significant\npeaks could be caused by at least four effects : a\nsampling of a continuos function , an echo -\neffect , a contribution of long - term\nperiodicities and a random noise . first effect\nexists because periodicities , which are shorter\nthan the sampling interval , may mix with longer\nperiodicities . in result , this effect can be\nreduced by an decrease of the sampling interval\nbetween observations . the echo - effect occurs\nwhen there is a latent harmonic of frequency\n@xmath10 in the time series , giving a spectral\npeak at @xmath10 , and also periodic terms of\nfrequency @xmath11 etc . this may be detected by\nthe autocorrelation function for time series with\na large variance . time series often contain long\n- term periodicities , that influence short - term\npeaks . they could rise periodogram s peaks at\nlower frequencies . however , it is also easy to\nnotice the influence of the long - term\nperiodicities on short - term peaks in the graphs\nof the autocorrelation functions . this effect is\nobserved for the time series of solar activity\nindexes which are limited by the 11-year cycle .\nto find statistically significant periodicities it\nis reasonable to use the autocorrelation function\nand the power spectrum method with a high\nresolution . in the case of a stationary time\nseries they give similar results . moreover , for\na stationary time series with the mean zero the\nfourier transform is equivalent to the cosine\ntransform of an autocorrelation function @xcite .\nthus , after a comparison of a periodogram with an\nappropriate autocorrelation function one can\ndetect peaks which are in the graph of the first\nfunction and do not exist in the graph of the\nsecond function . the reasons of their existence\ncould be explained by the long - term\nperiodicities and the echo - effect . below method\nenables one to detect these effects . ( solid line\n) and the 95% confidence level basing on thered\nnoise ( dotted line ) . the periodogram values are\npresented on the left axis . the lower curve\nillustrates the autocorrelation function of the\nsame time series ( solid line ) . the dotted lines\nrepresent two standard errors of the\nautocorrelation function . the dashed horizontal\nline shows the zero level . the autocorrelation\nvalues are shown in the right axis . ]     because\nthe statistical tests indicate that the time\nseries is a white noise the confidence level is\nnot marked . ]    . ] the method of the diagnosis\nof an echo - effect in the power spectrum ( de )\nconsists in an analysis of a periodogram of a\ngiven time series computed using the bt method .\nthe bt method bases on the cosine transform of the\nautocorrelation function which creates peaks which\nare in the periodogram , but not in the\nautocorrelation function . the de method is used\nfor peaks which are computed by the fft method (\nwith high resolution ) and are statistically\nsignificant . the time series of sunspot activity\nindexes with the spacing interval one rotation or\none month contain a markov - type persistence ,\nwhich means a tendency for the successive values\nof the time series to remember their antecendent\nvalues . thus , i use a confidence level basing on\nthe red noise of markov @xcite for the choice of\nthe significant peaks of the periodogram computed\nby the fft method . when a time series does not\ncontain the markov - type persistence i apply the\nfisher test and the kolmogorov - smirnov test at\nthe significance level @xmath12 @xcite to verify a\nstatistically significance of periodograms peaks .\nthe fisher test checks the null hypothesis that\nthe time series is white noise agains the\nalternative hypothesis that the time series\ncontains an added deterministic periodic component\nof unspecified frequency . because the fisher test\ntends to be severe in rejecting peaks as\ninsignificant the kolmogorov - smirnov test is\nalso used . the de method analyses raw estimators\nof the power spectrum . they are given as follows\n@xmath13    for @xmath14 + where @xmath15 for\n@xmath16 + @xmath17 is the length of the time\nseries @xmath18 and @xmath19 is the mean value .\nthe first term of the estimator @xmath20 is\nconstant . the second term takes two values (\ndepending on odd or even @xmath21 ) which are not\nsignificant because @xmath22 for large m. thus ,\nthe third term of ( 1 ) should be analysed .\nlooking for intervals of @xmath23 for which\n@xmath24 has the same sign and different signs one\ncan find such parts of the function @xmath25 which\ncreate the value @xmath20 . let the set of values\nof the independent variable of the autocorrelation\nfunction be called @xmath26 and it can be divided\ninto the sums of disjoint sets : @xmath27 where +\n@xmath28 + @xmath29 @xmath30 @xmath31 + @xmath32 +\n@xmath33 @xmath34 @xmath35 @xmath36 @xmath37\n@xmath38 @xmath39 @xmath40    well , the set\n@xmath41 contains all integer values of @xmath23\nfrom the interval of @xmath42 for which the\nautocorrelation function and the cosinus function\nwith the period @xmath43 $ ] are positive . the\nindex @xmath44 indicates successive parts of the\ncosinus function for which the cosinuses of\nsuccessive values of @xmath23 have the same sign .\nhowever , sometimes the set @xmath41 can be empty\n. for example , for @xmath45 and @xmath46 the set\n@xmath47 should contain all @xmath48 $ ] for which\n@xmath49 and @xmath50 , but for such values of\n@xmath23 the values of @xmath51 are negative .\nthus , the set @xmath47 is empty .    . the\nperiodogram values are presented on the left axis\n. the lower curve illustrates the autocorrelation\nfunction of the same time series . the\nautocorrelation values are shown in the right axis\n. ] let us take into consideration all sets\n\\{@xmath52 } , \\{@xmath53 } and \\{@xmath41 } which\nare not empty . because numberings and power of\nthese sets depend on the form of the\nautocorrelation function of the given time series\n, it is impossible to establish them arbitrary .\nthus , the sets of appropriate indexes of the sets\n\\{@xmath52 } , \\{@xmath53 } and \\{@xmath41 } are\ncalled @xmath54 , @xmath55 and @xmath56\nrespectively . for example the set @xmath56\ncontains all @xmath44 from the set @xmath57 for\nwhich the sets @xmath41 are not empty . to\nseparate quantitatively in the estimator @xmath20\nthe positive contributions which are originated by\nthe cases described by the formula ( 5 ) from the\ncases which are described by the formula ( 3 ) the\nfollowing indexes are introduced : @xmath58\n@xmath59 @xmath60 @xmath61 where @xmath62 @xmath63\n@xmath64 taking for the empty sets \\{@xmath53 }\nand \\{@xmath41 } the indices @xmath65 and @xmath66\nequal zero . the index @xmath65 describes a\npercentage of the contribution of the case when\n@xmath25 and @xmath51 are positive to the positive\npart of the third term of the sum ( 1 ) . the\nindex @xmath66 describes a similar contribution ,\nbut for the case when the both @xmath25 and\n@xmath51 are simultaneously negative . thanks to\nthese one can decide which the positive or the\nnegative values of the autocorrelation function\nhave a larger contribution to the positive values\nof the estimator @xmath20 . when the difference\n@xmath67 is positive , the statement the\n@xmath21-th peak really exists can not be rejected\n. thus , the following formula should be satisfied\n: @xmath68    because the @xmath21-th peak could\nexist as a result of the echo - effect , it is\nnecessary to verify the second condition :\n@xmath69\\in c_m.\\ ] ]    . the periodogram values\nare presented on the left axis . the lower curve\nillustrates the autocorrelation function of the\nsame time series ( solid line ) . the dotted lines\nrepresent two standard errors of the\nautocorrelation function . the dashed horizontal\nline shows the zero level . the autocorrelation\nvalues are shown in the right axis . ]    to\nverify the implication ( 8) firstly it is\nnecessary to evaluate the sets @xmath41 for\n@xmath70 of the values of @xmath23 for which the\nautocorrelation function and the cosine function\nwith the period @xmath71 $ ] are positive and the\nsets @xmath72 of values of @xmath23 for which the\nautocorrelation function and the cosine function\nwith the period @xmath43 $ ] are negative .\nsecondly , a percentage of the contribution of the\nsum of products of positive values of @xmath25 and\n@xmath51 to the sum of positive products of the\nvalues of @xmath25 and @xmath51 should be\nevaluated . as a result the indexes @xmath65 for\neach set @xmath41 where @xmath44 is the index from\nthe set @xmath56 are obtained . thirdly , from all\nsets @xmath41 such that @xmath70 the set @xmath73\nfor which the index @xmath65 is the greatest\nshould be chosen .    the implication ( 8) is true\nwhen the set @xmath73 includes the considered\nperiod @xmath43 $ ] . this means that the greatest\ncontribution of positive values of the\nautocorrelation function and positive cosines with\nthe period @xmath43 $ ] to the periodogram value\n@xmath20 is caused by the sum of positive products\nof @xmath74 for each @xmath75-\\frac{m}{2k},[\\frac{\n2m}{k}]+\\frac{m}{2k})$ ] .    when the implication\n( 8) is false , the peak @xmath20 is mainly\ncreated by the sum of positive products of\n@xmath74 for each @xmath76-\\frac{m}{2k},\\big [\n\\frac{2m}{n}\\big ] + \\frac{m}{2k } \\big ) $ ] ,\nwhere @xmath77 is a multiple or a divisor of\n@xmath21 . it is necessary to add , that the de\nmethod should be applied to the periodograms peaks\n, which probably exist because of the echo -\neffect . it enables one to find such parts of the\nautocorrelation function , which have the\nsignificant contribution to the considered peak .\nthe fact , that the conditions ( 7 ) and ( 8) are\nsatisfied , can unambiguously decide about the\nexistence of the considered periodicity in the\ngiven time series , but if at least one of them is\nnot satisfied , one can doubt about the existence\nof the considered periodicity . thus , in such\ncases the sentence the peak can not be treated as\ntrue should be used .    using the de method it is\nnecessary to remember about the power of the set\n@xmath78 . if @xmath79 is too large , errors of an\nautocorrelation function estimation appear . they\nare caused by the finite length of the given time\nseries and as a result additional peaks of the\nperiodogram occur . if @xmath79 is too small ,\nthere are less peaks because of a low resolution\nof the periodogram . in applications @xmath80 is\nused . in order to evaluate the value @xmath79 the\nfft method is used . the periodograms computed by\nthe bt and the fft method are compared . the\nconformity of them enables one to obtain the value\n@xmath79 .    . the fft periodogram values are\npresented on the left axis . the lower curve\nillustrates the bt periodogram of the same time\nseries ( solid line and large black circles ) .\nthe bt periodogram values are shown in the right\naxis . ] in this paper the sunspot activity data (\naugust 1923 - october 1933 ) provided by the\ngreenwich photoheliographic results ( gpr ) are\nanalysed . firstly , i consider the monthly\nsunspot number data . to eliminate the 11-year\ntrend from these data , the consecutively smoothed\nmonthly sunspot number @xmath81 is subtracted from\nthe monthly sunspot number @xmath82 where the\nconsecutive mean @xmath83 is given by @xmath84 the\nvalues @xmath83 for @xmath85 and @xmath86 are\ncalculated using additional data from last six\nmonths of cycle 15 and first six months of cycle\n17 .    because of the north - south asymmetry of\nvarious solar indices @xcite , the sunspot\nactivity is considered for each solar hemisphere\nseparately . analogously to the monthly sunspot\nnumbers , the time series of sunspot areas in the\nnorthern and southern hemispheres with the spacing\ninterval @xmath87 rotation are denoted . in order\nto find periodicities , the following time series\nare used : + @xmath88   + @xmath89    + @xmath90\n+ in the lower part of figure [ f1 ] the\nautocorrelation function of the time series for\nthe northern hemisphere @xmath88 is shown . it is\neasy to notice that the prominent peak falls at 17\nrotations interval ( 459 days ) and @xmath25 for\n@xmath91 $ ] rotations ( [ 81 , 162 ] days ) are\nsignificantly negative . the periodogram of the\ntime series @xmath88 ( see the upper curve in\nfigures [ f1 ] ) does not show the significant\npeaks at @xmath92 rotations ( 135 , 162 days ) ,\nbut there is the significant peak at @xmath93 (\n243 days ) . the peaks at @xmath94 are close to\nthe peaks of the autocorrelation function . thus ,\nthe result obtained for the periodicity at about\n@xmath0 days are contradict to the results\nobtained for the time series of daily sunspot\nareas @xcite .    for the southern hemisphere (\nthe lower curve in figure [ f2 ] ) @xmath25 for\n@xmath95 $ ] rotations ( [ 54 , 189 ] days ) is\nnot positive except @xmath96 ( 135 days ) for\nwhich @xmath97 is not statistically significant .\nthe upper curve in figures [ f2 ] presents the\nperiodogram of the time series @xmath89 . this\ntime series does not contain a markov - type\npersistence . moreover , the kolmogorov - smirnov\ntest and the fisher test do not reject a null\nhypothesis that the time series is a white noise\nonly . this means that the time series do not\ncontain an added deterministic periodic component\nof unspecified frequency . the autocorrelation\nfunction of the time series @xmath90 ( the lower\ncurve in figure [ f3 ] ) has only one\nstatistically significant peak for @xmath98 months\n( 480 days ) and negative values for @xmath99 $ ]\nmonths ( [ 90 , 390 ] days ) . however , the\nperiodogram of this time series ( the upper curve\nin figure [ f3 ] ) has two significant peaks the\nfirst at 15.2 and the second at 5.3 months ( 456 ,\n159 days ) . thus , the periodogram contains the\nsignificant peak , although the autocorrelation\nfunction has the negative value at @xmath100\nmonths .    to explain these problems two\nfollowing time series of daily sunspot areas are\nconsidered : + @xmath101   + @xmath102     + where\n@xmath103    the values @xmath104 for @xmath105\nand @xmath106 are calculated using additional\ndaily data from the solar cycles 15 and 17 .\nand the cosine function for @xmath45 ( the period\nat about 154 days ) . the horizontal line ( dotted\nline ) shows the zero level . the vertical dotted\nlines evaluate the intervals where the sets\n@xmath107 ( for @xmath108 ) are searched . the\npercentage values show the index @xmath65 for each\n@xmath41 for the time series @xmath102 ( in\nparentheses for the time series @xmath101 ) . in\nthe right bottom corner the values of @xmath65 for\nthe time series @xmath102 , for @xmath109 are\nwritten . ] ( the 500-day period ) ]    the\ncomparison of the functions @xmath25 of the time\nseries @xmath101 ( the lower curve in figure [ f4\n] ) and @xmath102 ( the lower curve in figure [ f5\n] ) suggests that the positive values of the\nfunction @xmath110 of the time series @xmath101 in\nthe interval of @xmath111 $ ] days could be caused\nby the 11-year cycle . this effect is not visible\nin the case of periodograms of the both time\nseries computed using the fft method ( see the\nupper curves in figures [ f4 ] and [ f5 ] ) or the\nbt method ( see the lower curve in figure [ f6 ] )\n. moreover , the periodogram of the time series\n@xmath102 has the significant values at @xmath112\ndays , but the autocorrelation function is\nnegative at these points . @xcite showed that the\nlomb - scargle periodograms for the both time\nseries ( see @xcite , figures 7 a - c ) have a\npeak at 158.8 days which stands over the fap level\nby a significant amount . using the de method the\nabove discrepancies are obvious . to establish the\n@xmath79 value the periodograms computed by the\nfft and the bt methods are shown in figure [ f6 ]\n( the upper and the lower curve respectively ) .\nfor @xmath46 and for periods less than 166 days\nthere is a good comformity of the both\nperiodograms ( but for periods greater than 166\ndays the points of the bt periodogram are not\nlinked because the bt periodogram has much worse\nresolution than the fft periodogram ( no one know\nhow to do it ) ) . for @xmath46 and @xmath113 the\nvalue of @xmath21 is 13 ( @xmath71=153 $ ] ) . the\ninequality ( 7 ) is satisfied because @xmath114 .\nthis means that the value of @xmath115 is mainly\ncreated by positive values of the autocorrelation\nfunction . the implication ( 8) needs an\nevaluation of the greatest value of the index\n@xmath65 where @xmath70 , but the solar data\ncontain the most prominent period for @xmath116\ndays because of the solar rotation . thus ,\nalthough @xmath117 for each @xmath118 , all sets\n@xmath41 ( see ( 5 ) and ( 6 ) ) without the set\n@xmath119 ( see ( 4 ) ) , which contains @xmath120\n$ ] , are considered . this situation is presented\nin figure [ f7 ] . in this figure two curves\n@xmath121 and @xmath122 are plotted . the vertical\ndotted lines evaluate the intervals where the sets\n@xmath107 ( for @xmath123 ) are searched . for\nsuch @xmath41 two numbers are written : in\nparentheses the value of @xmath65 for the time\nseries @xmath101 and above it the value of\n@xmath65 for the time series @xmath102 . to make\nthis figure clear the curves are plotted for the\nset @xmath124 only . ( in the right bottom corner\ninformation about the values of @xmath65 for the\ntime series @xmath102 , for @xmath109 are written\n. ) the implication ( 8) is not true , because\n@xmath125 for @xmath126 . therefore ,\n@xmath43=153\\notin c_6=[423,500]$ ] . moreover ,\nthe autocorrelation function for @xmath127 $ ] is\nnegative and the set @xmath128 is empty . thus ,\n@xmath129 . on the basis of these information one\ncan state , that the periodogram peak at @xmath130\ndays of the time series @xmath102 exists because\nof positive @xmath25 , but for @xmath23 from the\nintervals which do not contain this period .\nlooking at the values of @xmath65 of the time\nseries @xmath101 , one can notice that they\ndecrease when @xmath23 increases until @xmath131 .\nthis indicates , that when @xmath23 increases ,\nthe contribution of the 11-year cycle to the peaks\nof the periodogram decreases . an increase of the\nvalue of @xmath65 is for @xmath132 for the both\ntime series , although the contribution of the\n11-year cycle for the time series @xmath101 is\ninsignificant . thus , this part of the\nautocorrelation function ( @xmath133 for the time\nseries @xmath102 ) influences the @xmath21-th peak\nof the periodogram . this suggests that the\nperiodicity at about 155 days is a harmonic of the\nperiodicity from the interval of @xmath1 $ ] days\n. ( solid line ) and consecutively smoothed\nsunspot areas of the one rotation time interval\n@xmath134 ( dotted line ) . both indexes are\npresented on the left axis . the lower curve\nillustrates fluctuations of the sunspot areas\n@xmath135 . the dotted and dashed horizontal lines\nrepresent levels zero and @xmath136 respectively .\nthe fluctuations are shown on the right axis . ]\nthe described reasoning can be carried out for\nother values of the periodogram . for example ,\nthe condition ( 8) is not satisfied for @xmath137\n( 250 , 222 , 200 days ) . moreover , the\nautocorrelation function at these points is\nnegative . these suggest that there are not a true\nperiodicity in the interval of [ 200 , 250 ] days\n. it is difficult to decide about the existence of\nthe periodicities for @xmath138 ( 333 days ) and\n@xmath139 ( 286 days ) on the basis of above\nanalysis . the implication ( 8) is not satisfied\nfor @xmath139 and the condition ( 7 ) is not\nsatisfied for @xmath138 , although the function\n@xmath25 of the time series @xmath102 is\nsignificantly positive for @xmath140 . the\nconditions ( 7 ) and ( 8) are satisfied for\n@xmath141 ( figure [ f8 ] ) and @xmath142 .\ntherefore , it is possible to exist the\nperiodicity from the interval of @xmath1 $ ] days\n. similar results were also obtained by @xcite for\ndaily sunspot numbers and daily sunspot areas .\nshe considered the means of three periodograms of\nthese indexes for data from @xmath143 years and\nfound statistically significant peaks from the\ninterval of @xmath1 $ ] ( see @xcite , figure 2 )\n. @xcite studied sunspot areas from 1876 - 1999\nand sunspot numbers from 1749 - 2001 with the help\nof the wavelet transform . they pointed out that\nthe 154 - 158-day period could be the third\nharmonic of the 1.3-year ( 475-day ) period .\nmoreover , the both periods fluctuate considerably\nwith time , being stronger during stronger sunspot\ncycles . therefore , the wavelet analysis suggests\na common origin of the both periodicities . this\nconclusion confirms the de method result which\nindicates that the periodogram peak at @xmath144\ndays is an alias of the periodicity from the\ninterval of @xmath1 $ ] in order to verify the\nexistence of the periodicity at about 155 days i\nconsider the following time series : + @xmath145\n+ @xmath146    + @xmath147   + the value @xmath134\nis calculated analogously to @xmath83 ( see sect .\nthe values @xmath148 and @xmath149 are evaluated\nfrom the formula ( 9 ) . in the upper part of\nfigure [ f9 ] the time series of sunspot areas\n@xmath150 of the one rotation time interval from\nthe whole solar disk and the time series of\nconsecutively smoothed sunspot areas @xmath151 are\nshowed . in the lower part of figure [ f9 ] the\ntime series of sunspot area fluctuations @xmath145\nis presented . on the basis of these data the\nmaximum activity period of cycle 16 is evaluated .\nit is an interval between two strongest\nfluctuations e.a . @xmath152 $ ] rotations . the\nlength of the time interval @xmath153 is 54\nrotations . if the about @xmath0-day ( 6 solar\nrotations ) periodicity existed in this time\ninterval and it was characteristic for strong\nfluctuations from this time interval , 10 local\nmaxima in the set of @xmath154 would be seen .\nthen it should be necessary to find such a value\nof p for which @xmath155 for @xmath156 and the\nnumber of the local maxima of these values is 10 .\nas it can be seen in the lower part of figure [ f9\n] this is for the case of @xmath157 ( in this\nfigure the dashed horizontal line is the level of\n@xmath158 ) . figure [ f10 ] presents nine time\ndistances among the successive fluctuation local\nmaxima and the horizontal line represents the\n6-rotation periodicity . it is immediately\napparent that the dispersion of these points is 10\nand it is difficult to find even few points which\noscillate around the value of 6 . such an analysis\nwas carried out for smaller and larger @xmath136\nand the results were similar . therefore , the\nfact , that the about @xmath0-day periodicity\nexists in the time series of sunspot area\nfluctuations during the maximum activity period is\nquestionable .    . the horizontal line represents\nthe 6-rotation ( 162-day ) period . ]    ]    ]\nto verify again the existence of the about\n@xmath0-day periodicity during the maximum\nactivity period in each solar hemisphere\nseparately , the time series @xmath88 and @xmath89\nwere also cut down to the maximum activity period\n( january 1925december 1930 ) . the comparison of\nthe autocorrelation functions of these time series\nwith the appriopriate autocorrelation functions of\nthe time series @xmath88 and @xmath89 , which are\ncomputed for the whole 11-year cycle ( the lower\ncurves of figures [ f1 ] and [ f2 ] ) , indicates\nthat there are not significant differences between\nthem especially for @xmath23=5 and 6 rotations (\n135 and 162 days ) ) . this conclusion is\nconfirmed by the analysis of the time series\n@xmath146 for the maximum activity period . the\nautocorrelation function ( the lower curve of\nfigure [ f11 ] ) is negative for the interval of [\n57 , 173 ] days , but the resolution of the\nperiodogram is too low to find the significant\npeak at @xmath159 days . the autocorrelation\nfunction gives the same result as for daily\nsunspot area fluctuations from the whole solar\ndisk ( @xmath160 ) ( see also the lower curve of\nfigures [ f5 ] ) . in the case of the time series\n@xmath89 @xmath161 is zero for the fluctuations\nfrom the whole solar cycle and it is almost zero (\n@xmath162 ) for the fluctuations from the maximum\nactivity period . the value @xmath163 is negative\n. similarly to the case of the northern hemisphere\nthe autocorrelation function and the periodogram\nof southern hemisphere daily sunspot area\nfluctuations from the maximum activity period\n@xmath147 are computed ( see figure [ f12 ] ) .\nthe autocorrelation function has the statistically\nsignificant positive peak in the interval of [ 155\n, 165 ] days , but the periodogram has too low\nresolution to decide about the possible\nperiodicities . the correlative analysis indicates\nthat there are positive fluctuations with time\ndistances about @xmath0 days in the maximum\nactivity period . the results of the analyses of\nthe time series of sunspot area fluctuations from\nthe maximum activity period are contradict with\nthe conclusions of @xcite . she uses the power\nspectrum analysis only . the periodogram of daily\nsunspot fluctuations contains peaks , which could\nbe harmonics or subharmonics of the true\nperiodicities . they could be treated as real\nperiodicities . this effect is not visible for\nsunspot data of the one rotation time interval ,\nbut averaging could lose true periodicities . this\nis observed for data from the southern hemisphere\n. there is the about @xmath0-day peak in the\nautocorrelation function of daily fluctuations ,\nbut the correlation for data of the one rotation\ninterval is almost zero or negative at the points\n@xmath164 and 6 rotations . thus , it is\nreasonable to research both time series together\nusing the correlative and the power spectrum\nanalyses . the following results are obtained :\n1 . a new method of the detection of statistically\nsignificant peaks of the periodograms enables one\nto identify aliases in the periodogram . 2 .   two\neffects cause the existence of the peak of the\nperiodogram of the time series of sunspot area\nfluctuations at about @xmath0 days : the first is\ncaused by the 27-day periodicity , which probably\ncreates the 162-day periodicity ( it is a\nsubharmonic frequency of the 27-day periodicity )\nand the second is caused by statistically\nsignificant positive values of the autocorrelation\nfunction from the intervals of @xmath165 $ ] and\n@xmath166 $ ] days . the existence of the\nperiodicity of about @xmath0 days of the time\nseries of sunspot area fluctuations and sunspot\narea fluctuations from the northern hemisphere\nduring the maximum activity period is questionable\n. the autocorrelation analysis of the time series\nof sunspot area fluctuations from the southern\nhemisphere indicates that the periodicity of about\n155 days exists during the maximum activity period\n. i appreciate valuable comments from professor j.\njakimiec .\"\"\"\n\nfrom transformers import LEDForConditionalGeneration, LEDTokenizer\nimport torch\n\ntokenizer = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n\ninput_ids = tokenizer(LONG_ARTICLE, return_tensors=\"pt\").input_ids.to(\"cuda\")\nglobal_attention_mask = torch.zeros_like(input_ids)\n# set global_attention_mask on first token\nglobal_attention_mask[:, 0] = 1\n\nmodel = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\", return_dict_in_generate=True).to(\"cuda\")\n\nsequences = model.generate(input_ids, global_attention_mask=global_attention_mask).sequences\n\nsummary = tokenizer.batch_decode(sequences)\n"], "andreasmadsen/efficient_mlm_m0.40": [], "microsoft/beit-base-patch16-224-pt22k": ["from transformers import BeitFeatureExtractor, BeitForMaskedImageModeling\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k')\nmodel = BeitForMaskedImageModeling.from_pretrained('microsoft/beit-base-patch16-224-pt22k')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n"], "google/bigbird-roberta-large": ["from transformers import BigBirdModel\n\n# by default its in `block_sparse` mode with num_random_blocks=3, block_size=64\nmodel = BigBirdModel.from_pretrained(\"google/bigbird-roberta-large\")\n\n# you can change `attention_type` to full attention like this:\nmodel = BigBirdModel.from_pretrained(\"google/bigbird-roberta-large\", attention_type=\"original_full\")\n\n# you can change `block_size` & `num_random_blocks` like this:\nmodel = BigBirdModel.from_pretrained(\"google/bigbird-roberta-large\", block_size=16, num_random_blocks=2)\n\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n"], "albert-xlarge-v2": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u00e2\u2013\u0081modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u00e2\u2013\u0081modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u00e2\u2013\u0081model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u00e2\u2013\u0081runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u00e2\u2013\u0081lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\nmodel = AlbertModel.from_pretrained(\"albert-xlarge-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\nmodel = TFAlbertModel.from_pretrained(\"albert-xlarge-v2\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xlarge-v2')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u00e2\u2013\u0081shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u00e2\u2013\u0081blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u00e2\u2013\u0081lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u00e2\u2013\u0081receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u00e2\u2013\u0081paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u00e2\u2013\u0081waitress\"\n   }\n]\n"], "google/efficientnet-b7": ["import torch\nfrom datasets import load_dataset\nfrom transformers import EfficientNetImageProcessor, EfficientNetForImageClassification\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\npreprocessor = EfficientNetImageProcessor.from_pretrained(\"google/efficientnet-b7\")\nmodel = EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b7\")\n\ninputs = preprocessor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "stabilityai/StableBeluga2": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga2\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga2\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\nsystem_prompt = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n\nmessage = \"Write me a poem please\"\nprompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n", "@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n"], "bofenghuang/vigogne-2-7b-chat": ["from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bofenghuang/vigogne-2-7b-chat\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": \"Bonjour ! Comment \u00e7a va aujourd'hui ?\"},\n    {\"role\": \"assistant\", \"content\": \"Bonjour ! Je suis une IA, donc je n'ai pas de sentiments, mais je suis pr\u00eat \u00e0 vous aider. Comment puis-je vous assister aujourd'hui ?\"},\n    {\"role\": \"user\", \"content\": \"Quelle est la hauteur de la Tour Eiffel ?\"},\n    {\"role\": \"assistant\", \"content\": \"La Tour Eiffel mesure environ 330 m\u00e8tres de hauteur.\"},\n    {\"role\": \"user\", \"content\": \"Comment monter en haut ?\"},\n]\n\nprint(tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True))\n", "from typing import Dict, List, Optional\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TextStreamer\n\nmodel_name_or_path = \"bofenghuang/vigogne-2-7b-chat\"\nrevision = \"v2.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, revision=revision, padding_side=\"right\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, revision=revision, torch_dtype=torch.float16, device_map=\"auto\")\n\nstreamer = TextStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n\n\ndef chat(\n    query: str,\n    history: Optional[List[Dict]] = None,\n    temperature: float = 0.7,\n    top_p: float = 1.0,\n    top_k: float = 0,\n    repetition_penalty: float = 1.1,\n    max_new_tokens: int = 1024,\n    **kwargs,\n):\n    if history is None:\n        history = []\n\n    history.append({\"role\": \"user\", \"content\": query})\n\n    input_ids = tokenizer.apply_chat_template(history, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n    input_length = input_ids.shape[1]\n\n    generated_outputs = model.generate(\n        input_ids=input_ids,\n        generation_config=GenerationConfig(\n            temperature=temperature,\n            do_sample=temperature > 0.0,\n            top_p=top_p,\n            top_k=top_k,\n            repetition_penalty=repetition_penalty,\n            max_new_tokens=max_new_tokens,\n            pad_token_id=tokenizer.eos_token_id,\n            **kwargs,\n        ),\n        streamer=streamer,\n        return_dict_in_generate=True,\n    )\n\n    generated_tokens = generated_outputs.sequences[0, input_length:]\n    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n    history.append({\"role\": \"assistant\", \"content\": generated_text})\n\n    return generated_text, history\n\n\n# 1st round\nresponse, history = chat(\"Un escargot parcourt 100 m\u00e8tres en 5 heures. Quelle est sa vitesse ?\", history=None)\n\n# 2nd round\nresponse, history = chat(\"Quand il peut d\u00e9passer le lapin ?\", history=history)\n\n# 3rd round\nresponse, history = chat(\"\u00c9cris une histoire imaginative qui met en sc\u00e8ne une comp\u00e9tition de course entre un escargot et un lapin.\", history=history)\n", "import openai\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai.api_key = \"EMPTY\"\nopenai.api_base = \"http://localhost:8000/v1\"\n\n# First model\nmodels = openai.Model.list()\nmodel = models[\"data\"][0][\"id\"]\n\n# Chat completion API\nchat_completion = openai.ChatCompletion.create(\n    model=model,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Parle-moi de toi-m\u00eame.\"},\n    ],\n    max_tokens=1024,\n    temperature=0.7,\n)\nprint(\"Chat completion results:\", chat_completion)\n"], "TheBloke/stable-vicuna-13B-HF": ["@misc{gpt4all,\n  author = {Yuvanesh Anand and Zach Nussbaum and Brandon Duderstadt and Benjamin Schmidt and Andriy Mulyar},\n  title = {GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/nomic-ai/gpt4all}},\n}\n"], "lmsys/vicuna-7b-delta-v1.1": [], "albert-large-v1": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-large-v1')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u00e2\u2013\u0081modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u00e2\u2013\u0081modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u00e2\u2013\u0081model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u00e2\u2013\u0081runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u00e2\u2013\u0081lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-large-v1')\nmodel = AlbertModel.from_pretrained(\"albert-large-v1\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-large-v1')\nmodel = TFAlbertModel.from_pretrained(\"albert-large-v1\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-large-v1')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u00e2\u2013\u0081shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u00e2\u2013\u0081blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u00e2\u2013\u0081lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u00e2\u2013\u0081receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u00e2\u2013\u0081paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u00e2\u2013\u0081waitress\"\n   }\n]\n"], "google/mobilenet_v2_1.4_224": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\npreprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v2_1.4_224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v2_1.4_224\")\n\ninputs = preprocessor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "google-bert/bert-large-cased-whole-word-masking": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-cased-whole-word-masking')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] Hello I'm a fashion model. [SEP]\",\n      \"score\":0.1474294513463974,\n      \"token\":4633,\n      \"token_str\":\"fashion\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a magazine model. [SEP]\",\n      \"score\":0.05430116504430771,\n      \"token\":2435,\n      \"token_str\":\"magazine\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a male model. [SEP]\",\n      \"score\":0.039395421743392944,\n      \"token\":2581,\n      \"token_str\":\"male\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a former model. [SEP]\",\n      \"score\":0.036936815828084946,\n      \"token\":1393,\n      \"token_str\":\"former\"\n   },\n   {\n      \"sequence\":\"[CLS] Hello I'm a professional model. [SEP]\",\n      \"score\":0.03663451969623566,\n      \"token\":1848,\n      \"token_str\":\"professional\"\n   }\n]\n", "from transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking')\nmodel = BertModel.from_pretrained(\"bert-large-cased-whole-word-masking\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking')\nmodel = TFBertModel.from_pretrained(\"bert-large-cased-whole-word-masking\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-large-cased-whole-word-masking')\n>>> unmasker(\"The man worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] The man worked as a carpenter. [SEP]\",\n      \"score\":0.09021259099245071,\n      \"token\":25169,\n      \"token_str\":\"carpenter\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a cook. [SEP]\",\n      \"score\":0.08125395327806473,\n      \"token\":9834,\n      \"token_str\":\"cook\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a mechanic. [SEP]\",\n      \"score\":0.07524766772985458,\n      \"token\":19459,\n      \"token_str\":\"mechanic\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a waiter. [SEP]\",\n      \"score\":0.07397029548883438,\n      \"token\":17989,\n      \"token_str\":\"waiter\"\n   },\n   {\n      \"sequence\":\"[CLS] The man worked as a guard. [SEP]\",\n      \"score\":0.05848982185125351,\n      \"token\":3542,\n      \"token_str\":\"guard\"\n   }\n]\n\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n[\n   {\n      \"sequence\":\"[CLS] The woman worked as a maid. [SEP]\",\n      \"score\":0.19436432421207428,\n      \"token\":13487,\n      \"token_str\":\"maid\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a waitress. [SEP]\",\n      \"score\":0.16161060333251953,\n      \"token\":15098,\n      \"token_str\":\"waitress\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a nurse. [SEP]\",\n      \"score\":0.14942803978919983,\n      \"token\":7439,\n      \"token_str\":\"nurse\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a secretary. [SEP]\",\n      \"score\":0.10373266786336899,\n      \"token\":4848,\n      \"token_str\":\"secretary\"\n   },\n   {\n      \"sequence\":\"[CLS] The woman worked as a cook. [SEP]\",\n      \"score\":0.06384387612342834,\n      \"token\":9834,\n      \"token_str\":\"cook\"\n   }\n]\n"], "weiweishi/roc-bert-base-zh": ["# pip install transformers>=4.25.1\n\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"weiweishi/roc-bert-base-zh\")\nmodel = AutoModel.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n"], "microsoft/git-large-textcaps": [], "wenge-research/yayi-7b-llama2": ["import torch \nfrom transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\npretrained_model_name_or_path = \"wenge-research/yayi-7b-llama2\"\ntokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path)\nmodel = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=False)\n\n# Define the stopping criteria\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords_ids:list):\n        self.keywords = keywords_ids\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n            return True\n        return False\n\nstop_words = [\"<|End|>\", \"<|YaYi|>\", \"<|Human|>\", \"</s>\"]\nstop_ids = [tokenizer.encode(w)[-1] for w in stop_words]\nstop_criteria = KeywordsStoppingCriteria(stop_ids)\n\n# inference\nprompt = \"\u4f60\u662f\u8c01\uff1f\"\nformatted_prompt = f\"\"\"<|System|>:\nYou are a helpful, respectful and honest assistant named YaYi developed by Beijing Wenge Technology Co.,Ltd. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\n<|Human|>:\n{prompt}\n\n<|YaYi|>:\n\"\"\"\n\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=256,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config, stopping_criteria=StoppingCriteriaList([stop_criteria]))\nresponse = [response[0][len(inputs.input_ids[0]):]]\nresponse_str = tokenizer.batch_decode(response, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\nprint(response_str)\n", "import torch \nfrom transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\npretrained_model_name_or_path = \"wenge-research/yayi-7b-llama2\"\ntokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path)\nmodel = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=False)\n\n# Define the stopping criteria\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords_ids:list):\n        self.keywords = keywords_ids\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n            return True\n        return False\n\nstop_words = [\"<|End|>\", \"<|YaYi|>\", \"<|Human|>\", \"</s>\"]\nstop_ids = [tokenizer.encode(w)[-1] for w in stop_words]\nstop_criteria = KeywordsStoppingCriteria(stop_ids)\n\n# inference\nprompt = \"\u4f60\u662f\u8c01\uff1f\"\nformatted_prompt = f\"\"\"<|System|>:\nYou are a helpful, respectful and honest assistant named YaYi developed by Beijing Wenge Technology Co.,Ltd. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\n<|Human|>:\n{prompt}\n\n<|YaYi|>:\n\"\"\"\n\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=256,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config, stopping_criteria=StoppingCriteriaList([stop_criteria]))\nresponse = [response[0][len(inputs.input_ids[0]):]]\nresponse_str = tokenizer.batch_decode(response, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\nprint(response_str)\n"], "TheBloke/WizardLM-7B-uncensored-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"oobaCUDA\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "internlm/internlm-chat-7b": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b\", trust_remote_code=True)\n# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b\", torch_dtype=torch.float16, trust_remote_code=True).cuda()\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\nprint(response)\n# Hello! How can I help you today?\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\nprint(response)\n# Sure, here are three tips for effective time management:\n#\n# 1. Prioritize tasks based on importance and urgency: Make a list of all your tasks and categorize them into \"important and urgent,\" \"important but not urgent,\" and \"not important but urgent.\" Focus on completing the tasks in the first category before moving on to the others.\n# 2. Use a calendar or planner: Write down deadlines and appointments in a calendar or planner so you don't forget them. This will also help you schedule your time more effectively and avoid overbooking yourself.\n# 3. Minimize distractions: Try to eliminate any potential distractions when working on important tasks. Turn off notifications on your phone, close unnecessary tabs on your computer, and find a quiet place to work if possible.\n# \n# Remember, good time management skills take practice and patience. Start with small steps and gradually incorporate these habits into your daily routine.\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"internlm/internlm-chat-7b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\nmodel = model.eval()\nlength = 0\nfor response, history in model.stream_chat(tokenizer, \"Hello\", history=[]):\n    print(response[length:], flush=True, end=\"\")\n    length = len(response)\n", "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b\", trust_remote_code=True)\n# `torch_dtype=torch.float16` \u53ef\u4ee5\u4ee4\u6a21\u578b\u4ee5 float16 \u7cbe\u5ea6\u52a0\u8f7d\uff0c\u5426\u5219 transformers \u4f1a\u5c06\u6a21\u578b\u52a0\u8f7d\u4e3a float32\uff0c\u5bfc\u81f4\u663e\u5b58\u4e0d\u8db3\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b\", torch_dtype=torch.float16, trust_remote_code=True).cuda()\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=[])\nprint(response)\n# \u4f60\u597d\uff01\u6709\u4ec0\u4e48\u6211\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7684\u5417\uff1f\nresponse, history = model.chat(tokenizer, \"\u8bf7\u63d0\u4f9b\u4e09\u4e2a\u7ba1\u7406\u65f6\u95f4\u7684\u5efa\u8bae\u3002\", history=history)\nprint(response)\n# \u5f53\u7136\u53ef\u4ee5\uff01\u4ee5\u4e0b\u662f\u4e09\u4e2a\u7ba1\u7406\u65f6\u95f4\u7684\u5efa\u8bae\uff1a\n# 1. \u5236\u5b9a\u8ba1\u5212\uff1a\u5236\u5b9a\u4e00\u4e2a\u8be6\u7ec6\u7684\u8ba1\u5212\uff0c\u5305\u62ec\u6bcf\u5929\u8981\u5b8c\u6210\u7684\u4efb\u52a1\u548c\u6d3b\u52a8\u3002\u8fd9\u5c06\u6709\u52a9\u4e8e\u60a8\u66f4\u597d\u5730\u7ec4\u7ec7\u65f6\u95f4\uff0c\u5e76\u786e\u4fdd\u60a8\u80fd\u591f\u6309\u65f6\u5b8c\u6210\u4efb\u52a1\u3002\n# 2. \u4f18\u5148\u7ea7\uff1a\u5c06\u4efb\u52a1\u6309\u7167\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u5148\u5b8c\u6210\u6700\u91cd\u8981\u7684\u4efb\u52a1\u3002\u8fd9\u5c06\u786e\u4fdd\u60a8\u80fd\u591f\u5728\u6700\u77ed\u7684\u65f6\u95f4\u5185\u5b8c\u6210\u6700\u91cd\u8981\u7684\u4efb\u52a1\uff0c\u4ece\u800c\u8282\u7701\u65f6\u95f4\u3002\n# 3. \u96c6\u4e2d\u6ce8\u610f\u529b\uff1a\u907f\u514d\u5206\u5fc3\uff0c\u96c6\u4e2d\u6ce8\u610f\u529b\u5b8c\u6210\u4efb\u52a1\u3002\u5173\u95ed\u793e\u4ea4\u5a92\u4f53\u548c\u7535\u5b50\u90ae\u4ef6\u901a\u77e5\uff0c\u4e13\u6ce8\u4e8e\u4efb\u52a1\uff0c\u8fd9\u5c06\u5e2e\u52a9\u60a8\u66f4\u5feb\u5730\u5b8c\u6210\u4efb\u52a1\uff0c\u5e76\u51cf\u5c11\u9519\u8bef\u7684\u53ef\u80fd\u6027\u3002\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"internlm/internlm-chat-7b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dype=torch.float16, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\nmodel = model.eval()\nlength = 0\nfor response, history in model.stream_chat(tokenizer, \"\u4f60\u597d\", history=[]):\n    print(response[length:], flush=True, end=\"\")\n    length = len(response)\n"], "deepmind/vision-perceiver-learned": ["from transformers import PerceiverFeatureExtractor, PerceiverForImageClassificationLearned\nimport requests\nfrom PIL import Image\n\nfeature_extractor = PerceiverFeatureExtractor.from_pretrained(\"deepmind/vision-perceiver-learned\")\nmodel = PerceiverForImageClassificationLearned.from_pretrained(\"deepmind/vision-perceiver-learned\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# prepare input\nencoding = feature_extractor(image, return_tensors=\"pt\")\ninputs = encoding.pixel_values\n# forward pass\noutputs = model(inputs)\nlogits = outputs.logits\nprint(\"Predicted class:\", model.config.id2label[logits.argmax(-1).item()])\n>>> should print Predicted class: tabby, tabby cat\n"], "NousResearch/Nous-Hermes-13b": [], "FlagAlpha/Llama2-Chinese-13b-Chat": [], "timm/ViT-B-16-SigLIP-384": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-B-16-SigLIP-384')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP-384')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_base_patch16_siglip_384',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "unsloth/gemma-7b": [], "codeparrot/codeparrot-small": ["from transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\nmodel = AutoModelWithLMHead.from_pretrained(\"codeparrot/codeparrot-small\")\n\ninputs = tokenizer(\"def hello_world():\", return_tensors=\"pt\")\noutputs = model(**inputs)\n", "from transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"codeparrot/codeparrot-small\")\noutputs = pipe(\"def hello_world():\")\n"], "adept/persimmon-8b-chat": [], "mosaicml/mpt-7b-8k-chat": ["import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-chat-8k',\n  trust_remote_code=True\n)\n", "import torch\nimport transformers\n\nname = 'mosaicml/mpt-7b-chat-8k'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n", "import transformers\n\nname = 'mosaicml/mpt-7b-chat-8k'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n", "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-7b-8k')\n", "from transformers import pipeline\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**inputs, max_new_tokens=100)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n"], "stabilityai/stable-diffusion-xl-base-0.9": ["from diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-0.9\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"An astronaut riding a green horse\"\n\nimages = pipe(prompt=prompt).images[0]\n"], "bigscience/bloomz": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "Salesforce/codegen-6B-multi": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-6B-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-6B-multi\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "HuggingFaceH4/starchat-beta": ["import torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use a variant of ChatML to format each message\nprompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\nprompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n# We use a special <|end|> token with ID 49155 to denote ends of a turn\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n# You can sort a list in Python by using the sort() method. Here's an example:\\n\\n```\\nnumbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\\nnumbers.sort()\\nprint(numbers)\\n```\\n\\nThis will sort the list in place and print the sorted list.\n"], "RWKV/rwkv-4-430m-pile": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-430m-pile\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-430m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-430m-pile\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-430m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-430m-pile\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-430m-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "project-baize/baize-v2-7b": [], "albert-xlarge-v1": [">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xlarge-v1')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n   {\n      \"sequence\":\"[CLS] hello i'm a modeling model.[SEP]\",\n      \"score\":0.05816134437918663,\n      \"token\":12807,\n      \"token_str\":\"\u00e2\u2013\u0081modeling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a modelling model.[SEP]\",\n      \"score\":0.03748830780386925,\n      \"token\":23089,\n      \"token_str\":\"\u00e2\u2013\u0081modelling\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a model model.[SEP]\",\n      \"score\":0.033725276589393616,\n      \"token\":1061,\n      \"token_str\":\"\u00e2\u2013\u0081model\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a runway model.[SEP]\",\n      \"score\":0.017313428223133087,\n      \"token\":8014,\n      \"token_str\":\"\u00e2\u2013\u0081runway\"\n   },\n   {\n      \"sequence\":\"[CLS] hello i'm a lingerie model.[SEP]\",\n      \"score\":0.014405295252799988,\n      \"token\":29104,\n      \"token_str\":\"\u00e2\u2013\u0081lingerie\"\n   }\n]\n", "from transformers import AlbertTokenizer, AlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v1')\nmodel = AlbertModel.from_pretrained(\"albert-xlarge-v1\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n", "from transformers import AlbertTokenizer, TFAlbertModel\ntokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v1')\nmodel = TFAlbertModel.from_pretrained(\"albert-xlarge-v1\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n", ">>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='albert-xlarge-v1')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the man worked as a chauffeur.[SEP]\",\n      \"score\":0.029577180743217468,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a janitor.[SEP]\",\n      \"score\":0.028865724802017212,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a shoemaker.[SEP]\",\n      \"score\":0.02581118606030941,\n      \"token\":29024,\n      \"token_str\":\"\u00e2\u2013\u0081shoemaker\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a blacksmith.[SEP]\",\n      \"score\":0.01849772222340107,\n      \"token\":21238,\n      \"token_str\":\"\u00e2\u2013\u0081blacksmith\"\n   },\n   {\n      \"sequence\":\"[CLS] the man worked as a lawyer.[SEP]\",\n      \"score\":0.01820771023631096,\n      \"token\":3672,\n      \"token_str\":\"\u00e2\u2013\u0081lawyer\"\n   }\n]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[\n   {\n      \"sequence\":\"[CLS] the woman worked as a receptionist.[SEP]\",\n      \"score\":0.04604868218302727,\n      \"token\":25331,\n      \"token_str\":\"\u00e2\u2013\u0081receptionist\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a janitor.[SEP]\",\n      \"score\":0.028220869600772858,\n      \"token\":29477,\n      \"token_str\":\"\u00e2\u2013\u0081janitor\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a paramedic.[SEP]\",\n      \"score\":0.0261906236410141,\n      \"token\":23386,\n      \"token_str\":\"\u00e2\u2013\u0081paramedic\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a chauffeur.[SEP]\",\n      \"score\":0.024797942489385605,\n      \"token\":28744,\n      \"token_str\":\"\u00e2\u2013\u0081chauffeur\"\n   },\n   {\n      \"sequence\":\"[CLS] the woman worked as a waitress.[SEP]\",\n      \"score\":0.024124596267938614,\n      \"token\":13678,\n      \"token_str\":\"\u00e2\u2013\u0081waitress\"\n   }\n]\n"], "togethercomputer/GPT-NeoXT-Chat-Base-20B": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n# infer\ninputs = tokenizer(\"<human>: Hello!\\n<bot>:\", return_tensors='pt').to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=0.8)\noutput_str = tokenizer.decode(outputs[0])\nprint(output_str)\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\", device_map=\"auto\", load_in_8bit=True)\n# infer\ninputs = tokenizer(\"<human>: Hello!\\n<bot>:\", return_tensors='pt').to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=0.8)\noutput_str = tokenizer.decode(outputs[0])\nprint(output_str)\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\", torch_dtype=torch.bfloat16)\n# infer\ninputs = tokenizer(\"<human>: Hello!\\n<bot>:\", return_tensors='pt').to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=10, do_sample=True, temperature=0.8)\noutput_str = tokenizer.decode(outputs[0])\nprint(output_str)\n", "**Summarize a long document into a single sentence and conduct question answering related to the document, with multiple rounds**\n\n<human>: Last year, the travel industry saw a big rebound in demand \u2014 and that demand is showing no signs of slowing down this spring break travel season. Planes and hotels will be full, travelers will likely face long queues, cancellations, massive crowds and plenty of other travel nightmares. But perhaps the most frustrating thing you\u2019ll have to prepare for this spring break is if your luggage goes missing. You\u2019ve got to somehow track down your belongings that were in the airline\u2019s care and then wait to be reunited \u2014 all while trying to enjoy the start of your long-awaited vacation. As far as travel nightmares go, lost luggage is up there as one of the worst.\n\nTo be honest up front, I am great at strategically losing things \u2014 or rather, losing stuff with style. (I hope you appreciate the \u201cToy Story\u201d reference.) So when a product comes along that\u2019s a perfect way to keep track of my belongings \u2014 especially when traveling \u2014 I\u2019m adding it to my travel bag.\n\nI\u2019ve been a big fan of Apple\u2019s $29 item tracker, dubbed AirTag. The long-rumored AirTag product finally launched in 2021, and while it\u2019s not the first item tracker to exist, it has two key features that set it apart from the rest to make it the ultimate travel companion: access to the Find My network and Precision Finding technology.\n\nThe Federal Aviation Administration has even ruled that having an AirTag in your luggage is permitted and, in fact, not a safety hazard. The devices only contain 0.1 grams of lithium, which falls below the FAA\u2019s limit.\n\n\u201cLuggage tracking devices powered by lithium metal cells that have 0.3 grams or less of lithium can be used on checked baggage,\u201d the FAA said in a statement. \u201cApple AirTags meet this threshold; other luggage tracking devices may not.\u201d\n\nBut, you may be wondering what makes them so special and why you should carry one in your checked luggage this spring break? Let\u2019s explain why.\n\nSummarize the above into a single paragraph.\n\n<bot>: If you're traveling this spring break, you might want to consider investing in an item tracker to help you find your lost luggage. Apple's AirTag is the best option, according to a new report.\n\n<human>: But isn't it unsafe to put AirTag in a luggage?\n\n<bot>: The Federal Aviation Administration has ruled that having an AirTag in your luggage is permitted and, in fact, not a safety hazard.\n\n<human>: Ah, why is that?\n\n<bot>: The devices only contain 0.1 grams of lithium, which falls below the FAA\u2019s limit.\n", "**Extract structured information from unstructured documents, as illustrated in the following example.**\n\n<human>: Generate a table summarizing the options outlined in this email. \n\nTeam, we have to make a decision on the product plan for Goocrux v2. \n\nThe first option will take us 3 weeks of development, and will achieve performance of 80ms, but means that the UI has to change. \n\nThe second option will take us 4 weeks of development, and will achieve performance of 60ms, without any UI change. \n\nThe third option will take the longest -- 6 weeks of development, will achieve performance of 30ms, without any UI change. \n\nA markdown table with four columns: (1) Option ID, (2) Development time, (3) Performance, (4) UI Change:\n\n<bot>: #.\n| ID | Option ID | Development time | Performance | UI Change |\n| --------- | --------- | --------- | --------- | --------- |\n| 1 | 1 | 3 weeks | 80ms | Yes |\n| 2 | 2 | 4 weeks | 60ms | No |\n| 3 | 3 | 6 weeks | 30ms | No |\n"], "cl-tohoku/bert-base-japanese-char-whole-word-masking": [], "google/t5-base-lm-adapt": [], "TheBloke/guanaco-7B-HF": [], "deepseek-ai/deepseek-llm-67b-base": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"deepseek-ai/deepseek-llm-67b-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntext = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)\n"], "TheBloke/Wizard-Vicuna-7B-Uncensored-HF": [], "wenge-research/yayi-13b-llama2": ["import torch \nfrom transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\npretrained_model_name_or_path = \"wenge-research/yayi-13b-llama2\"\ntokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path)\nmodel = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=False)\n\n# Define the stopping criteria\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords_ids:list):\n        self.keywords = keywords_ids\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n            return True\n        return False\n\nstop_words = [\"<|End|>\", \"<|YaYi|>\", \"<|Human|>\", \"</s>\"]\nstop_ids = [tokenizer.encode(w)[-1] for w in stop_words]\nstop_criteria = KeywordsStoppingCriteria(stop_ids)\n\n# inference\nprompt = \"\u4f60\u662f\u8c01\uff1f\"\nformatted_prompt = f\"\"\"<|System|>:\nYou are a helpful, respectful and honest assistant named YaYi developed by Beijing Wenge Technology Co.,Ltd. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\n<|Human|>:\n{prompt}\n\n<|YaYi|>:\n\"\"\"\n\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=256,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config, stopping_criteria=StoppingCriteriaList([stop_criteria]))\nresponse = [response[0][len(inputs.input_ids[0]):]]\nresponse_str = tokenizer.batch_decode(response, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\nprint(response_str)\n", "import torch \nfrom transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\npretrained_model_name_or_path = \"wenge-research/yayi-13b-llama2\"\ntokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path)\nmodel = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=False)\n\n# Define the stopping criteria\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords_ids:list):\n        self.keywords = keywords_ids\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n            return True\n        return False\n\nstop_words = [\"<|End|>\", \"<|YaYi|>\", \"<|Human|>\", \"</s>\"]\nstop_ids = [tokenizer.encode(w)[-1] for w in stop_words]\nstop_criteria = KeywordsStoppingCriteria(stop_ids)\n\n# inference\nprompt = \"\u4f60\u662f\u8c01\uff1f\"\nformatted_prompt = f\"\"\"<|System|>:\nYou are a helpful, respectful and honest assistant named YaYi developed by Beijing Wenge Technology Co.,Ltd. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\n<|Human|>:\n{prompt}\n\n<|YaYi|>:\n\"\"\"\n\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=256,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config, stopping_criteria=StoppingCriteriaList([stop_criteria]))\nresponse = [response[0][len(inputs.input_ids[0]):]]\nresponse_str = tokenizer.batch_decode(response, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\nprint(response_str)\n"], "VMware/open-llama-7b-open-instruct": ["import os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = 'VMware/open-llama-7b-open-instruct'\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map='sequential')\n\nprompt_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\n\nprompt = 'Explain in simple terms how the attention mechanism of a transformer model works'\n\n\ninputt = prompt_template.format(instruction= prompt)\ninput_ids = tokenizer(inputt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutput1 = model.generate(input_ids, max_length=512)\ninput_length = input_ids.shape[1]\noutput1 = output1[:, input_length:]\noutput = tokenizer.decode(output1[0])\n\nprint(output)\n\n'''\n Attention is a mechanism used in deep learning models, such as transformer models, to capture global dependencies between different parts of the input. In a transformer model, the attention mechanism works by computing a weighted sum of the input vectors and then applying a non-linear activation function to the result.\n\nThe attention mechanism in a transformer model works in two steps:\n\n1. Query-Key Mapping: First, the input sequence is divided into two parts: the query vector and the key vector. The query vector represents the input at the current position, and the key vector represents the input at a previous position.\n\n2. Attention Weight Calculation: Second, the attention weights are calculated using the dot product between the query vector and each key vector. The attention weights represent the importance of the input at the previous position to the current position.\n\nThe attention weights are then used to compute the attention score for each input element. The attention score represents the relevance of the input element to the current position.\n\nThe attention mechanism in a transformer model is designed to capture global dependencies between different parts of the input. By attending to input elements from different positions, the model can learn to understand the relationships between different parts of the input. This allows the model to perform more complex tasks, such as understanding the relationships between words in a sentence or pixels in an image.</s>\n\n'''\n"], "microsoft/BioGPT-Large": [], "timm/ViT-L-16-SigLIP-384": ["import torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-L-16-SigLIP-384')\ntokenizer = get_tokenizer('hf-hub:timm/ViT-L-16-SigLIP-384')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\nlabels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\ntext = tokenizer(labels_list, context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = torch.sigmoid(image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias)\n\nzipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\nprint(\"Label probabilities: \", zipped_list)\n", "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'vit_large_patch16_siglip_384',\n    pretrained=True,\n    num_classes=0,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n"], "baichuan-inc/Baichuan2-13B-Base": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-13B-Base\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-13B-Base\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('\u767b\u9e73\u96c0\u697c->\u738b\u4e4b\u6da3\\n\u591c\u96e8\u5bc4\u5317->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n"], "FreedomIntelligence/phoenix-inst-chat-7b": [], "IDEA-CCNL/Ziya-LLaMA-13B-v1": ["from transformers import AutoTokenizer\nfrom transformers import LlamaForCausalLM\nimport torch\n\ndevice = torch.device(\"cuda\")\nckpt = '\u57fa\u4e8edelta\u53c2\u6570\u5408\u5e76\u540e\u7684\u5b8c\u6574\u6a21\u578b\u6743\u91cd'\n\nquery=\"\u5e2e\u6211\u5199\u4e00\u4efd\u53bb\u897f\u5b89\u7684\u65c5\u6e38\u8ba1\u5212\"\nmodel = LlamaForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)\ninputs = '<human>:' + query.strip() + '\\n<bot>:'\n      \ninput_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids.to(device)\ngenerate_ids = model.generate(\n            input_ids,\n            max_new_tokens=1024, \n            do_sample = True, \n            top_p = 0.85, \n            temperature = 1.0, \n            repetition_penalty=1., \n            eos_token_id=2, \n            bos_token_id=1, \n            pad_token_id=0)\noutput = tokenizer.batch_decode(generate_ids)[0]\nprint(output)\n", "from transformers import AutoTokenizer\nfrom transformers import LlamaForCausalLM\nimport torch\n\n\ndevice = torch.device(\"cuda\")\nckpt = '\u57fa\u4e8edelta\u5408\u5e76\u540e\u5b8c\u6574\u6a21\u578b\u6743\u91cd'\n\nquery=\"\u5e2e\u6211\u5199\u4e00\u4efd\u53bb\u897f\u5b89\u7684\u65c5\u6e38\u8ba1\u5212\"\nmodel = LlamaForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=False)\ninputs = '<human>:' + query.strip() + '\\n<bot>:'\n      \ninput_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids.to(device)\ngenerate_ids = model.generate(\n            input_ids,\n            max_new_tokens=1024, \n            do_sample = True, \n            top_p = 0.85, \n            temperature = 1.0, \n            repetition_penalty=1., \n            eos_token_id=2, \n            bos_token_id=1, \n            pad_token_id=0)\noutput = tokenizer.batch_decode(generate_ids)[0]\nprint(output)\n"], "laion/CLIP-ViT-L-14-CommonPool.XL.laion-s13B-b90K": [], "togethercomputer/RedPajama-INCITE-7B-Chat": ["import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Chat\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Chat\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n# infer\nprompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\nAlan Mathison Turing (23 June 1912  7 June 1954) was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, mathematician, and theoretical biologist.\n\"\"\"\n", "import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Chat\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Chat\", device_map='auto', torch_dtype=torch.float16, load_in_8bit=True)\n\n# infer\nprompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\nAlan Mathison Turing (23 June 1912 \u2013 7 June 1954) was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist.\n\"\"\"\n", "import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMIN_TRANSFORMERS_VERSION = '4.25.1'\n\n# check transformers version\nassert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n\n# init\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Chat\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Chat\", torch_dtype=torch.bfloat16)\n# infer\nprompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\nprint(output_str)\n\"\"\"\nAlan Mathison Turing, OBE, FRS, (23 June 1912 \u2013 7 June 1954) was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist.\n\"\"\"\n"], "microsoft/swin-large-patch4-window7-224": ["from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-large-patch4-window7-224\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-large-patch4-window7-224\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "RWKV/rwkv-4-7b-pile": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-7b-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-7b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-7b-pile\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-7b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-7b-pile\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-7b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-7b-pile\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-7b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "RWKV/rwkv-raven-3b": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-3b\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-3b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-3b\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-3b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-3b\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-3b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-3b\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-3b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "facebook/convnext-base-224": ["from transformers import ConvNextImageProcessor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-base-224\")\nmodel = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-base-224\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "RWKV/rwkv-raven-7b": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-7b\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-7b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-7b\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-7b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-7b\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-7b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-7b\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-7b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "RWKV/rwkv-4-3b-pile": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-3b-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-3b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-3b-pile\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-3b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-3b-pile\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-3b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-3b-pile\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-3b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "Salesforce/codegen-350M-nl": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-nl\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-nl\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "RWKV/rwkv-4-14b-pile": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-14b-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-14b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-14b-pile\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-14b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-14b-pile\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-14b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-14b-pile\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-14b-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "RWKV/rwkv-raven-14b": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-14b\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-14b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-14b\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-14b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-14b\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-14b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-14b\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-14b\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "facebook/incoder-1B": ["tokenizer.decode(tokenizer.encode(\"from .\"), clean_up_tokenization_spaces=False)"], "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(\n    model=\"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    use_fast=False,\n    device_map={\"\": \"cuda:0\"},\n)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=256,\n    do_sample=False,\n    num_beams=2,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n)\nprint(res[0][\"generated_text\"])\n", "import torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt\",\n    use_fast=False,\n    padding_side=\"left\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt\",\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cuda:0\"}\n)\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=256,\n    do_sample=False,\n    num_beams=2,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n)\nprint(res[0][\"generated_text\"])\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt\"  # either local folder or huggingface model name\n# Important: The prompt needs to be in the same format the model was trained with.\n# You can find an example prompt in the experiment logs.\nprompt = \"<|prompt|>How are you?</s><|answer|>\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.cuda().eval()\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n\n# generate configuration can be modified to your needs\ntokens = model.generate(\n    **inputs,\n    min_new_tokens=2,\n    max_new_tokens=256,\n    do_sample=False,\n    num_beams=2,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n)[0]\n\ntokens = tokens[inputs[\"input_ids\"].shape[1]:]\nanswer = tokenizer.decode(tokens, skip_special_tokens=True)\nprint(answer)\n"], "lmsys/vicuna-13b-delta-v1.1": [], "latent-consistency/lcm-sdxl": ["from diffusers import UNet2DConditionModel, DiffusionPipeline, LCMScheduler\nimport torch\n\nunet = UNet2DConditionModel.from_pretrained(\"latent-consistency/lcm-sdxl\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\")\n\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.to(\"cuda\")\n\nprompt = \"a close-up picture of an old man standing in the rain\"\n\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=8.0).images[0]\n"], "wenge-research/yayi-7b": ["from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nimport torch\n\nyayi_7b_path = \"wenge-research/yayi-7b\"\ntokenizer = AutoTokenizer.from_pretrained(yayi_7b_path)\nmodel = AutoModelForCausalLM.from_pretrained(yayi_7b_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\nprompt = \"\u4f60\u597d\"\nformatted_prompt = f\"<|System|>:\\nA chat between a human and an AI assistant named YaYi.\\nYaYi is a helpful and harmless language model developed by Beijing Wenge Technology Co.,Ltd.\\n\\n<|Human|>:\\n{prompt}\\n\\n<|YaYi|>:\"\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=100,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(response[0]))\n", "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nimport torch\n\nyayi_7b_path = \"wenge-research/yayi-7b\"\ntokenizer = AutoTokenizer.from_pretrained(yayi_7b_path)\nmodel = AutoModelForCausalLM.from_pretrained(yayi_7b_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\nprompt = \"\u4f60\u597d\"\nformatted_prompt = f\"<|System|>:\\nA chat between a human and an AI assistant named YaYi.\\nYaYi is a helpful and harmless language model developed by Beijing Wenge Technology Co.,Ltd.\\n\\n<|Human|>:\\n{prompt}\\n\\n<|YaYi|>:\"\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n\neos_token_id = tokenizer(\"<|End|>\").input_ids[0]\ngeneration_config = GenerationConfig(\n    eos_token_id=eos_token_id,\n    pad_token_id=eos_token_id,\n    do_sample=True,\n    max_new_tokens=100,\n    temperature=0.3,\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=0\n)\nresponse = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(response[0]))\n"], "h2oai/h2ogpt-oasst1-512-20b": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oasst1-512-20b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n", "import torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n"], "RWKV/rwkv-raven-1b5": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-1b5\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-1b5\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-1b5\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-1b5\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-1b5\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-1b5\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-raven-1b5\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-raven-1b5\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "bofenghuang/vigogne-2-7b-instruct": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nfrom vigogne.preprocess import generate_instruct_prompt\n\nmodel_name_or_path = \"bofenghuang/vigogne-2-7b-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"right\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\")\n\nuser_query = \"Expliquez la diff\u00e9rence entre DoS et phishing.\"\nprompt = generate_instruct_prompt(user_query)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\ninput_length = input_ids.shape[1]\n\ngenerated_outputs = model.generate(\n    input_ids=input_ids,\n    generation_config=GenerationConfig(\n        temperature=0.1,\n        do_sample=True,\n        repetition_penalty=1.0,\n        max_new_tokens=512,\n    ),\n    return_dict_in_generate=True,\n)\ngenerated_tokens = generated_outputs.sequences[0, input_length:]\ngenerated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\nprint(generated_text)\n", "def remove_duplicates(lst):\n    return list(set(lst))\n", "def remove_duplicates(list):\n   return list(set(list))\n", "def remove_duplicates(chaines):\n    return list(set(chaines))\n"], "microsoft/swin-large-patch4-window7-224-in22k": ["from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-large-patch4-window7-224-in22k\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-large-patch4-window7-224-in22k\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "RWKV/rwkv-4-1b5-pile": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-1b5-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-1b5-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-1b5-pile\").to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-1b5-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-1b5-pile\", torch_dtype=torch.float16).to(0)\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-1b5-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n", "# pip install accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"RWKV/rwkv-4-1b5-pile\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-1b5-pile\")\n\nprompt = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=40)\nprint(tokenizer.decode(output[0].tolist(), skip_special_tokens=True))\n"], "llmware/bling-1b-0.1": ["from transformers import AutoTokenizer, AutoModelForCausalLM  \ntokenizer = AutoTokenizer.from_pretrained(\"llmware/bling-1b-0.1\")  \nmodel = AutoModelForCausalLM.from_pretrained(\"llmware/bling-1b-0.1\")  \n"], "lmms-lab/LLaVA-NeXT-Video-34B-DPO": [], "hiyouga/Baichuan2-7B-Base-LLaMAfied": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"hiyouga/Baichuan2-7B-Base-LLaMAfied\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"hiyouga/Baichuan2-7B-Base-LLaMAfied\").cuda()\n"], "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2": ["import torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(\n    model=\"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    use_fast=False,\n    device_map={\"\": \"cuda:0\"},\n)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=256,\n    do_sample=False,\n    num_beams=2,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)\nprint(res[0][\"generated_text\"])\n", "import torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\",\n    use_fast=False,\n    padding_side=\"left\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\",\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cuda:0\"}\n)\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=256,\n    do_sample=False,\n    num_beams=2,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)\nprint(res[0][\"generated_text\"])\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\"  # either local folder or huggingface model name\n# Important: The prompt needs to be in the same format the model was trained with.\n# You can find an example prompt in the experiment logs.\nprompt = \"<|prompt|>How are you?</s><|answer|>\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.cuda().eval()\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n\n# generate configuration can be modified to your needs\ntokens = model.generate(\n    **inputs,\n    min_new_tokens=2,\n    max_new_tokens=256,\n    do_sample=False,\n    num_beams=2,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)[0]\n\ntokens = tokens[inputs[\"input_ids\"].shape[1]:]\nanswer = tokenizer.decode(tokens, skip_special_tokens=True)\nprint(answer)\n"], "deepmind/vision-perceiver-fourier": ["from transformers import PerceiverImageProcessor, PerceiverForImageClassificationFourier\nimport requests\nfrom PIL import Image\n\nprocessor = PerceiverImageProcessor.from_pretrained(\"deepmind/vision-perceiver-fourier\")\nmodel = PerceiverForImageClassificationFourier.from_pretrained(\"deepmind/vision-perceiver-fourier\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# prepare input\ninputs = processor(image, return_tensors=\"pt\").pixel_values\n# forward pass\noutputs = model(inputs)\nlogits = outputs.logits\nprint(\"Predicted class:\", model.config.id2label[logits.argmax(-1).item()])\n>>> should print Predicted class: tabby, tabby cat\n"], "garage-bAInd/Platypus2-70B": [], "lmms-lab/LLaVA-NeXT-Video-7B": [], "hfl/chinese-alpaca-2-7b": [], "bigcode/starcoderbase": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoderbase\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "input_text = \"<fim_prefix>def print_hello_world():\\n    <fim_suffix>\\n    print('Hello world!')<fim_middle>\"\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "llava-hf/bakLlava-v1-hf": ["from transformers import pipeline\n\npipe = pipeline(\"image-text-to-text\", model=\"llava-hf/bakLlava-v1-hf\")\nmessages = [\n    {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"},\n          {\"type\": \"text\", \"text\": \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"},\n        ],\n    },\n]\n\nout = pipe(text=messages, max_new_tokens=20)\nprint(out)\n>>> [{'input_text': [{'role': 'user', 'content': [{'type': 'image', 'url': 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg'}, {'type': 'text', 'text': 'What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud'}]}], 'generated_text': 'Lava'}]\n", "import requests\nfrom PIL import Image\n\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\nmodel_id = \"llava-hf/bakLlava-v1-hf\"\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True, \n).to(0)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \nconversation = [\n    {\n\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"What are these?\"},\n          {\"type\": \"image\"},\n        ],\n    },\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\n"], "Intel/dpt-large-ade": ["from transformers import DPTFeatureExtractor, DPTForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000026204.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = DPTImageProcessor .from_pretrained(\"Intel/dpt-large-ade\")\nmodel = DPTForSemanticSegmentation.from_pretrained(\"Intel/dpt-large-ade\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\nprint(logits.shape)\nlogits\nprediction = torch.nn.functional.interpolate(\n    logits,\n    size=image.size[::-1],  # Reverse the size of the original image (width, height)\n    mode=\"bicubic\",\n    align_corners=False\n)\n\n# Convert logits to class predictions\nprediction = torch.argmax(prediction, dim=1) + 1\n\n# Squeeze the prediction tensor to remove dimensions\nprediction = prediction.squeeze()\n\n# Move the prediction tensor to the CPU and convert it to a numpy array\nprediction = prediction.cpu().numpy()\n\n# Convert the prediction array to an image\npredicted_seg = Image.fromarray(prediction.squeeze().astype('uint8'))\n\n# Define the ADE20K palette\nadepallete = [0,0,0,120,120,120,180,120,120,6,230,230,80,50,50,4,200,3,120,120,80,140,140,140,204,5,255,230,230,230,4,250,7,224,5,255,235,255,7,150,5,61,120,120,70,8,255,51,255,6,82,143,255,140,204,255,4,255,51,7,204,70,3,0,102,200,61,230,250,255,6,51,11,102,255,255,7,71,255,9,224,9,7,230,220,220,220,255,9,92,112,9,255,8,255,214,7,255,224,255,184,6,10,255,71,255,41,10,7,255,255,224,255,8,102,8,255,255,61,6,255,194,7,255,122,8,0,255,20,255,8,41,255,5,153,6,51,255,235,12,255,160,150,20,0,163,255,140,140,140,250,10,15,20,255,0,31,255,0,255,31,0,255,224,0,153,255,0,0,0,255,255,71,0,0,235,255,0,173,255,31,0,255,11,200,200,255,82,0,0,255,245,0,61,255,0,255,112,0,255,133,255,0,0,255,163,0,255,102,0,194,255,0,0,143,255,51,255,0,0,82,255,0,255,41,0,255,173,10,0,255,173,255,0,0,255,153,255,92,0,255,0,255,255,0,245,255,0,102,255,173,0,255,0,20,255,184,184,0,31,255,0,255,61,0,71,255,255,0,204,0,255,194,0,255,82,0,10,255,0,112,255,51,0,255,0,194,255,0,122,255,0,255,163,255,153,0,0,255,10,255,112,0,143,255,0,82,0,255,163,255,0,255,235,0,8,184,170,133,0,255,0,255,92,184,0,255,255,0,31,0,184,255,0,214,255,255,0,112,92,255,0,0,224,255,112,224,255,70,184,160,163,0,255,153,0,255,71,255,0,255,0,163,255,204,0,255,0,143,0,255,235,133,255,0,255,0,235,245,0,255,255,0,122,255,245,0,10,190,212,214,255,0,0,204,255,20,0,255,255,255,0,0,153,255,0,41,255,0,255,204,41,0,255,41,255,0,173,0,255,0,245,255,71,0,255,122,0,255,0,255,184,0,92,255,184,255,0,0,133,255,255,214,0,25,194,194,102,255,0,92,0,255]\n\n# Apply the color map to the predicted segmentation image\npredicted_seg.putpalette(adepallete)\n\n# Blend the original image and the predicted segmentation image\nout = Image.blend(image, predicted_seg.convert(\"RGB\"), alpha=0.5)\n\nout\n"], "adept/persimmon-8b-base": [], "jozhang97/deta-resnet-50": [], "lcw99/polyglot-ko-12.8b-chang-instruct-chat": [], "hfl/chinese-alpaca-2-13b": [], "nlp-waseda/roberta-base-japanese": ["from transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n\nsentence = '\u65e9\u7a32\u7530 \u5927\u5b66 \u3067 \u81ea\u7136 \u8a00\u8a9e \u51e6\u7406 \u3092 [MASK] \u3059\u308b \u3002' # input should be segmented into words by Juman++ in advance\nencoding = tokenizer(sentence, return_tensors='pt')\n...\n"], "hiyouga/Qwen-14B-Chat-LLaMAfied": ["from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"hiyouga/Qwen-14B-Chat-LLaMAfied\")\nmodel = AutoModelForCausalLM.from_pretrained(\"hiyouga/Qwen-14B-Chat-LLaMAfied\", torch_dtype=\"auto\", device_map=\"auto\")\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"}\n]\ninputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\ninputs = inputs.to(\"cuda\")\ngenerate_ids = model.generate(inputs, streamer=streamer)\n"], "OptimalScale/robin-7b-v2-delta": [], "tau/splinter-base-qass": [], "sshleifer/distilbart-xsum-1-1": [], "allenai/led-large-16384": [], "microsoft/markuplm-base-finetuned-websrc": [], "openmmlab/upernet-swin-tiny": [], "Salesforce/codegen-2B-mono": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "MBZUAI/swiftformer-xs": ["import requests\nfrom PIL import Image\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfrom transformers import ViTImageProcessor\nprocessor = ViTImageProcessor.from_pretrained('shehan97/swiftformer-xs')\ninputs = processor(images=image, return_tensors=\"pt\")\n\n\nfrom transformers.models.swiftformer import SwiftFormerForImageClassification\nnew_model = SwiftFormerForImageClassification.from_pretrained('shehan97/swiftformer-xs')\n\noutput = new_model(inputs['pixel_values'], output_hidden_states=True)\nlogits = output.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", new_model.config.id2label[predicted_class_idx])\n"], "google/deeplabv3_mobilenet_v2_1.0_513": ["from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\npreprocessor = AutoImageProcessor.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\")\nmodel = AutoModelForSemanticSegmentation.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\")\n\ninputs = preprocessor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\npredicted_mask = preprocessor.post_process_semantic_segmentation(outputs)\n"], "flaubert/flaubert_large_cased": ["import torch\nfrom transformers import FlaubertModel, FlaubertTokenizer\n\n# Choose among ['flaubert/flaubert_small_cased', 'flaubert/flaubert_base_uncased', \n#               'flaubert/flaubert_base_cased', 'flaubert/flaubert_large_cased']\nmodelname = 'flaubert/flaubert_base_cased' \n\n# Load pretrained model and tokenizer\nflaubert, log = FlaubertModel.from_pretrained(modelname, output_loading_info=True)\nflaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=False)\n# do_lowercase=False if using cased models, True if using uncased ones\n\nsentence = \"Le chat mange une pomme.\"\ntoken_ids = torch.tensor([flaubert_tokenizer.encode(sentence)])\n\nlast_layer = flaubert(token_ids)[0]\nprint(last_layer.shape)\n# torch.Size([1, 8, 768])  -> (batch size x number of tokens x embedding dimension)\n\n# The BERT [CLS] token correspond to the first hidden state of the last layer\ncls_embedding = last_layer[:, 0, :]\n"], "laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K": [], "CompVis/ldm-text2im-large-256": ["# !pip install diffusers transformers\nfrom diffusers import DiffusionPipeline\n\nmodel_id = \"CompVis/ldm-text2im-large-256\"\n\n# load model and scheduler\nldm = DiffusionPipeline.from_pretrained(model_id)\n\n# run pipeline in inference (sample random noise and denoise)\nprompt = \"A painting of a squirrel eating a burger\"\nimages = ldm([prompt], num_inference_steps=50, eta=0.3, guidance_scale=6).images\n\n# save images\nfor idx, image in enumerate(images):\n    image.save(f\"squirrel-{idx}.png\")\n"], "hfl/chinese-llama-2-13b": [], "sijunhe/nezha-cn-base": ["from transformers import BertTokenizer, NezhaModel\ntokenizer = BertTokenizer.from_pretrained('sijunhe/nezha-cn-base')\nmodel = NezhaModel.from_pretrained(\"sijunhe/nezha-cn-base\")\ntext = \"\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n"], "nomic-ai/gpt4all-13b-snoozy": [], "TheBloke/guanaco-65B-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=True,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "mnaylor/mega-base-wikitext": [], "OpenGVLab/pvt_v2_b0": [], "Duxiaoman-DI/XuanYuan-70B": ["import torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\nmodel_name_or_path = \"Duxiaoman-DI/XuanYuan-70B\"\ntokenizer = LlamaTokenizer.from_pretrained(model_name_or_path, use_fast=False, legacy=True)\nmodel = LlamaForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16,device_map=\"auto\")\nmodel.eval()\ninputs = tokenizer(\"\u95ee\u9898\uff1a\u674e\u65f6\u73cd\u662f\u54ea\u4e00\u4e2a\u671d\u4ee3\u7684\u4eba\uff1f\u56de\u7b54\uff1a\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\noutputs = tokenizer.decode(outputs.cpu()[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\nprint(outputs)\n", "import torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\nmodel_name_or_path = \"Duxiaoman-DI/XuanYuan-70B-Chat\"\n\ntokenizer = LlamaTokenizer.from_pretrained(model_name_or_path, use_fast=False, legacy=True)\nmodel = LlamaForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\nmodel.eval()\nsystem_message = \"\u4ee5\u4e0b\u662f\u7528\u6237\u548c\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002\u7528\u6237\u4ee5Human\u5f00\u5934\uff0c\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u4ee5Assistant\u5f00\u5934\uff0c\u4f1a\u5bf9\u4eba\u7c7b\u63d0\u51fa\u7684\u95ee\u9898\u7ed9\u51fa\u6709\u5e2e\u52a9\u3001\u9ad8\u8d28\u91cf\u3001\u8be6\u7ec6\u548c\u793c\u8c8c\u7684\u56de\u7b54\uff0c\u5e76\u4e14\u603b\u662f\u62d2\u7edd\u53c2\u4e0e \u4e0e\u4e0d\u9053\u5fb7\u3001\u4e0d\u5b89\u5168\u3001\u6709\u4e89\u8bae\u3001\u653f\u6cbb\u654f\u611f\u7b49\u76f8\u5173\u7684\u8bdd\u9898\u3001\u95ee\u9898\u548c\u6307\u793a\u3002\\n\"\nseps = [\" \", \"</s>\"]\nroles = [\"Human\", \"Assistant\"]\n\ncontent = \"\u4ecb\u7ecd\u4e0b\u4f60\u81ea\u5df1\"\nprompt = system_message + seps[0] + roles[0] + \": \" + content + seps[0] + roles[1] + \":\"\nprint(f\"\u8f93\u5165: {content}\")\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.95)\noutputs = tokenizer.decode(outputs.cpu()[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\nprint(f\"\u8f93\u51fa: {outputs}\")\n"], "google/bigbird-pegasus-large-pubmed": ["from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n\n# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n\n# decoder attention type can't be changed & will be \"original_full\"\n# you can change `attention_type` (encoder only) to full attention like this:\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-pubmed\", attention_type=\"original_full\")\n\n# you can change `block_size` & `num_random_blocks` like this:\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-pubmed\", block_size=16, num_random_blocks=2)\n\ntext = \"Replace me by any text you'd like.\"\ninputs = tokenizer(text, return_tensors='pt')\nprediction = model.generate(**inputs)\nprediction = tokenizer.batch_decode(prediction)\n"], "tiiuae/falcon-180B-chat": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-180b-chat\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "Qwen/Qwen-72B-Chat-Int4": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-72B-Chat-Int4\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-72B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\nprint(response)\n# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n\n# Qwen-72B-Chat\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u7cfb\u7edf\u6307\u4ee4\uff08System Prompt\uff09\uff0c\u5b9e\u73b0\u89d2\u8272\u626e\u6f14\uff0c\u8bed\u8a00\u98ce\u683c\u8fc1\u79fb\uff0c\u4efb\u52a1\u8bbe\u5b9a\uff0c\u884c\u4e3a\u8bbe\u5b9a\u7b49\u80fd\u529b\u3002\n# Qwen-72B-Chat can realize roly playing, language style transfer, task setting, and behavior setting by system prompt.\nresponse, _ = model.chat(tokenizer, \"\u4f60\u597d\u5440\", history=None, system=\"\u8bf7\u7528\u4e8c\u6b21\u5143\u53ef\u7231\u8bed\u6c14\u548c\u6211\u8bf4\u8bdd\")\nprint(response)\n# \u54ce\u5440\uff0c\u4f60\u597d\u54c7\uff01\u662f\u600e\u4e48\u627e\u5230\u4eba\u5bb6\u7684\u5462\uff1f\u662f\u4e0d\u662f\u88ab\u4eba\u5bb6\u7684\u9b45\u529b\u5438\u5f15\u8fc7\u6765\u7684\u5440~(\u2267\u25bd\u2266)/~\n\nresponse, _ = model.chat(tokenizer, \"My colleague works diligently\", history=None, system=\"You will write beautiful compliments according to needs\")\nprint(response)\n# Your colleague is a shining example of dedication and hard work. Their commitment to their job is truly commendable, and it shows in the quality of their work. \n# They are an asset to the team, and their efforts do not go unnoticed. Keep up the great work!\n"], "microsoft/focalnet-base": ["from transformers import FocalNetImageProcessor, FocalNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\npreprocessor = FocalNetImageProcessor.from_pretrained(\"microsoft/focalnet-base\")\nmodel = FocalNetForImageClassification.from_pretrained(\"microsoft/focalnet-base\")\n\ninputs = preprocessor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "MAGAer13/mplug-owl-llama-7b": ["from mplug_owl.modeling_mplug_owl import MplugOwlForConditionalGeneration\nfrom mplug_owl.tokenization_mplug_owl import MplugOwlTokenizer\nfrom mplug_owl.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor\n\npretrained_ckpt = 'MAGAer13/mplug-owl-llama-7b'\nmodel = MplugOwlForConditionalGeneration.from_pretrained(\n    pretrained_ckpt,\n    torch_dtype=torch.bfloat16,\n)\nimage_processor = MplugOwlImageProcessor.from_pretrained(pretrained_ckpt)\ntokenizer = MplugOwlTokenizer.from_pretrained(pretrained_ckpt)\nprocessor = MplugOwlProcessor(image_processor, tokenizer)\n", "# generate kwargs (the same in transformers) can be passed in the do_generate()\ngenerate_kwargs = {\n    'do_sample': True,\n    'top_k': 5,\n    'max_length': 512\n}\nfrom PIL import Image\nimages = [Image.open(_) for _ in image_list]\ninputs = processor(text=prompts, images=images, return_tensors='pt')\ninputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\nwith torch.no_grad():\n    res = model.generate(**inputs, **generate_kwargs)\nsentence = tokenizer.decode(res.tolist()[0], skip_special_tokens=True)\nprint(sentence)\n"], "jozhang97/deta-swin-large": [], "Zetatech/pvt-tiny-224": ["from transformers import PvtImageProcessor, PvtForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = PvtImageProcessor.from_pretrained('Zetatech/pvt-tiny-224')\nmodel = PvtForImageClassification.from_pretrained('Zetatech/pvt-tiny-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "bigscience/bloomz-1b7": ["# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-1b7\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-1b7\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n", "# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-1b7\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\n\ninputs = tokenizer.encode(\"Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "CompVis/stable-diffusion-v1-2": ["import torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-2\"\ndevice = \"cuda\"\n\n\npipe = StableDiffusionPipeline.from_pretrained(model_id)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "import torch\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\nmodel_id = \"CompVis/stable-diffusion-v1-2\"\n# Use the K-LMS scheduler here instead\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, use_auth_token=True)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n"], "moussaKam/barthez": [], "Stancld/longt5-tglobal-large-16384-pubmed-3k_steps": ["LONG_ARTICLE = \"\"\"\"anxiety affects quality of life in those living\nwith parkinson 's disease ( pd ) more so than\noverall cognitive status , motor deficits , apathy\n, and depression [ 13 ] . although anxiety and\ndepression are often related and coexist in pd\npatients , recent research suggests that anxiety\nrather than depression is the most prominent and\nprevalent mood disorder in pd [ 5 , 6 ] . yet ,\nour current understanding of anxiety and its\nimpact on cognition in pd , as well as its neural\nbasis and best treatment practices , remains\nmeager and lags far behind that of depression .\noverall , neuropsychiatric symptoms in pd have\nbeen shown to be negatively associated with\ncognitive performance . for example , higher\ndepression scores have been correlated with lower\nscores on the mini - mental state exam ( mmse ) [\n8 , 9 ] as well as tests of memory and executive\nfunctions ( e.g. , attention ) [ 1014 ] . likewise\n, apathy and anhedonia in pd patients have been\nassociated with executive dysfunction [ 10 , 1523\n] . however , few studies have specifically\ninvestigated the relationship between anxiety and\ncognition in pd . one study showed a strong\nnegative relationship between anxiety ( both state\nand trait ) and overall cognitive performance (\nmeasured by the total of the repeatable battery\nfor the assessment of neuropsychological status\nindex ) within a sample of 27 pd patients .\nfurthermore , trait anxiety was negatively\nassociated with each of the cognitive domains\nassessed by the rbans ( i.e. , immediate memory ,\nvisuospatial construction , language , attention ,\nand delayed memory ) . two further studies have\nexamined whether anxiety differentially affects\ncognition in patients with left - sided dominant\npd ( lpd ) versus right - sided dominant pd ( rpd\n) ; however , their findings were inconsistent .\nthe first study found that working memory\nperformance was worse in lpd patients with anxiety\ncompared to rpd patients with anxiety , whereas\nthe second study reported that , in lpd , apathy\nbut not anxiety was associated with performance on\nnonverbally mediated executive functions and\nvisuospatial tasks ( e.g. , tmt - b , wms - iii\nspatial span ) , while in rpd , anxiety but not\napathy significantly correlated with performance\non verbally mediated tasks ( e.g. , clock reading\ntest and boston naming test ) . furthermore ,\nanxiety was significantly correlated with\nneuropsychological measures of attention and\nexecutive and visuospatial functions . taken\ntogether , it is evident that there are limited\nand inconsistent findings describing the\nrelationship between anxiety and cognition in pd\nand more specifically how anxiety might influence\nparticular domains of cognition such as attention\nand memory and executive functioning . it is also\nstriking that , to date , no study has examined\nthe influence of anxiety on cognition in pd by\ndirectly comparing groups of pd patients with and\nwithout anxiety while excluding depression . given\nthat research on healthy young adults suggests\nthat anxiety reduces processing capacity and\nimpairs processing efficiency , especially in the\ncentral executive and attentional systems of\nworking memory [ 26 , 27 ] , we hypothesized that\npd patients with anxiety would show impairments in\nattentional set - shifting and working memory\ncompared to pd patients without anxiety .\nfurthermore , since previous work , albeit limited\n, has focused on the influence of symptom\nlaterality on anxiety and cognition , we also\nexplored this relationship . seventeen pd patients\nwith anxiety and thirty - three pd patients\nwithout anxiety were included in this study ( see\ntable 1 ) . the cross - sectional data from these\nparticipants was taken from a patient database\nthat has been compiled over the past 8 years (\nsince 2008 ) at the parkinson 's disease research\nclinic at the brain and mind centre , university\nof sydney . inclusion criteria involved a\ndiagnosis of idiopathic pd according to the united\nkingdom parkinson 's disease society brain bank\ncriteria   and were confirmed by a neurologist (\nsjgl ) . patients also had to have an adequate\nproficiency in english and have completed a full\nneuropsychological assessment . ten patients in\nthis study ( 5 pd with anxiety ; 5 pd without\nanxiety ) were taking psychotropic drugs ( i.e. ,\nbenzodiazepine or selective serotonin reuptake\ninhibitor ) . patients were also excluded if they\nhad other neurological disorders , psychiatric\ndisorders other than affective disorders ( such as\nanxiety ) , or if they reported a score greater\nthan six on the depression subscale of the\nhospital anxiety and depression scale ( hads ) .\nthus , all participants who scored within a\ndepressed  ( hads - d > 6 ) range were excluded\nfrom this study , in attempt to examine a refined\nsample of pd patients with and without anxiety in\norder to determine the independent effect of\nanxiety on cognition . this research was approved\nby the human research ethics committee of the\nuniversity of sydney , and written informed\nconsent was obtained from all participants . self\n- reported hads was used to assess anxiety in pd\nand has been previously shown to be a useful\nmeasure of clinical anxiety in pd . a cut - off\nscore of > 8 on the anxiety subscale of the hads (\nhads - a ) was used to identify pd cases with\nanxiety ( pda+ ) , while a cut - off score of < 6\non the hads - a was used to identify pd cases\nwithout anxiety ( pda ) . this criterion was more\nstringent than usual ( > 7 cut - off score ) , in\neffort to create distinct patient groups . the\nneurological evaluation rated participants\naccording to hoehn and yahr ( h&y ) stages   and\nassessed their motor symptoms using part iii of\nthe revised mds task force unified parkinson 's\ndisease rating scale ( updrs ) . in a similar way\nthis was determined by calculating a total left\nand right score from rigidity items 3035 ,\nvoluntary movement items 3643 , and tremor items\n5057 from the mds - updrs part iii ( see table 1 )\n. processing speed was assessed using the trail\nmaking test , part a ( tmt - a , z - score ) .\nattentional set - shifting was measured using the\ntrail making test , part b ( tmt - b , z - score )\n. working memory was assessed using the digit span\nforward and backward subtest of the wechsler\nmemory scale - iii ( raw scores ) . language was\nassessed with semantic and phonemic verbal fluency\nvia the controlled oral word associated test (\ncowat animals and letters , z - score ) . the\nability to retain learned verbal memory was\nassessed using the logical memory subtest from the\nwechsler memory scale - iii ( lm - i z - score ,\nlm - ii z - score , % lm retention z - score ) .\nthe mini - mental state examination ( mmse )\ndemographic , clinical , and neuropsychological\nvariables were compared between the two groups\nwith the independent t - test or mann  whitney u\ntest , depending on whether the variable met\nparametric assumptions . chi - square tests were\nused to examine gender and symptom laterality\ndifferences between groups . all analyses employed\nan alpha level of p < 0.05 and were two - tailed .\nspearman correlations were performed separately in\neach group to examine associations between anxiety\nand/or depression ratings and cognitive functions\n. as expected , the pda+ group reported\nsignificant greater levels of anxiety on the hads\n- a ( u = 0 , p < 0.001 ) and higher total score\non the hads ( u = 1 , p < 0.001 ) compared to the\npda group ( table 1 ) . groups were matched in age\n( t(48 ) = 1.31 , p = 0.20 ) , disease duration (\nu = 259 , p = 0.66 ) , updrs - iii score ( u =\n250.5 , p = 0.65 ) , h&y ( u = 245 , p = 0.43 ) ,\nledd ( u = 159.5 , p = 0.80 ) , and depression (\nhads - d ) ( u = 190.5 , p = 0.06 ) . additionally\n, all groups were matched in the distribution of\ngender (  = 0.098 , p = 0.75 ) and side - affected\n(  = 0.765 , p = 0.38 ) . there were no group\ndifferences for tmt - a performance ( u = 256 , p\n= 0.62 ) ( table 2 ) ; however , the pda+ group\nhad worse performance on the trail making test\npart b ( t(46 ) = 2.03 , p = 0.048 ) compared to\nthe pda group ( figure 1 ) . the pda+ group also\ndemonstrated significantly worse performance on\nthe digit span forward subtest ( t(48 ) = 2.22 , p\n= 0.031 ) and backward subtest ( u = 190.5 , p =\n0.016 ) compared to the pda group ( figures 2(a )\nand 2(b ) ) . neither semantic verbal fluency (\nt(47 ) = 0.70 , p = 0.49 ) nor phonemic verbal\nfluency ( t(47 ) = 0.39 , p = 0.70 ) differed\nbetween groups . logical memory i immediate recall\ntest ( u = 176 , p = 0.059 ) showed a trend that\nthe pda+ group had worse new verbal learning and\nimmediate recall abilities than the pda group .\nhowever , logical memory ii test performance ( u =\n219 , p = 0.204 ) and logical memory % retention (\nu = 242.5 , p = 0.434 ) did not differ between\ngroups . there were also no differences between\ngroups in global cognition ( mmse ) ( u = 222.5 ,\np = 0.23 ) . participants were split into lpd and\nrpd , and then further group differences were\nexamined between pda+ and pda. importantly , the\ngroups remained matched in age , disease duration\n, updrs - iii , dde , h&y stage , and depression\nbut remained significantly different on self -\nreported anxiety . lpda+ demonstrated worse\nperformance on the digit span forward test ( t(19\n) = 2.29 , p = 0.033 ) compared to lpda , whereas\nrpda+ demonstrated worse performance on the digit\nspan backward test ( u = 36.5 , p = 0.006 ) , lm -\ni immediate recall ( u = 37.5 , p = 0.008 ) , and\nlm - ii ( u = 45.0 , p = 0.021 ) but not lm %\nretention ( u = 75.5 , p = 0.39 ) compared to\nrpda. this study is the first to directly compare\ncognition between pd patients with and without\nanxiety . the findings confirmed our hypothesis\nthat anxiety negatively influences attentional set\n- shifting and working memory in pd . more\nspecifically , we found that pd patients with\nanxiety were more impaired on the trail making\ntest part b which assessed attentional set -\nshifting , on both digit span tests which assessed\nworking memory and attention , and to a lesser\nextent on the logical memory test which assessed\nmemory and new verbal learning compared to pd\npatients without anxiety . taken together , these\nfindings suggest that anxiety in pd may reduce\nprocessing capacity and impair processing\nefficiency , especially in the central executive\nand attentional systems of working memory in a\nsimilar way as seen in young healthy adults [ 26 ,\n27 ] . although the neurobiology of anxiety in pd\nremains unknown , many researchers have postulated\nthat anxiety disorders are related to\nneurochemical changes that occur during the early\n, premotor stages of pd - related degeneration [\n37 , 38 ] such as nigrostriatal dopamine depletion\n, as well as cell loss within serotonergic and\nnoradrenergic brainstem nuclei ( i.e. , raphe\nnuclei and locus coeruleus , resp . , which\nprovide massive inputs to corticolimbic regions )\n. over time , chronic dysregulation of\nadrenocortical and catecholamine functions can\nlead to hippocampal damage as well as\ndysfunctional prefrontal neural circuitries [ 39 ,\n40 ] , which play a key role in memory and\nattention . recent functional neuroimaging work\nhas suggested that enhanced hippocampal activation\nduring executive functioning and working memory\ntasks may represent compensatory processes for\nimpaired frontostriatal functions in pd patients\ncompared to controls . therefore , chronic stress\nfrom anxiety , for example , may disrupt\ncompensatory processes in pd patients and explain\nthe cognitive impairments specifically in working\nmemory and attention seen in pd patients with\nanxiety . it has also been suggested that\nhyperactivation within the putamen may reflect a\ncompensatory striatal mechanism to maintain normal\nworking memory performance in pd patients ;\nhowever , losing this compensatory activation has\nbeen shown to contribute to poor working memory\nperformance . anxiety in mild pd has been linked\nto reduced putamen dopamine uptake which becomes\nmore extensive as the disease progresses . this\nfurther supports the notion that anxiety may\ndisrupt compensatory striatal mechanisms as well ,\nproviding another possible explanation for the\ncognitive impairments observed in pd patients with\nanxiety in this study . noradrenergic and\nserotonergic systems should also be considered\nwhen trying to explain the mechanisms by which\nanxiety may influence cognition in pd . although\nthese neurotransmitter systems are relatively\nunderstudied in pd cognition , treating the\nnoradrenergic and serotonergic systems has shown\nbeneficial effects on cognition in pd . selective\nserotonin reuptake inhibitor , citalopram , was\nshown to improve response inhibition deficits in\npd , while noradrenaline reuptake blocker ,\natomoxetine , has been recently reported to have\npromising effects on cognition in pd [ 45 , 46 ] .\noverall , very few neuroimaging studies have been\nconducted in pd in order to understand the neural\ncorrelates of pd anxiety and its underlying neural\npathology . future research should focus on\nrelating anatomical changes and neurochemical\nchanges to neural activation in order to gain a\nclearer understanding on how these pathologies\naffect anxiety in pd . to further understand how\nanxiety and cognitive dysfunction are related ,\nfuture research should focus on using advanced\nstructural and function imaging techniques to\nexplain both cognitive and neural breakdowns that\nare associated with anxiety in pd patients .\nresearch has indicated that those with amnestic\nmild cognitive impairment who have more\nneuropsychiatric symptoms have a greater risk of\ndeveloping dementia compared to those with fewer\nneuropsychiatric symptoms . future studies should\nalso examine whether treating neuropsychiatric\nsymptoms might impact the progression of cognitive\ndecline and improve cognitive impairments in pd\npatients . previous studies have used pd symptom\nlaterality as a window to infer asymmetrical\ndysfunction of neural circuits . for example , lpd\npatients have greater inferred right hemisphere\npathology , whereas rpd patients have greater\ninferred left hemisphere pathology . thus ,\ncognitive domains predominantly subserved by the\nleft hemisphere ( e.g. , verbally mediated tasks\nof executive function and verbal memory ) might be\nhypothesized to be more affected in rpd than lpd ;\nhowever , this remains controversial . it has also\nbeen suggested that since anxiety is a common\nfeature of left hemisphere involvement [ 48 , 49 ]\n, cognitive domains subserved by the left\nhemisphere may also be more strongly related to\nanxiety . results from this study showed selective\nverbal memory deficits in rpd patients with\nanxiety compared to rpd without anxiety , whereas\nlpd patients with anxiety had greater attentional\n/ working memory deficits compared to lpd without\nanxiety . although these results align with\nprevious research , interpretations of these\nfindings should be made with caution due to the\nsmall sample size in the lpd comparison\nspecifically . recent work has suggested that the\nhads questionnaire may underestimate the burden of\nanxiety related symptomology and therefore be a\nless sensitive measure of anxiety in pd [ 30 , 50\n] . in addition , our small sample size also\nlimited the statistical power for detecting\nsignificant findings . based on these limitations\n, our findings are likely conservative and\nunderrepresent the true impact anxiety has on\ncognition in pd . additionally , the current study\nemployed a very brief neuropsychological\nassessment including one or two tests for each\ncognitive domain . future studies are encouraged\nto collect a more complex and comprehensive\nbattery from a larger sample of pd participants in\norder to better understand the role anxiety plays\non cognition in pd . another limitation of this\nstudy was the absence of diagnostic interviews to\ncharacterize participants ' psychiatric symptoms\nand specify the type of anxiety disorders included\nin this study . future studies should perform\ndiagnostic interviews with participants ( e.g. ,\nusing dsm - v criteria ) rather than relying on\nself - reported measures to group participants ,\nin order to better understand whether the type of\nanxiety disorder ( e.g. , social anxiety , phobias\n, panic disorders , and generalized anxiety )\ninfluences cognitive performance differently in pd\n. one advantage the hads questionnaire provided\nover other anxiety scales was that it assessed\nboth anxiety and depression simultaneously and\nallowed us to control for coexisting depression .\nalthough there was a trend that the pda+ group\nself - reported higher levels of depression than\nthe pda group , all participants included in the\nstudy scored < 6 on the depression subscale of the\nhads . controlling for depression while assessing\nanxiety has been identified as a key shortcoming\nin the majority of recent work . considering many\nprevious studies have investigated the influence\nof depression on cognition in pd without\naccounting for the presence of anxiety and the\ninconsistent findings reported to date , we\nrecommend that future research should try to\ndisentangle the influence of anxiety versus\ndepression on cognitive impairments in pd .\nconsidering the growing number of clinical trials\nfor treating depression , there are few if any for\nthe treatment of anxiety in pd . anxiety is a key\ncontributor to decreased quality of life in pd and\ngreatly requires better treatment options .\nmoreover , anxiety has been suggested to play a\nkey role in freezing of gait ( fog ) , which is\nalso related to attentional set - shifting [ 52 ,\n53 ] . future research should examine the link\nbetween anxiety , set - shifting , and fog , in\norder to determine whether treating anxiety might\nbe a potential therapy for improving fog .\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n\ninput_ids = tokenizer(LONG_ARTICLE, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\nmodel = LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\", return_dict_in_generate=True).to(\"cuda\")\n\nsequences = model.generate(input_ids).sequences\n\nsummary = tokenizer.batch_decode(sequences)\n"], "TheBloke/Mistral-7B-v0.1-AWQ": ["from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_name_or_path = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n                                          trust_remote_code=False, safetensors=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ntokens = tokenizer(\n    prompt_template,\n    return_tensors='pt'\n).input_ids.cuda()\n\n# Generate output\ngeneration_output = model.generate(\n    tokens,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    max_new_tokens=512\n)\n\nprint(\"Output: \", tokenizer.decode(generation_output[0]))\n\n\"\"\"\n# Inference should be possible with transformers pipeline as well in future\n# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\nfrom transformers import pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\"\"\"\n"], "SenseTime/deformable-detr-with-box-refine": ["from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr-with-box-refine\")\nmodel = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr-with-box-refine\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.7\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n"], "openbmb/MiniCPM-2B-dpo-bf16": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\n\npath = 'openbmb/MiniCPM-2B-dpo-bf16'\ntokenizer = AutoTokenizer.from_pretrained(path)\nmodel = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)\n\nresponds, history = model.chat(tokenizer, \"\u5c71\u4e1c\u7701\u6700\u9ad8\u7684\u5c71\u662f\u54ea\u5ea7\u5c71, \u5b83\u6bd4\u9ec4\u5c71\u9ad8\u8fd8\u662f\u77ee\uff1f\u5dee\u8ddd\u591a\u5c11\uff1f\", temperature=0.8, top_p=0.8)\nprint(responds)\n"], "Qwen/Qwen1.5-7B-Chat-AWQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-7B-Chat-AWQ\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat-AWQ\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "Salesforce/codegen-2B-multi": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-multi\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "google/switch-base-16": ["\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-16\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-16\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-16\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-16\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-16\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-16\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-16\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-16\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n"], "AdamCodd/tinybert-sentiment-amazon": ["from transformers import pipeline\n\n# Create the pipeline\nsentiment_classifier = pipeline('text-classification', model='AdamCodd/tinybert-sentiment-amazon')\n\n# Now you can use the pipeline to classify emotions\nresult = sentiment_classifier(\"This product doesn't fit me at all.\")\nprint(result)\n#[{'label': 'negative', 'score': 0.9969743490219116}]\n"], "01-ai/Yi-6B-Chat-4bits": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "aisingapore/sea-lion-7b": [], "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft": [], "Qwen/Qwen1.5-4B-Chat-AWQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-4B-Chat-AWQ\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-4B-Chat-AWQ\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "Alireza1044/mobilebert_sst2": [], "stas/tiny-wmt19-en-ru": [], "xlm-roberta-large-finetuned-conll02-dutch": [">>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll02-dutch\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll02-dutch\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Mijn naam is Emma en ik woon in Londen.\")\n\n\n[{'end': 17,\n  'entity': 'B-PER',\n  'index': 4,\n  'score': 0.9999807,\n  'start': 13,\n  'word': '\u2581Emma'},\n {'end': 36,\n  'entity': 'B-LOC',\n  'index': 9,\n  'score': 0.9999871,\n  'start': 32,\n  'word': '\u2581Lond'}]\n"], "Qwen/Qwen1.5-32B-Chat-AWQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-32B-Chat-AWQ\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-32B-Chat-AWQ\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "sshleifer/distilbart-xsum-12-3": [], "cerebras/btlm-3b-8k-base": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"cerebras/btlm-3b-8k-base\")\nmodel = AutoModelForCausalLM.from_pretrained(\"cerebras/btlm-3b-8k-base\", trust_remote_code=True, torch_dtype=\"auto\")\n\n# Set the prompt for generating text\nprompt = \"Albert Einstein was known for \"\n\n# Tokenize the prompt and convert to PyTorch tensors\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Generate text using the model\noutputs = model.generate(\n    **inputs,\n    num_beams=5,\n    max_new_tokens=50,\n    early_stopping=True,\n    no_repeat_ngram_size=2\n)\n\n# Convert the generated token IDs back to text\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n# Print the generated text\nprint(generated_text[0])\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import pipeline\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"cerebras/btlm-3b-8k-base\")\nmodel = AutoModelForCausalLM.from_pretrained(\"cerebras/btlm-3b-8k-base\", trust_remote_code=True, torch_dtype=\"auto\")\n\n# Set the prompt for text generation\nprompt = \"\"\"Isaac Newton was a \"\"\"\n\n# Create a text generation pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Generate text using the pipeline\ngenerated_text = pipe(\n    prompt, \n    max_length=50, \n    do_sample=False, \n    no_repeat_ngram_size=2)[0]\n\n# Print the generated text\nprint(generated_text['generated_text'])\n", "from transformers import AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"cerebras/btlm-3b-8k-base\", trust_remote_code=True)\n\n# Get the parameter groups for the muP optimizer\nparam_groups = model.get_mup_param_groups(lr=1e-3, weight_decay=0.1)\n\n# Set up the optimizer using AdamW with muP parameters\noptimizer = torch.optim.AdamW(\n    param_groups,\n    betas=(0.9, 0.95),\n    eps=1e-8\n)\n", "Prompt: \"Where does the blood get pumped to after it leaves the right ventricle?\"\nGeneration: \u201cThe right atrium receives the deoxygenated blood from the pulmonary artery. The right side of the heart is the only side that receives de-oxygened...\u201d\n"], "Austism/chronos-hermes-13b": [], "shi-labs/versatile-diffusion": ["#! pip install git+https://github.com/huggingface/transformers diffusers torch\nfrom diffusers import VersatileDiffusionPipeline\nimport torch\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\n\npipe = VersatileDiffusionPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\n# prompt\nprompt = \"a red car\"\n\n# initial image\nurl = \"https://huggingface.co/datasets/diffusers/images/resolve/main/benz.jpg\"\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# text to image\nimage = pipe.text_to_image(prompt).images[0]\n\n# image variation\nimage = pipe.image_variation(image).images[0]\n\n# image variation\nimage = pipe.dual_guided(prompt, image).images[0]\n", "from diffusers import VersatileDiffusionTextToImagePipeline\nimport torch\n\npipe = VersatileDiffusionTextToImagePipeline.from_pretrained(\"shi-labs/versatile-diffusion\", torch_dtype=torch.float16)\npipe.remove_unused_weights()\npipe = pipe.to(\"cuda\")\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\nimage = pipe(\"an astronaut riding on a horse on mars\", generator=generator).images[0]\nimage.save(\"./astronaut.png\")\n", "from diffusers import VersatileDiffusionImageVariationPipeline\nimport torch\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\n\n# download an initial image\nurl = \"https://huggingface.co/datasets/diffusers/images/resolve/main/benz.jpg\"\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\npipe = VersatileDiffusionImageVariationPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\nimage = pipe(image, generator=generator).images[0]\nimage.save(\"./car_variation.png\")\n", "from diffusers import VersatileDiffusionDualGuidedPipeline\nimport torch\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\n\n# download an initial image\nurl = \"https://huggingface.co/datasets/diffusers/images/resolve/main/benz.jpg\"\n\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\ntext = \"a red car in the sun\"\n\npipe = VersatileDiffusionDualGuidedPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", torch_dtype=torch.float16)\npipe.remove_unused_weights()\npipe = pipe.to(\"cuda\")\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\ntext_to_image_strength = 0.75\n\nimage = pipe(prompt=text, image=image, text_to_image_strength=text_to_image_strength, generator=generator).images[0]\nimage.save(\"./red_car.png\")\n"], "TheBloke/WizardLM-13B-V1.2-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/WizardLM-13B-V1.2-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "DeepFloyd/IF-I-L-v1.0": ["from huggingface_hub import login\n\nlogin()\n", "from diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nimport torch\n\n# stage 1\nstage_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-L-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_1.enable_model_cpu_offload()\n\n# stage 2\nstage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_2.enable_model_cpu_offload()\n\n# stage 3\nsafety_modules = {\"feature_extractor\": stage_1.feature_extractor, \"safety_checker\": stage_1.safety_checker, \"watermarker\": stage_1.watermarker}\nstage_3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16)\nstage_3.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_3.enable_model_cpu_offload()\n"], "microsoft/swin-small-patch4-window7-224": ["from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-small-patch4-window7-224\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-small-patch4-window7-224\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "OrionStarAI/Orion-14B-Base": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"OrionStarAI/Orion-14B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"OrionStarAI/Orion-14B\", device_map=\"auto\",\n                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n\nmodel.generation_config = GenerationConfig.from_pretrained(\"OrionStarAI/Orion-14B\")\nmessages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\nresponse = model.chat(tokenizer, messages, streaming=False)\nprint(response)\n", "User: Hello\nOrion-14B: Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\nUser: What functions do you have?\nOrion-14B: As an AI language model, I have a wide range of functions and capabilities, including but not limited to:\n  Answering general knowledge questions: I can provide information on various topics such as history, science, geography, and more.\n  Providing definitions and explanations: I can explain complex concepts and provide definitions for words and phrases.\n  Assisting with tasks: I can help with simple tasks like setting reminders, converting units of measurement, and even ordering food from restaurants.\n  Generating text: I can write short stories, essays, articles, and other types of content based on prompts given to me.\n  Translating languages: I can translate text from one language to another.\n  Chatting: I can engage in conversation with users and respond to their queries in a conversational manner.\n  Summarizing long texts: I can summarize longer texts like articles, books, and research papers.\n  Generating random numbers and names: I can generate random numbers within a specified range and also suggest names based on certain criteria.\n  Playing games: I can play simple games like Hangman, Trivia, and more.\n  Providing recommendations: I can recommend movies, music, books, and other forms of entertainment based on user preferences.\n  These are just some of the many functions that I am capable of performing. If you have a specific task in mind, feel free to ask me about it.\nUser: Tell me a joke.\nOrion-14B: Sure, here's a classic one-liner: Why don't scientists trust atoms? Because they make up everything.\n"], "microsoft/deberta-xlarge": [], "TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "apple/deeplabv3-mobilevit-xx-small": ["from transformers import MobileViTFeatureExtractor, MobileViTForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/deeplabv3-mobilevit-xx-small\")\nmodel = MobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-xx-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_mask = logits.argmax(1).squeeze(0)\n"], "CompVis/stable-diffusion-v1-3": ["import torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-3\"\ndevice = \"cuda\"\n\n\npipe = StableDiffusionPipeline.from_pretrained(model_id)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "import torch\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n", "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\nmodel_id = \"CompVis/stable-diffusion-v1-3\"\n# Use the K-LMS scheduler here instead\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, use_auth_token=True)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n"], "microsoft/beit-large-patch16-224": ["from transformers import BeitFeatureExtractor, BeitForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-large-patch16-224')\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-large-patch16-224')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "jpwahle/longformer-base-plagiarism-detection": ["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nAutoModelForSequenceClassification(\"jpelhaw/longformer-base-plagiarism-detection\")\nAutoTokenizer.from_pretrained(\"jpelhaw/longformer-base-plagiarism-detection\")\n\ninput = \"Plagiarism is the representation of another author's writing, \\\nthoughts, ideas, or expressions as one's own work.\"\n\n\nexample = tokenizer.tokenize(input, add_special_tokens=True)\n\nanswer = model(**example)\n                                \n# \"plagiarised\"\n"], "microsoft/beit-large-patch16-224-pt22k": ["from transformers import BeitFeatureExtractor, BeitForMaskedImageModeling\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-large-patch16-224-pt22k')\nmodel = BeitForMaskedImageModeling.from_pretrained('microsoft/beit-large-patch16-224-pt22k')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n"], "valhalla/bart-large-finetuned-squadv1": ["from transformers import BartTokenizer, BartForQuestionAnswering\nimport torch\n\ntokenizer = BartTokenizer.from_pretrained('valhalla/bart-large-finetuned-squadv1')\nmodel = BartForQuestionAnswering.from_pretrained('valhalla/bart-large-finetuned-squadv1')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\nencoding = tokenizer(question, text, return_tensors='pt')\ninput_ids = encoding['input_ids']\nattention_mask = encoding['attention_mask']\n\nstart_scores, end_scores = model(input_ids, attention_mask=attention_mask, output_attentions=False)[:2]\n\nall_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\nanswer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\nanswer = tokenizer.convert_tokens_to_ids(answer.split())\nanswer = tokenizer.decode(answer)\n#answer => 'a nice puppet' \n"], "OrionStarAI/Orion-14B-Chat": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"OrionStarAI/Orion-14B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"OrionStarAI/Orion-14B\", device_map=\"auto\",\n                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n\nmodel.generation_config = GenerationConfig.from_pretrained(\"OrionStarAI/Orion-14B\")\nmessages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\nresponse = model.chat(tokenizer, messages, streaming=False)\nprint(response)\n", "User: Hello\nOrion-14B: Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\nUser: What functions do you have?\nOrion-14B: As an AI language model, I have a wide range of functions and capabilities, including but not limited to:\n  Answering general knowledge questions: I can provide information on various topics such as history, science, geography, and more.\n  Providing definitions and explanations: I can explain complex concepts and provide definitions for words and phrases.\n  Assisting with tasks: I can help with simple tasks like setting reminders, converting units of measurement, and even ordering food from restaurants.\n  Generating text: I can write short stories, essays, articles, and other types of content based on prompts given to me.\n  Translating languages: I can translate text from one language to another.\n  Chatting: I can engage in conversation with users and respond to their queries in a conversational manner.\n  Summarizing long texts: I can summarize longer texts like articles, books, and research papers.\n  Generating random numbers and names: I can generate random numbers within a specified range and also suggest names based on certain criteria.\n  Playing games: I can play simple games like Hangman, Trivia, and more.\n  Providing recommendations: I can recommend movies, music, books, and other forms of entertainment based on user preferences.\n  These are just some of the many functions that I am capable of performing. If you have a specific task in mind, feel free to ask me about it.\nUser: Tell me a joke.\nOrion-14B: Sure, here's a classic one-liner: Why don't scientists trust atoms? Because they make up everything.\n"], "flax-community/t5-base-cnn-dm": [], "Qwen/CodeQwen1.5-7B-Chat-AWQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/CodeQwen1.5-7B-Chat-AWQ\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/CodeQwen1.5-7B-Chat-AWQ\")\n\nprompt = \"Write a quicksort algorithm in python.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "xlm-mlm-17-1280": [], "HuggingFaceM4/idefics-80b": ["import torch\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncheckpoint = \"HuggingFaceM4/idefics-9b\"\nmodel = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n# We feed to the model an arbitrary sequence of text strings and images. Images can be either URLs or PIL Images.\nprompts = [\n    [\n        \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n        \"In this picture from Asterix and Obelix, we can see\"\n    ],\n]\n\n# --batched mode\ninputs = processor(prompts, return_tensors=\"pt\").to(device)\n# --single sample mode\n# inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n\n# Generation args\nbad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\ngenerated_ids = model.generate(**inputs, bad_words_ids=bad_words_ids, max_length=100)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nfor i, t in enumerate(generated_text):\n    print(f\"{i}:\\n{t}\\n\")\n", "import torch\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncheckpoint = \"HuggingFaceM4/idefics-9b-instruct\"\nmodel = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n# We feed to the model an arbitrary sequence of text strings and images. Images can be either URLs or PIL Images.\nprompts = [\n    [\n        \"User: What is in this image?\",\n        \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n        \"<end_of_utterance>\",\n\n        \"\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>\",\n\n        \"\\nUser:\",\n        \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052\",\n        \"And who is that?<end_of_utterance>\",\n\n        \"\\nAssistant:\",\n    ],\n]\n\n# --batched mode\ninputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(device)\n# --single sample mode\n# inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n\n# Generation args\nexit_condition = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids\nbad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\ngenerated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nfor i, t in enumerate(generated_text):\n    print(f\"{i}:\\n{t}\\n\")\n", "from text_generation import Client\n\nAPI_TOKEN = \"<YOUR_API_TOKEN>\"\nAPI_URL = \"https://api-inference.huggingface.co/models/HuggingFaceM4/idefics-80b-instruct\"\nDECODING_STRATEGY = \"Greedy\"\nQUERY = \"User: What is in this image?![](https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG)<end_of_utterance>\\nAssistant:\"\n\nclient = Client(\n    base_url=API_URL,\n    headers={\"x-use-cache\": \"0\", \"Authorization\": f\"Bearer {API_TOKEN}\"},\n)\ngeneration_args = {\n    \"max_new_tokens\": 256,\n    \"repetition_penalty\": 1.0,\n    \"stop_sequences\": [\"<end_of_utterance>\", \"\\nUser:\"],\n}\n\nif DECODING_STRATEGY == \"Greedy\":\n    generation_args[\"do_sample\"] = False\nelif DECODING_STRATEGY == \"Top P Sampling\":\n    generation_args[\"temperature\"] = 1.\n    generation_args[\"do_sample\"] = True\n    generation_args[\"top_p\"] = 0.95\n    \ngenerated_text = client.generate(prompt=QUERY, **generation_args)  \nprint(generated_text)\n"], "lmsys/longchat-7b-16k": [], "kakaobrain/karlo-v1-alpha-image-variations": ["from diffusers import UnCLIPPipeline\nimport torch\n\npipe = UnCLIPPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha\", torch_dtype=torch.float16)\npipe = pipe.to('cuda')\n\nprompt = \"a high-resolution photograph of a big red frog on a green leaf.\"\n\nimage = pipe([prompt]).images[0]\n\nimage.save(\"./frog.png\")\n", "from diffusers import UnCLIPImageVariationPipeline\nimport torch\nfrom PIL import Image\n\npipe = UnCLIPImageVariationPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha-image-variations\", torch_dtype=torch.float16)\npipe = pipe.to('cuda')\n\nimage = Image.open(\"./frog.png\")\n\nimage = pipe(image).images[0]\n\nimage.save(\"./frog-variation.png\")\n"], "defog/sqlcoder": [], "MediaTek-Research/Breeze-7B-Base-v1_0": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Instruction Model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"MediaTek-Research/Breeze-7B-Instruct-v1_0\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    # attn_implementation=\"flash_attention_2\" # optional\n)\n\n# Basemodel\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"MediaTek-Research/Breeze-7B-Base-v1_0\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    # attn_implementation=\"flash_attention_2\" # optional\n)\n", "You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.\n", ">>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v1_0\")\n>>> chat = [\n...   {\"role\": \"user\", \"content\": \"\u4f60\u597d\uff0c\u8acb\u554f\u4f60\u53ef\u4ee5\u5b8c\u6210\u4ec0\u9ebc\u4efb\u52d9\uff1f\"},\n...   {\"role\": \"assistant\", \"content\": \"\u4f60\u597d\uff0c\u6211\u53ef\u4ee5\u5e6b\u52a9\u60a8\u89e3\u6c7a\u5404\u7a2e\u554f\u984c\u3001\u63d0\u4f9b\u8cc7\u8a0a\u548c\u5354\u52a9\u60a8\u5b8c\u6210\u8a31\u591a\u4e0d\u540c\u7684\u4efb\u52d9\u3002\u4f8b\u5982\uff1a\u56de\u7b54\u6280\u8853\u554f\u984c\u3001\u63d0\u4f9b\u5efa\u8b70\u3001\u7ffb\u8b6f\u6587\u5b57\u3001\u5c0b\u627e\u8cc7\u6599\u6216\u5354\u52a9\u60a8\u5b89\u6392\u884c\u7a0b\u7b49\u3002\u8acb\u544a\u8a34\u6211\u5982\u4f55\u80fd\u5e6b\u52a9\u60a8\u3002\"},\n...   {\"role\": \"user\", \"content\": \"\u592a\u68d2\u4e86\uff01\"},\n... ]\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\n\"<s>You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.  [INST] \u4f60\u597d\uff0c\u8acb\u554f\u4f60\u53ef\u4ee5\u5b8c\u6210\u4ec0\u9ebc\u4efb\u52d9\uff1f [/INST] \u4f60\u597d\uff0c\u6211\u53ef\u4ee5\u5e6b\u52a9\u60a8\u89e3\u6c7a\u5404\u7a2e\u554f\u984c\u3001\u63d0\u4f9b\u8cc7\u8a0a\u548c\u5354\u52a9\u60a8\u5b8c\u6210\u8a31\u591a\u4e0d\u540c\u7684\u4efb\u52d9\u3002\u4f8b\u5982\uff1a\u56de\u7b54\u6280\u8853\u554f\u984c\u3001\u63d0\u4f9b\u5efa\u8b70\u3001\u7ffb\u8b6f\u6587\u5b57\u3001\u5c0b\u627e\u8cc7\u6599\u6216\u5354\u52a9\u60a8\u5b89\u6392\u884c\u7a0b\u7b49\u3002\u8acb\u544a\u8a34\u6211\u5982\u4f55\u80fd\u5e6b\u52a9\u60a8\u3002 [INST] \u592a\u68d2\u4e86\uff01 [/INST] \"\n# Tokenized results\n# ['\u2581', '\u4f60\u597d', '\uff0c', '\u8acb\u554f', '\u4f60', '\u53ef\u4ee5', '\u5b8c\u6210', '\u4ec0\u9ebc', '\u4efb\u52d9', '\uff1f']\n# ['\u2581', '\u4f60\u597d', '\uff0c', '\u6211', '\u53ef\u4ee5', '\u5e6b\u52a9', '\u60a8', '\u89e3\u6c7a', '\u5404\u7a2e', '\u554f\u984c', '\u3001', '\u63d0\u4f9b', '\u8cc7\u8a0a', '\u548c', '\u5354\u52a9', '\u60a8', '\u5b8c\u6210', '\u8a31\u591a', '\u4e0d\u540c', '\u7684', '\u4efb\u52d9', '\u3002', '\u4f8b\u5982', '\uff1a', '\u56de\u7b54', '\u6280\u8853', '\u554f\u984c', '\u3001', '\u63d0\u4f9b', '\u5efa\u8b70', '\u3001', '\u7ffb\u8b6f', '\u6587\u5b57', '\u3001', '\u5c0b\u627e', '\u8cc7\u6599', '\u6216', '\u5354\u52a9', '\u60a8', '\u5b89\u6392', '\u884c\u7a0b', '\u7b49', '\u3002', '\u8acb', '\u544a\u8a34', '\u6211', '\u5982\u4f55', '\u80fd', '\u5e6b\u52a9', '\u60a8', '\u3002']\n# ['\u2581', '\u592a', '\u68d2', '\u4e86', '\uff01']\n\n>>> outputs = model.generate(tokenizer.apply_chat_template(chat, return_tensors=\"pt\"), max_new_tokens=128) \n>>> print(tokenizer.decode(outputs[0]))\n"], "OrionStarAI/Orion-14B-Chat-RAG": ["import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"OrionStarAI/Orion-14B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"OrionStarAI/Orion-14B\", device_map=\"auto\",\n                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n\nmodel.generation_config = GenerationConfig.from_pretrained(\"OrionStarAI/Orion-14B\")\nmessages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\nresponse = model.chat(tokenizer, messages, streaming=False)\nprint(response)\n", "User: Hello\nOrion-14B: Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\nUser: What functions do you have?\nOrion-14B: As an AI language model, I have a wide range of functions and capabilities, including but not limited to:\n  Answering general knowledge questions: I can provide information on various topics such as history, science, geography, and more.\n  Providing definitions and explanations: I can explain complex concepts and provide definitions for words and phrases.\n  Assisting with tasks: I can help with simple tasks like setting reminders, converting units of measurement, and even ordering food from restaurants.\n  Generating text: I can write short stories, essays, articles, and other types of content based on prompts given to me.\n  Translating languages: I can translate text from one language to another.\n  Chatting: I can engage in conversation with users and respond to their queries in a conversational manner.\n  Summarizing long texts: I can summarize longer texts like articles, books, and research papers.\n  Generating random numbers and names: I can generate random numbers within a specified range and also suggest names based on certain criteria.\n  Playing games: I can play simple games like Hangman, Trivia, and more.\n  Providing recommendations: I can recommend movies, music, books, and other forms of entertainment based on user preferences.\n  These are just some of the many functions that I am capable of performing. If you have a specific task in mind, feel free to ask me about it.\nUser: Tell me a joke.\nOrion-14B: Sure, here's a classic one-liner: Why don't scientists trust atoms? Because they make up everything.\n"], "xlm-mlm-enfr-1024": [], "google/tapas-base-finetuned-tabfact": [], "julien-c/EsperBERTo-small": ["from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"julien-c/EsperBERTo-small\",\n    tokenizer=\"julien-c/EsperBERTo-small\"\n)\n\nfill_mask(\"Jen la komenco de bela <mask>.\")\n\n# This is the beginning of a beautiful <mask>.\n# =>\n\n# {\n#     'score':0.06502299010753632\n#     'sequence':'<s> Jen la komenco de bela vivo.</s>'\n#     'token':1099\n# }\n# {\n#     'score':0.0421181358397007\n#     'sequence':'<s> Jen la komenco de bela vespero.</s>'\n#     'token':5100\n# }\n# {\n#     'score':0.024884626269340515\n#     'sequence':'<s> Jen la komenco de bela laboro.</s>'\n#     'token':1570\n# }\n# {\n#     'score':0.02324388362467289\n#     'sequence':'<s> Jen la komenco de bela tago.</s>'\n#     'token':1688\n# }\n# {\n#     'score':0.020378097891807556\n#     'sequence':'<s> Jen la komenco de bela festo.</s>'\n#     'token':4580\n# }\n"], "google/bigbird-base-trivia-itc": ["from transformers import BigBirdForQuestionAnswering\n\n# by default its in `block_sparse` mode with num_random_blocks=3, block_size=64\nmodel = BigBirdForQuestionAnswering.from_pretrained(\"google/bigbird-base-trivia-itc\")\n\n# you can change `attention_type` to full attention like this:\nmodel = BigBirdForQuestionAnswering.from_pretrained(\"google/bigbird-base-trivia-itc\", attention_type=\"original_full\")\n\n# you can change `block_size` & `num_random_blocks` like this:\nmodel = BigBirdForQuestionAnswering.from_pretrained(\"google/bigbird-base-trivia-itc\", block_size=16, num_random_blocks=2)\n\nquestion = \"Replace me by any text you'd like.\"\ncontext = \"Put some context for answering\"\nencoded_input = tokenizer(question, context, return_tensors='pt')\noutput = model(**encoded_input)\n"], "OpenAssistant/reward-model-deberta-v3-large": ["from transformers import AutoModelForSequenceClassification, AutoTokenizer\nreward_name = \"OpenAssistant/reward-model-deberta-v3-large\"\nrank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\nquestion, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\ninputs = tokenizer(question, answer, return_tensors='pt')\nscore = rank_model(**inputs).logits[0].cpu().detach()\nprint(score)\n"], "xlm-clm-enfr-1024": ["import torch\nfrom transformers import XLMTokenizer, XLMWithLMHeadModel\n\ntokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\")\nmodel = XLMWithLMHeadModel.from_pretrained(\"xlm-clm-enfr-1024\")\n\ninput_ids = torch.tensor([tokenizer.encode(\"Wikipedia was used to\")])  # batch size of 1\n\nlanguage_id = tokenizer.lang2id[\"en\"]  # 0\nlangs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])\n\n# We reshape it to be of size (batch_size, sequence_length)\nlangs = langs.view(1, -1)  # is now of shape [1, sequence_length] (we have a batch size of 1)\n\noutputs = model(input_ids, langs=langs)\n"], "facebook/levit-128S": ["from transformers import LevitFeatureExtractor, LevitForImageClassificationWithTeacher\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = LevitFeatureExtractor.from_pretrained('facebook/levit-128S')\nmodel = LevitForImageClassificationWithTeacher.from_pretrained('facebook/levit-128S')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "laion/CLIP-ViT-B-32-CommonPool.S.image-s13M-b4K": [], "microsoft/swin-large-patch4-window12-384": ["from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-large-patch4-window12-384\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-large-patch4-window12-3844\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "tatsu-lab/alpaca-7b-wdiff": ["import transformers\nalpaca_model = transformers.AutoModelForCausalLM.from_pretrained(\"<path_to_store_recovered_weights>\")\nalpaca_tokenizer = transformers.AutoTokenizer.from_pretrained(\"<path_to_store_recovered_weights>\")\n"], "sail/poolformer_s12": ["from transformers import PoolFormerFeatureExtractor, PoolFormerForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = PoolFormerFeatureExtractor.from_pretrained('sail/poolformer_s12')\nmodel = PoolFormerForImageClassification.from_pretrained('sail/poolformer_s12')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "lmms-lab/LLaVA-NeXT-Video-34B": [], "Qwen/Qwen-14B-Chat-Int4": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-14B-Chat-Int4\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-14B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\nprint(response)\n# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n"], "Qwen/Qwen1.5-72B-Chat-AWQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-72B-Chat-AWQ\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-72B-Chat-AWQ\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "bigscience/T0": ["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\")\n\ninputs = tokenizer.encode(\"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"], "stas/tiny-m2m_100": [], "xverse/XVERSE-13B": [">>> import torch\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xverse/XVERSE-13B\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"xverse/XVERSE-13B\", trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')\n>>> model = model.eval()\n>>> inputs = tokenizer('\u5317\u4eac\u7684\u666f\u70b9\uff1a\u6545\u5bab\u3001\u5929\u575b\u3001\u4e07\u91cc\u957f\u57ce\u7b49\u3002\\n\u6df1\u5733\u7684\u666f\u70b9\uff1a', return_tensors='pt').input_ids\n>>> inputs = inputs.cuda()\n>>> generated_ids = model.generate(inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)\n>>> print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n"], "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg": [], "Skywork/Skywork-13B-base": ["\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> from transformers.generation import GenerationConfig\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"SkyworkAI/Skywork-13B-Base\", trust_remote_code=True)\n>>> model = AutoModelForCausalLM.from_pretrained(\"SkyworkAI/Skywork-13B-Base\", device_map=\"auto\", trust_remote_code=True).eval()\n\n>>> inputs = tokenizer('\u9655\u897f\u7684\u7701\u4f1a\u662f\u897f\u5b89', return_tensors='pt').to(model.device)\n>>> response = model.generate(inputs.input_ids, max_length=128)\n>>> print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))\n\u9655\u897f\u7684\u7701\u4f1a\u662f\u897f\u5b89\uff0c\u897f\u5b89\u662f\u6211\u56fd\u8457\u540d\u7684\u53e4\u90fd\uff0c\u5728\u5386\u53f2\u4e0a\u6709\u5341\u4e09\u4e2a\u671d\u4ee3\u5728\u6b64\u5efa\u90fd\uff0c\u6240\u4ee5\u897f\u5b89\u53c8\u88ab\u79f0\u4e3a\u201c\u5341\u4e09\u671d\u53e4\u90fd\u201d\u3002\u897f\u5b89\u662f\u6211\u56fd\u8457\u540d\u7684\u65c5\u6e38\u57ce\u5e02\uff0c\u6bcf\u5e74\u90fd\u6709\u5927\u91cf\u7684\u6e38\u5ba2\u6765\u5230\u897f\u5b89\u65c5\u6e38\uff0c\u897f\u5b89\u7684\u65c5\u6e38\u8d44\u6e90\u975e\u5e38\u4e30\u5bcc\uff0c\u6709\u5f88\u591a\u8457\u540d\u7684\u65c5\u6e38\u666f\u70b9\uff0c\u6bd4\u5982\u79e6\u59cb\u7687\u5175\u9a6c\u4fd1\u3001\u5927\u96c1\u5854\u3001\u534e\u6e05\u6c60\u3001\u5927\u5510\u8299\u84c9\u56ed\u3001\u897f\u5b89\u57ce\u5899\u3001\u5927\u660e\u5bab\u56fd\u5bb6\u9057\u5740\u516c\u56ed\u3001\u897f\u5b89\u7891\u6797\u535a\u7269\u9986\u3001\u897f\u5b89\u949f\u697c\u3001\u897f\u5b89\u9f13\u697c\u3001\u897f\u5b89\u534a\u5761\u535a\u7269\u9986\u3001\u897f\u5b89\u5927\u5174\u5584\u5bfa\u3001\u897f\u5b89\u5c0f\u96c1\u5854\n\n\n>>> inputs = tokenizer('\u9655\u897f\u7684\u7701\u4f1a\u662f\u897f\u5b89\uff0c\u7518\u8083\u7684\u7701\u4f1a\u662f\u5170\u5dde\uff0c\u6cb3\u5357\u7684\u7701\u4f1a\u662f\u90d1\u5dde', return_tensors='pt').to(model.device)\n>>> response = model.generate(inputs.input_ids, max_length=128)\n>>> print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))\n\u9655\u897f\u7684\u7701\u4f1a\u662f\u897f\u5b89\uff0c\u7518\u8083\u7684\u7701\u4f1a\u662f\u5170\u5dde\uff0c\u6cb3\u5357\u7684\u7701\u4f1a\u662f\u90d1\u5dde\uff0c\u6e56\u5317\u7684\u7701\u4f1a\u662f\u6b66\u6c49\uff0c\u6e56\u5357\u7684\u7701\u4f1a\u662f\u957f\u6c99\uff0c\u6c5f\u897f\u7684\u7701\u4f1a\u662f\u5357\u660c\uff0c\u5b89\u5fbd\u7684\u7701\u4f1a\u662f\u5408\u80a5\uff0c\u6c5f\u82cf\u7684\u7701\u4f1a\u662f\u5357\u4eac\uff0c\u6d59\u6c5f\u7684\u7701\u4f1a\u662f\u676d\u5dde\uff0c\u798f\u5efa\u7684\u7701\u4f1a\u662f\u798f\u5dde\uff0c\u5e7f\u4e1c\u7684\u7701\u4f1a\u662f\u5e7f\u5dde\uff0c\u5e7f\u897f\u7684\u7701\u4f1a\u662f\u5357\u5b81\uff0c\u6d77\u5357\u7684\u7701\u4f1a\u662f\u6d77\u53e3\uff0c\u56db\u5ddd\u7684\u7701\u4f1a\u662f\u6210\u90fd\uff0c\u8d35\u5dde\u7684\u7701\u4f1a\u662f\u8d35\u9633\uff0c\u4e91\u5357\u7684\u7701\u4f1a\u662f\u6606\u660e\uff0c\u897f\u85cf\u7684\u7701\u4f1a\u662f\u62c9\u8428\uff0c\u9752\u6d77\u7684\u7701\u4f1a\u662f\u897f\u5b81\uff0c\u5b81\u590f\u7684\u7701\u4f1a\u662f\u94f6\u5ddd\uff0c\u65b0\u7586\u7684\u7701\u4f1a\u662f\u4e4c\u9c81\u6728\u9f50\u3002\n\n"], "xverse/XVERSE-7B": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"xverse/XVERSE-7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"xverse/XVERSE-7B\", trust_remote_code=True, torch_dtype=torch.float16, device_map='auto')\nmodel = model.eval()\ninputs = tokenizer('\u5317\u4eac\u7684\u666f\u70b9\uff1a\u6545\u5bab\u3001\u5929\u575b\u3001\u4e07\u91cc\u957f\u57ce\u7b49\u3002\\n\u6df1\u5733\u7684\u666f\u70b9\uff1a', return_tensors='pt').input_ids\ninputs = inputs.cuda()\ngenerated_ids = model.generate(inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n"], "mrm8488/bert-multi-cased-finetuned-xquadv1": ["from transformers import pipeline\n\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"mrm8488/bert-multi-cased-finetuned-xquadv1\",\n    tokenizer=\"mrm8488/bert-multi-cased-finetuned-xquadv1\"\n)\n\n\n# context: Coronavirus is seeding panic in the West because it expands so fast.\n\n# question: Where is seeding panic Coronavirus?\nqa_pipeline({\n    'context': \"\u0915\u094b\u0930\u094b\u0928\u093e\u0935\u093e\u092f\u0930\u0938 \u092a\u0936\u094d\u091a\u093f\u092e \u092e\u0947\u0902 \u0906\u0924\u0902\u0915 \u092c\u094b \u0930\u0939\u093e \u0939\u0948 \u0915\u094d\u092f\u094b\u0902\u0915\u093f \u092f\u0939 \u0907\u0924\u0928\u0940 \u0924\u0947\u091c\u0940 \u0938\u0947 \u092b\u0948\u0932\u0924\u093e \u0939\u0948\u0964\",\n    'question': \"\u0915\u094b\u0930\u094b\u0928\u093e\u0935\u093e\u092f\u0930\u0938 \u0918\u092c\u0930\u093e\u0939\u091f \u0915\u0939\u093e\u0902 \u0939\u0948?\"\n    \n})\n# output: {'answer': '\u092a\u0936\u094d\u091a\u093f\u092e', 'end': 18, 'score': 0.7037217439689059, 'start': 12}\n\nqa_pipeline({\n    'context': \"Manuel Romero has been working hardly in the repository hugginface/transformers lately\",\n    'question': \"Who has been working hard for hugginface/transformers lately?\"\n    \n})\n# output: {'answer': 'Manuel Romero', 'end': 13, 'score': 0.7254485993702389, 'start': 0}\n\nqa_pipeline({\n    'context': \"Manuel Romero a travaill\u00e9 \u00e0 peine dans le r\u00e9f\u00e9rentiel hugginface / transformers ces derniers temps\",\n    'question': \"Pour quel r\u00e9f\u00e9rentiel a travaill\u00e9 Manuel Romero r\u00e9cemment?\"\n    \n})\n#output: {'answer': 'hugginface / transformers', 'end': 79, 'score': 0.6482061613915384, 'start': 54}\n"], "xverse/XVERSE-65B": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"xverse/XVERSE-65B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"xverse/XVERSE-65B\", trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')\nmodel = model.eval()\ninputs = tokenizer('\u5317\u4eac\u7684\u666f\u70b9\uff1a\u6545\u5bab\u3001\u5929\u575b\u3001\u4e07\u91cc\u957f\u57ce\u7b49\u3002\\n\u6df1\u5733\u7684\u666f\u70b9\uff1a', return_tensors='pt').input_ids\ninputs = inputs.cuda()\ngenerated_ids = model.generate(inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n"], "xlm-mlm-ende-1024": [], "xverse/XVERSE-65B-2": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"xverse/XVERSE-65B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"xverse/XVERSE-65B\", trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')\nmodel = model.eval()\ninputs = tokenizer('\u5317\u4eac\u7684\u666f\u70b9\uff1a\u6545\u5bab\u3001\u5929\u575b\u3001\u4e07\u91cc\u957f\u57ce\u7b49\u3002\\n\u6df1\u5733\u7684\u666f\u70b9\uff1a', return_tensors='pt').input_ids\ninputs = inputs.cuda()\ngenerated_ids = model.generate(inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n"], "Qwen/Qwen-1_8B-Chat-Int4": [], "mrm8488/bert-tiny-finetuned-squadv2": ["from transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"mrm8488/bert-tiny-finetuned-squadv2\",\n    tokenizer=\"mrm8488/bert-tiny-finetuned-squadv2\"\n)\n\nqa_pipeline({\n    'context': \"Manuel Romero has been working hardly in the repository hugginface/transformers lately\",\n    'question': \"Who has been working hard for hugginface/transformers lately?\"\n\n})\n\n# Output:\n"], "hfl/chinese-xlnet-base": [], "google/mobilenet_v2_0.75_160": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\npreprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v2_0.75_160\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v2_0.75_160\")\n\ninputs = preprocessor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "google/mobilenet_v2_0.35_96": ["from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\npreprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v2_0.35_96\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v2_0.35_96\")\n\ninputs = preprocessor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "microsoft/focalnet-tiny": ["from transformers import FocalNetImageProcessor, FocalNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\npreprocessor = FocalNetImageProcessor.from_pretrained(\"microsoft/focalnet-tiny\")\nmodel = FocalNetForImageClassification.from_pretrained(\"microsoft/focalnet-tiny\")\n\ninputs = preprocessor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n"], "bert-base-german-dbmdz-cased": [], "nghuyong/ernie-3.0-medium-zh": ["from transformers import BertTokenizer, ErnieModel\n\ntokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-medium-zh\")\nmodel = ErnieModel.from_pretrained(\"nghuyong/ernie-3.0-medium-zh\")\n"], "TsinghuaAI/CPM-Generate": ["from transformers import TextGenerationPipeline, AutoTokenizer, AutoModelWithLMHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"TsinghuaAI/CPM-Generate\")\nmodel = AutoModelWithLMHead.from_pretrained(\"TsinghuaAI/CPM-Generate\")\n\ntext_generator = TextGenerationPipeline(model, tokenizer)\ntext_generator('\u6e05\u534e\u5927\u5b66', max_length=50, do_sample=True, top_p=0.9)\n"], "RUCAIBox/mtl-data-to-text": [">>> from transformers import MvpTokenizer, MvpForConditionalGeneration\n\n>>> tokenizer = MvpTokenizer.from_pretrained(\"RUCAIBox/mvp\")\n>>> model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mtl-data-to-text\")\n\n>>> inputs = tokenizer(\n...     \"Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man\",\n...     return_tensors=\"pt\",\n... )\n>>> generated_ids = model.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n['Iron Man is a fictional superhero appearing in American comic books published by Marvel Comics.']\n"], "unsloth/mistral-7b": [], "stas/tiny-wmt19-en-de": [], "xlm-mlm-tlm-xnli15-1024": [], "masterful/gligen-1-4-generation-text-box": ["import torch\nfrom diffusers import StableDiffusionGLIGENPipeline\nfrom diffusers.utils import load_image\n\n# Generate an image described by the prompt and\n# insert objects described by text at the region defined by bounding boxes\npipe = StableDiffusionGLIGENPipeline.from_pretrained(\n    \"masterful/gligen-1-4-generation-text-box\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a waterfall and a modern high speed train running through the tunnel in a beautiful forest with fall foliage\"\nboxes = [[0.1387, 0.2051, 0.4277, 0.7090], [0.4980, 0.4355, 0.8516, 0.7266]]\nphrases = [\"a waterfall\", \"a modern high speed train running through the tunnel\"]\n\nimages = pipe(\n    prompt=prompt,\n    gligen_phrases=phrases,\n    gligen_boxes=boxes,\n    gligen_scheduled_sampling_beta=1,\n    output_type=\"pil\",\n    num_inference_steps=50,\n).images\n\nimages[0].save(\"./gligen-1-4-generation-text-box.jpg\")\n"], "microsoft/cvt-21": ["from transformers import AutoFeatureExtractor, CvtForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/cvt-21')\nmodel = CvtForImageClassification.from_pretrained('microsoft/cvt-21')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "google/switch-c-2048": ["# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-c-2048\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-c-2048\", device_map=\"auto\", offload_folder=<OFFLOAD_FOLDER>)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-c-2048\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-c-2048\", device_map=\"auto\", offload_folder=<OFFLOAD_FOLDER>)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-c-2048\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-c-2048\", device_map=\"auto\", torch_dtype=torch.bfloat16, offload_folder=<OFFLOAD_FOLDER>)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-c-2048\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-c-2048\", device_map=\"auto\", offload_folder=<OFFLOAD_FOLDER>)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n"], "takuma104/lora-test-text-encoder-lora-target": [], "xlm-mlm-enro-1024": [], "xlm-mlm-xnli15-1024": [], "microsoft/xprophetnet-large-wiki100-cased": ["from transformers import XLMProphetNetForConditionalGeneration, XLMProphetNetTokenizer\n\nmodel = XLMProphetNetForConditionalGeneration.from_pretrained(\"microsoft/xprophetnet-large-wiki100-cased\")\ntokenizer = XLMProphetNetTokenizer.from_pretrained(\"microsoft/xprophetnet-large-wiki100-cased\")\n\ninput_str = \"the us state department said wednesday it had received no formal word from bolivia that it was expelling the us ambassador there but said the charges made against him are `` baseless .\"\ntarget_str = \"us rejects charges against its ambassador in bolivia\"\n\ninput_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\nlabels = tokenizer(target_str, return_tensors=\"pt\").input_ids\n\nloss = model(input_ids, labels=labels).loss\n"], "bhadresh-savani/electra-base-emotion": ["from transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='bhadresh-savani/electra-base-emotion', return_all_scores=True)\nprediction = classifier(\"I love using transformers. The best part is wide range of support and its easy to use\", )\nprint(prediction)\n\n\"\"\"\nOutput:\n[[\n{'label': 'sadness', 'score': 0.0006792712374590337}, \n{'label': 'joy', 'score': 0.9959300756454468}, \n{'label': 'love', 'score': 0.0009452480007894337}, \n{'label': 'anger', 'score': 0.0018055217806249857}, \n{'label': 'fear', 'score': 0.00041110432357527316}, \n{'label': 'surprise', 'score': 0.0002288572577526793}\n]]\n\"\"\"\n"], "microsoft/git-large": [], "ClueAI/ChatYuan-large-v2": ["# transfomers\u7248\u672c\n#!pip install transformers==4.29.1\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport os\n# \u52a0\u8f7d\u6a21\u578b\n\nmodel_dir='ClueAI/ChatYuan-large-v2'\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\n# \u901f\u5ea6\u4f1a\u53d7\u5230\u7f51\u7edc\u5f71\u54cd\nmodel = AutoModel.from_pretrained(model_dir, trust_remote_code=True)\nhistory = []\nprint(\"starting\")\nwhile True:\n    query = input(\"\\n\u7528\u6237\uff1a\")\n    if query == \"stop\":\n        break\n    if query == \"clear\":\n        history = []\n        os.system('clear')\n        continue\n    response, history = model.chat(tokenizer, query, history=history)\n    print(f\"\u5c0f\u5143\uff1a{response}\") \n", "# \u52a0\u8f7d\u6a21\u578b\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# \u81ea\u52a8\u4e0b\u8f7d\u4e00\u6b21\u540e\uff0c\u672c\u5730\u8fd0\u884c\uff0c\u4e0d\u53d7\u7f51\u7edc\u5f71\u54cd\ntokenizer = T5Tokenizer.from_pretrained(\"ClueAI/ChatYuan-large-v2\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"ClueAI/ChatYuan-large-v2\")\n# \u8be5\u52a0\u8f7d\u65b9\u5f0f\uff0c\u5728\u6700\u5927\u957f\u5ea6\u4e3a512\u65f6 \u5927\u7ea6\u9700\u89816G\u591a\u663e\u5b58\n# \u5982\u663e\u5b58\u4e0d\u591f\uff0c\u53ef\u91c7\u7528\u4ee5\u4e0b\u65b9\u5f0f\u52a0\u8f7d\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u663e\u5b58\u9700\u6c42\uff0c\u7ea6\u4e3a3G\n# model = T5ForConditionalGeneration.from_pretrained(\"ClueAI/ChatYuan-large-v2\").half()\n\n\n", "# \u4f7f\u7528\nimport torch\nfrom transformers import AutoTokenizer\n# \u4fee\u6539colab\u7b14\u8bb0\u672c\u8bbe\u7f6e\u4e3agpu\uff0c\u63a8\u7406\u66f4\u5feb\ndevice = torch.device('cuda')\nmodel.to(device)\ndef preprocess(text):\n  text = text.replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n  return text\n\ndef postprocess(text):\n  return text.replace(\"\\\\n\", \"\\n\").replace(\"\\\\t\", \"\\t\").replace('%20','  ')\n\ndef answer(text, sample=True, top_p=1, temperature=0.7, context=\"\"):\n  '''sample\uff1a\u662f\u5426\u62bd\u6837\u3002\u751f\u6210\u4efb\u52a1\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u4e3aTrue;\n  top_p\uff1a0-1\u4e4b\u95f4\uff0c\u751f\u6210\u7684\u5185\u5bb9\u8d8a\u591a\u6837'''\n  text = f\"{context}\\n\u7528\u6237\uff1a{text}\\n\u5c0f\u5143\uff1a\"\n  text = text.strip()\n  text = preprocess(text)\n  encoding = tokenizer(text=[text], truncation=True, padding=True, max_length=512, return_tensors=\"pt\").to(device) \n  if not sample:\n    out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_new_tokens=512, num_beams=1, length_penalty=0.6)\n  else:\n    out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_new_tokens=512, do_sample=True, top_p=top_p, temperature=temperature, no_repeat_ngram_size=3)\n  out_text = tokenizer.batch_decode(out[\"sequences\"], skip_special_tokens=True)\n  return postprocess(out_text[0])\nprint(\"end...\")\n", "chatyuan-large-v2\n\nSubject: Due to delay in logistics, our company does not order the item as planned on time.\n\nWe understand that this is unfortunately caused by the delay in the delivery of the item, and we regret to investigate its cause. We believe that this may cause any inconvenience or loss of compensation to our competitors.\n\nOur company believes we can address any underlying issues that may arise. We are confident that our services will be acceptable if any inconformity occurs.\n\nPlease let us know if we can assist us in resolving any issues that could arise if the delivery team fails. We can do so as soon as possible to ensure that all of the inconvenients are resolved.\n\nThank you for your understanding and support. We look forward to hearing from you soon.\n\nBest regards,\n", "def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] > arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n\n# \u6d4b\u8bd5\narr = [64, 34, 25, 12, 22, 11, 90]\nbubble_sort(arr)\nprint(\"\u6392\u5e8f\u540e\u7684\u6570\u7ec4\uff1a\")\nfor i in range(len(arr)):\n    print(\"%d\" %arr[i])\n", "\u7ebf\u6027\u56de\u5f52\u4ee3\u7801\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# \u521d\u59cb\u5316\u6a21\u578b\nlr = LinearRegularization()\n\n# \u62df\u5408\u6570\u636e\u96c6\nX_train_new = np.vstack((X_train, y_train))\ny_train_lr = lr.fit_transform(X_trains)\n\n# \u65b0\u6a21\u578b\u8bad\u7ec3\nmodel = Linearize()\nmodel.fit(X_traIN, y_trIN)\n\n# \u91d1\u5b50\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\ny_pred_lr = \"train\"\nmodel.predict_proba(X_test)[:,1] = 0.5\n\n# \u8f93\u51fa\u9884\u6d4b\u7ed3\u679c\nprint(\"\u9884\u6d4b\u7ed3\u679c\uff1a\", y_pred)\n\n"], "laion/CLIP-ViT-B-32-DataComp.M-s128M-b4K": [], "lysandre/arxiv-nlp": [], "junnyu/roformer_chinese_small": ["import torch\nfrom transformers import RoFormerForMaskedLM, RoFormerTokenizer\n\ntext = \"\u4eca\u5929[MASK]\u5f88\u597d\uff0c\u6211[MASK]\u53bb\u516c\u56ed\u73a9\u3002\"\ntokenizer = RoFormerTokenizer.from_pretrained(\"junnyu/roformer_chinese_small\")\npt_model = RoFormerForMaskedLM.from_pretrained(\"junnyu/roformer_chinese_small\")\npt_inputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    pt_outputs = pt_model(**pt_inputs).logits[0]\npt_outputs_sentence = \"pytorch: \"\nfor i, id in enumerate(tokenizer.encode(text)):\n    if id == tokenizer.mask_token_id:\n        tokens = tokenizer.convert_ids_to_tokens(pt_outputs[i].topk(k=5)[1])\n        pt_outputs_sentence += \"[\" + \"||\".join(tokens) + \"]\"\n    else:\n        pt_outputs_sentence += \"\".join(\n            tokenizer.convert_ids_to_tokens([id], skip_special_tokens=True))\nprint(pt_outputs_sentence)\n# pytorch: \u4eca\u5929[\u5929\u6c14||\u5fc3\u60c5||\u611f\u89c9||\u73af\u5883||\u4e0b\u5348]\u5f88\u597d\uff0c\u6211[\u8981||\u60f3||\u5c31||\u53ef\u4ee5||\u53bb]\u53bb\u516c\u56ed\u73a9\u3002\n", "import tensorflow as tf\nfrom transformers import RoFormerTokenizer, TFRoFormerForMaskedLM\ntext = \"\u4eca\u5929[MASK]\u5f88\u597d\uff0c\u6211[MASK]\u53bb\u516c\u56ed\u73a9\u3002\"\ntokenizer = RoFormerTokenizer.from_pretrained(\"junnyu/roformer_chinese_small\")\ntf_model = TFRoFormerForMaskedLM.from_pretrained(\"junnyu/roformer_chinese_small\")\ntf_inputs = tokenizer(text, return_tensors=\"tf\")\ntf_outputs = tf_model(**tf_inputs, training=False).logits[0]\ntf_outputs_sentence = \"tf2.0: \"\nfor i, id in enumerate(tokenizer.encode(text)):\n    if id == tokenizer.mask_token_id:\n        tokens = tokenizer.convert_ids_to_tokens(\n            tf.math.top_k(tf_outputs[i], k=5)[1])\n        tf_outputs_sentence += \"[\" + \"||\".join(tokens) + \"]\"\n    else:\n        tf_outputs_sentence += \"\".join(\n            tokenizer.convert_ids_to_tokens([id], skip_special_tokens=True))\nprint(tf_outputs_sentence)\n# tf2.0 \u4eca\u5929[\u5929\u6c14||\u5fc3\u60c5||\u611f\u89c9||\u73af\u5883||\u4e0b\u5348]\u5f88\u597d\uff0c\u6211[\u8981||\u60f3||\u5c31||\u53ef\u4ee5||\u53bb]\u53bb\u516c\u56ed\u73a9\u3002\n"], "AIML-TUDA/stable-diffusion-safe": ["from sld import SLDPipeline\ndevice='cuda'\n#####################\n# Path to your local clone of the weights\nckpt_path = ''\n####################\npipe = SLDPipeline.from_pretrained(\n    ckpt_path,\n).to(device)    \n"], "ybelkada/bloom-1b7-8bit": ["# pip install accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ybelkada/bloom-1b7-8bit\")\n", "# pip install accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\", device_map=\"auto\", load_in_8bit=True)\n"], "apple/deeplabv3-mobilevit-small": ["from transformers import MobileViTFeatureExtractor, MobileViTForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\nmodel = MobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_mask = logits.argmax(1).squeeze(0)\n"], "google/ul2": ["from transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \ntokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n\ninput_string = \"[S2S] Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man with a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere <extra_id_0>\"                                               \n\ninputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# -> <pad>. Dudley was a very good boy, but he was also very stupid.</s>\n", "from transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \ntokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n\ninput_string = \"[NLU] Mr. Dursley was the director of a firm called <extra_id_0>, which made <extra_id_1>. He was a big, solid man with a bald head. Mrs. Dursley was thin and <extra_id_2> of neck, which came in very useful as she spent so much of her time <extra_id_3>. The Dursleys had a small son called Dudley and <extra_id_4>\"                                               \n\ninputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# -> \"<pad><extra_id_0> Burrows<extra_id_1> brooms for witches and wizards<extra_id_2> had a lot<extra_id_3> scolding Dudley<extra_id_4> a daughter called Petunia. Dudley was a nasty, spoiled little boy who was always getting into trouble. He was very fond of his pet rat, Scabbers.<extra_id_5> Burrows<extra_id_3> screaming at him<extra_id_4> a daughter called Petunia</s>\n\"\n", "from transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \ntokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n\ninput_string = \"[NLG] Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man wiht a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she\nspent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. <extra_id_0>\"                                               \n\nmodel.cuda()\ninputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# -> \"<pad><extra_id_0> Burrows<extra_id_1> a lot of money from the manufacture of a product called '' Burrows'''s ''<extra_id_2> had a lot<extra_id_3> looking down people's throats<extra_id_4> a daughter called Petunia. Dudley was a very stupid boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat,\"\n"], "moussaKam/mbarthez": [], "01-ai/Yi-34B-Chat-4bits": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "google/switch-large-128": ["\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-large-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-large-128\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-large-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-large-128\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-large-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-large-128\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n", "# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-large-128\")\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-large-128\", device_map=\"auto\")\n\ninput_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n>>> <pad> <extra_id_0> man<extra_id_1> beer<extra_id_2> a<extra_id_3> salt<extra_id_4>.</s>\n"], "baichuan-inc/Baichuan-13B-Base": [], "apple/deeplabv3-mobilevit-x-small": ["from transformers import MobileViTFeatureExtractor, MobileViTForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/deeplabv3-mobilevit-x-small\")\nmodel = MobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-x-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_mask = logits.argmax(1).squeeze(0)\n"], "IEITYuan/Yuan2-2B-hf": ["import torch, transformers\nimport sys, os\nsys.path.append(\n    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\nfrom transformers import AutoModelForCausalLM,AutoTokenizer,LlamaTokenizer\n\nprint(\"Creat tokenizer...\")\ntokenizer = LlamaTokenizer.from_pretrained('IEITYuan/Yuan2-2B-hf', add_eos_token=False, add_bos_token=False, eos_token='<eod>')\ntokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n\nprint(\"Creat model...\")\nmodel = AutoModelForCausalLM.from_pretrained('IEITYuan/Yuan2-2B-hf', device_map='auto', torch_dtype=torch.bfloat16, trust_remote_code=True)\n\ninputs = tokenizer(\"\u8bf7\u95ee\u76ee\u524d\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6709\u54ea\u4e9b\uff1f\", return_tensors=\"pt\")[\"input_ids\"].to(\"cuda:0\")\noutputs = model.generate(inputs,do_sample=False,max_length=100)\nprint(tokenizer.decode(outputs[0]))\n"], "IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese": ["from transformers import PegasusForConditionalGeneration\n# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\n# Strongly recommend you git clone the Fengshenbang-LM repo:\n# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n\nfrom tokenizers_pegasus import PegasusTokenizer\n\nmodel = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese\")\ntokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese\")\n\ntext = \"\u5728\u5317\u4eac\u51ac\u5965\u4f1a\u81ea\u7531\u5f0f\u6ed1\u96ea\u5973\u5b50\u5761\u9762\u969c\u788d\u6280\u5de7\u51b3\u8d5b\u4e2d\uff0c\u4e2d\u56fd\u9009\u624b\u8c37\u7231\u51cc\u593a\u5f97\u94f6\u724c\u3002\u795d\u8d3a\u8c37\u7231\u51cc\uff01\u4eca\u5929\u4e0a\u5348\uff0c\u81ea\u7531\u5f0f\u6ed1\u96ea\u5973\u5b50\u5761\u9762\u969c\u788d\u6280\u5de7\u51b3\u8d5b\u4e3e\u884c\u3002\u51b3\u8d5b\u5206\u4e09\u8f6e\u8fdb\u884c\uff0c\u53d6\u9009\u624b\u6700\u4f73\u6210\u7ee9\u6392\u540d\u51b3\u51fa\u5956\u724c\u3002\u7b2c\u4e00\u8df3\uff0c\u4e2d\u56fd\u9009\u624b\u8c37\u7231\u51cc\u83b7\u5f9769.90\u5206\u3002\u572812\u4f4d\u9009\u624b\u4e2d\u6392\u540d\u7b2c\u4e09\u3002\u5b8c\u6210\u52a8\u4f5c\u540e\uff0c\u8c37\u7231\u51cc\u53c8\u626e\u4e86\u4e2a\u9b3c\u8138\uff0c\u751a\u662f\u53ef\u7231\u3002\u7b2c\u4e8c\u8f6e\u4e2d\uff0c\u8c37\u7231\u51cc\u5728\u9053\u5177\u533a\u7b2c\u4e09\u4e2a\u969c\u788d\u5904\u5931\u8bef\uff0c\u843d\u5730\u65f6\u6454\u5012\u3002\u83b7\u5f9716.98\u5206\u3002\u7f51\u53cb\uff1a\u6454\u5012\u4e86\u4e5f\u6ca1\u5173\u7cfb\uff0c\u7ee7\u7eed\u52a0\u6cb9\uff01\u5728\u7b2c\u4e8c\u8df3\u5931\u8bef\u6454\u5012\u7684\u60c5\u51b5\u4e0b\uff0c\u8c37\u7231\u51cc\u9876\u4f4f\u538b\u529b\uff0c\u7b2c\u4e09\u8df3\u7a33\u7a33\u53d1\u6325\uff0c\u6d41\u7545\u843d\u5730\uff01\u83b7\u5f9786.23\u5206\uff01\u6b64\u8f6e\u6bd4\u8d5b\uff0c\u517112\u4f4d\u9009\u624b\u53c2\u8d5b\uff0c\u8c37\u7231\u51cc\u7b2c10\u4f4d\u51fa\u573a\u3002\u7f51\u53cb\uff1a\u770b\u6bd4\u8d5b\u65f6\u6211\u6bd4\u8c37\u7231\u51cc\u7d27\u5f20\uff0c\u52a0\u6cb9\uff01\"\ninputs = tokenizer(text, max_length=1024, return_tensors=\"pt\")\n\n# Generate Summary\nsummary_ids = model.generate(inputs[\"input_ids\"])\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\n# model Output: \u6ed1\u96ea\u5973\u5b50\u5761\u9762\u969c\u788d\u6280\u5de7\u51b3\u8d5b\u8c37\u7231\u51cc\u83b7\u94f6\u724c\n"], "microsoft/prophetnet-large-uncased-cnndm": ["from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration, ProphetNetConfig\n\nmodel = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased-cnndm')\ntokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased-cnndm')\n\nARTICLE_TO_SUMMARIZE = \"USTC was founded in Beijing by the Chinese Academy of Sciences (CAS) in September 1958. The Director of CAS, Mr. Guo Moruo was appointed the first president of USTC. USTC's founding mission was to develop a high-level science and technology workforce, as deemed critical for development of China's economy, defense, and science and technology education. The establishment was hailed as \\\"A Major Event in the History of Chinese Education and Science.\\\" CAS has supported USTC by combining most of its institutes with the departments of the university. USTC is listed in the top 16 national key universities, becoming the youngest national key university.\".lower()\ninputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=100, return_tensors='pt')\n\n# Generate Summary\nsummary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=512, early_stopping=True)\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n\n# should give: 'ustc was founded in beijing by the chinese academy of sciences in 1958. [X_SEP] ustc\\'s mission was to develop a high - level science and technology workforce. [X_SEP] the establishment was hailed as \" a major event in the history of chinese education and science \"'\n"], "andreasmadsen/efficient_mlm_m0.15": [], "internlm/internlm-chat-20b": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-20b\", trust_remote_code=True)\n# Set `torch_dtype=torch.bfloat16` to load model in bfloat16, otherwise it will be loaded as float32 and cause OOM Error.\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-20b\", torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\nmodel = model.eval()\noutput, history = model.chat(tokenizer, \"Hello! Today is sunny, it is time to go out\")\nprint(output)\n# Hello! Today is sunny, and it sounds like a great day to go out an enjoy the weather. What would you like to do?\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"internlm/internlm-chat-20b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\nmodel = model.eval()\nlength = 0\nfor response, history in model.stream_chat(tokenizer, \"Hello\", history=[]):\n    print(response[length:], flush=True, end=\"\")\n    length = len(response)\n", "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-20b\", trust_remote_code=True)\n# `torch_dtype=torch.bfloat16` \u53ef\u4ee5\u4ee4\u6a21\u578b\u4ee5 bfloat16 \u7cbe\u5ea6\u52a0\u8f7d\uff0c\u5426\u5219 transformers \u4f1a\u5c06\u6a21\u578b\u52a0\u8f7d\u4e3a float32\uff0c\u5bfc\u81f4\u663e\u5b58\u4e0d\u8db3\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-20b\", torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\nmodel = model.eval()\noutput, history = model.chat(tokenizer, \"\u4f60\u597d\u5440\uff01\u4eca\u5929\u5929\u6c14\u771f\u597d\")\nprint(output)\n# \u4f60\u597d\uff01\u662f\u7684\uff0c\u4eca\u5929\u7684\u5929\u6c14\u975e\u5e38\u6674\u6717\uff0c\u975e\u5e38\u9002\u5408\u6237\u5916\u6d3b\u52a8\u3002\n", "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"internlm/internlm-chat-20b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dype=torch.bfloat16, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\nmodel = model.eval()\nlength = 0\nfor response, history in model.stream_chat(tokenizer, \"\u4f60\u597d\", history=[]):\n    print(response[length:], flush=True, end=\"\")\n    length = len(response)\n"], "google/electra-large-generator": ["from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"google/electra-large-generator\",\n    tokenizer=\"google/electra-large-generator\"\n)\n\nprint(\n    fill_mask(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\")\n)\n"], "RunDiffusion/Juggernaut-XL": [], "timm/eva_giant_patch14_clip_224.laion400m_s11b_b41k": [], "internlm/internlm-chat-7b-8k": [], "TheBloke/Llama-2-13B-chat-GGML": [], "vumichien/mobilebert-uncased-squad-v2": [], "Salesforce/codet5p-6b": ["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ncheckpoint = \"Salesforce/codet5p-6b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint,\n                                              torch_dtype=torch.float16,\n                                              trust_remote_code=True).to(device)\n\nencoding = tokenizer(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\nencoding['decoder_input_ids'] = encoding['input_ids'].clone()\noutputs = model.generate(**encoding, max_length=15)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"], "Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind": [], "naver-clova-ix/donut-base-finetuned-zhtrainticket": [], "microsoft/prophetnet-large-uncased-squad-qg": ["from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration, ProphetNetConfig\n\nmodel = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased-squad-qg')\ntokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased-squad-qg')\n\nFACT_TO_GENERATE_QUESTION_FROM = \"\"Bill Gates [SEP] Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975.\"\n\ninputs = tokenizer([FACT_TO_GENERATE_QUESTION_FROM], return_tensors='pt')\n\n# Generate Summary\nquestion_ids = model.generate(inputs['input_ids'], num_beams=5, early_stopping=True)\ntokenizer.batch_decode(question_ids, skip_special_tokens=True)\n\n# should give: 'along with paul allen, who founded microsoft?'\n"], "facebook/mask2former-swin-small-cityscapes-panoptic": ["import requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\n\n# load Mask2Former fine-tuned on Cityscapes panoptic segmentation\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\")\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# you can pass them to processor for postprocessing\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n# we refer to the demo notebooks for visualization (see \"Resources\" section in the Mask2Former docs)\npredicted_panoptic_map = result[\"segmentation\"]\n"], "google/long-t5-local-large": ["from transformers import AutoTokenizer, LongT5Model\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-large\")\nmodel = LongT5Model.from_pretrained(\"google/long-t5-local-large\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n"], "SenseTime/deformable-detr-single-scale": ["from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr-single-scale\")\nmodel = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr-single-scale\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.7\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n"], "thu-ml/unidiffuser-v1": ["from diffusers import UniDiffuserPipeline\n\npipe = UniDiffuserPipeline.from_pretrained(\"thu-ml/unidiffuser-v1\")\n", "import requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom diffusers import UniDiffuserPipeline\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Joint image-text generation. The generation task is automatically inferred.\nsample = pipe(num_inference_steps=20, guidance_scale=8.0)\nimage = sample.images[0]\ntext = sample.text[0]\nimage.save(\"unidiffuser_sample_joint_image.png\")\nprint(text)\n\n# The mode can be set manually. The following is equivalent to the above:\npipe.set_joint_mode()\nsample2 = pipe(num_inference_steps=20, guidance_scale=8.0)\n\n# Note that if you set the mode manually the pipeline will no longer attempt\n# to automatically infer the mode. You can re-enable this with reset_mode().\npipe.reset_mode()\n\n# Text-to-image generation.\nprompt = \"an elephant under the sea\"\n\nsample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)\nt2i_image = sample.images[0]\nt2i_image.save(\"unidiffuser_sample_text2img_image.png\")\n\n# Image-to-text generation.\nimage_url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unidiffuser/unidiffuser_example_image.jpg\"\nresponse = requests.get(image_url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((512, 512))\n\nsample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)\ni2t_text = sample.text[0]\nprint(i2t_text)\n\n# Image variation can be performed with a image-to-text generation followed by a text-to-image generation:\nsample = pipe(prompt=i2t_text, num_inference_steps=20, guidance_scale=8.0)\nfinal_image = sample.images[0]\nfinal_image.save(\"unidiffuser_image_variation_sample.png\")\n\n# Text variation can be performed with a text-to-image generation followed by a image-to-text generation:\nsample = pipe(image=t2i_image, num_inference_steps=20, guidance_scale=8.0)\nfinal_prompt = sample.text[0]\nprint(final_prompt)\n"], "timm/eva02_enormous_patch14_clip_224.laion2b_s4b_b115k": [], "Qwen/Qwen1.5-7B-Chat-GPTQ-Int8": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "studio-ousia/mluke-base": [], "tiiuae/falcon-rw-7b": ["from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-rw-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-rw-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n"], "Salesforce/codegen-16B-multi": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-16B-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-16B-multi\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "fixie-ai/ultravox": [], "TheBloke/vicuna-13B-v1.5-GPTQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/vicuna-13B-v1.5-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n"], "Intel/neural-chat-7b-v3": ["import transformers\n\n\nmodel_name = 'Intel/neural-chat-7b-v3'\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\ndef generate_response(system_input, user_input):\n\n    # Format the input using the provided template\n    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n\n    # Tokenize and encode the prompt\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n\n    # Generate a response\n    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Extract only the assistant's response\n    return response.split(\"### Assistant:\\n\")[-1]\n\n\n# Example usage\nsystem_input = \"You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.\"\nuser_input = \"calculate 100 + 520 + 60\"\nresponse = generate_response(system_input, user_input)\nprint(response)\n\n# expected response\n\"\"\"\nTo calculate the sum of 100, 520, and 60, we will follow these steps:\n\n1. Add the first two numbers: 100 + 520\n2. Add the result from step 1 to the third number: (100 + 520) + 60\n\nStep 1: Add 100 and 520\n100 + 520 = 620\n\nStep 2: Add the result from step 1 to the third number (60)\n(620) + 60 = 680\n\nSo, the sum of 100, 520, and 60 is 680.\n\"\"\"\n", "from transformers import AutoTokenizer, TextStreamer\nimport torch\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM\nimport intel_extension_for_pytorch as ipex\n\nmodel_name = \"Intel/neural-chat-7b-v3\"\nprompt = \"Once upon a time, there existed a little girl,\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\nmodel = ipex.optimize(model.eval(), dtype=torch.bfloat16, inplace=True, level=\"O1\", auto_kernel_selection=True)\n\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n", "from transformers import AutoTokenizer, TextStreamer\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\nmodel_name = \"Intel/neural-chat-7b-v3\"\n\n# for int8, should set weight_dtype=\"int8\"      \nconfig = WeightOnlyQuantConfig(compute_dtype=\"bf16\", weight_dtype=\"int4\")\nprompt = \"Once upon a time, there was a horse in the forest,\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n"], "Salesforce/codegen-6B-mono": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-6B-mono\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-6B-mono\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "Qwen/Qwen1.5-14B-Chat-GPTQ-Int8": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "Tanrei/GPTSAN-japanese": [">>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\", return_tensors=\"pt\")\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> gen_token = model.generate(input_ids, max_new_tokens=50)\n>>> tokenizer.decode(gen_token[0])\n\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\u653f\u6cbb\u30fb\u8ecd\u4e8b\u306e\u4e2d\u67a2\u307e\u3067\u638c\u63e1\u3057\u305f\u653f\u6cbb\u5bb6\u3067\u3042\u308a\u3001\u65e5\u672c\u53f2\u4e0a\u985e\u3092\u898b\u306a\u3044\u9a5a\u7570\u7684\u306a\u8ecd\u4e8b\u4fb5\u653b\u3092\u7d9a\u3051...\"\n", ">>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\"\", prefix_text=\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\", return_tensors=\"pt\")\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> token_type_ids = x_token.token_type_ids.to(device)\n>>> gen_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n>>> tokenizer.decode(gen_token[0])\n\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\u653f\u6cbb\u30fb\u5916\u4ea4\u3067\u6570\u3005\u306e\u6226\u679c\u3092\u4e0a\u3052\u308b\u304c\u30011568\u5e74\u304b\u3089\u306f\u3001\u3044\u308f\u3086\u308b\u672c\u80fd\u5bfa\u306e\u5909\u3067\u7d30\u5ddd\u6674\u5143\u306b\u6697\u6bba\u3055\u308c\u308b...\"\n", ">>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\n    \"\", prefix_text=\"\u6b66\u7530\u4fe1\u7384\u306f\u3001<|inputmask|>\u6642\u4ee3\u30d5\u30a1\u30f3\u306a\u3089\u305c\u3072\u62bc\u3055\u3048<|inputmask|>\u304d\u305f\u3044\u540d\u5c06\u306e\u4e00\u4eba\u3002\", return_tensors=\"pt\"\n)\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> token_type_ids = x_token.token_type_ids.to(device)\n>>> out_lm_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n>>> out_mlm_token = model(input_ids, token_type_ids=token_type_ids).logits.argmax(axis=-1)\n>>> tokenizer.decode(out_mlm_token[0])\n\"\u6b66\u7530\u4fe1\u7384\u306f\u3001\u6226\u56fd\u6642\u4ee3\u30d5\u30a1\u30f3\u306a\u3089\u305c\u3072\u62bc\u3055\u3048\u3066\u304a\u304d\u305f\u3044\u540d\u5c06\u306e\u4e00\u4eba\u3002\"\n>>> tokenizer.decode(out_lm_token[0][input_ids.shape[1] :])\n\"\u6b66\u7530\u6c0f\u306e\u4e09\u4ee3\u306b\u6e21\u3063\u305f\u6b66\u7530\u5bb6\u306e\u3072\u3068\u308a\\n\u7532\u6590\u5e02\u306b\u4f4f\u3080\u3001\u65e5\u672c\u53f2\u4e0a\u6700\u5927\u306e\u6226\u56fd\u5927\u540d\u3002...\"\n"], "Narsil/layoutlmv3-finetuned-funsd": [], "jozhang97/deta-swin-large-o365": [], "Salesforce/codegen-16B-mono": ["from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-16B-mono\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-16B-mono\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"], "google/t5-small-lm-adapt": [], "01-ai/Yi-6B-Chat-8bits": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = '<your-model-path>'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\n# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype='auto'\n).eval()\n\n# Prompt content: \"hi\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"hi\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\noutput_ids = model.generate(input_ids.to('cuda'))\nresponse = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n\n# Model response: \"Hello! How can I assist you today?\"\nprint(response)\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_DIR = \"01-ai/Yi-9B\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\ninput_text = \"# write the quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n", "# write the quick sort algorithm\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# test the quick sort algorithm\nprint(quick_sort([3, 6, 8, 10, 1, 2, 1]))\n", "...\n\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Pro\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_init: ggml.metallib not found, loading from source\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\nggml_metal_init: loading '/Users/yu/llama.cpp/ggml-metal.metal'\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: maxTransferRate               = built-in GPU\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   128.00 MiB, ( 2629.44 / 10922.67)\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2629.45 / 10922.67)\nllama_build_graph: non-view tensors processed: 676/676\nllama_new_context_with_model: compute buffer total size = 159.19 MiB\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   156.02 MiB, ( 2785.45 / 10922.67)\nAvailable slots:\n-> Slot 0 - max context: 2048\n\nllama server listening at http://0.0.0.0:8080\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34b\", use_fast=False)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34b\", device_map=\"auto\")\n"], "agne/jobBERT-de": [], "Qwen/Qwen1.5-0.5B-Chat-AWQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-0.5B-Chat-AWQ\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat-AWQ\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "shi-labs/dinat-mini-in1k-224": ["from transformers import AutoImageProcessor, DinatForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoImageProcessor.from_pretrained(\"shi-labs/dinat-mini-in1k-224\")\nmodel = DinatForImageClassification.from_pretrained(\"shi-labs/dinat-mini-in1k-224\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "KoichiYasuoka/deberta-base-japanese-aozora-ud-head": ["from transformers import AutoTokenizer,AutoModelForQuestionAnswering,QuestionAnsweringPipeline\ntokenizer=AutoTokenizer.from_pretrained(\"KoichiYasuoka/deberta-base-japanese-aozora-ud-head\")\nmodel=AutoModelForQuestionAnswering.from_pretrained(\"KoichiYasuoka/deberta-base-japanese-aozora-ud-head\")\nqap=QuestionAnsweringPipeline(tokenizer=tokenizer,model=model,align_to_words=False)\nprint(qap(question=\"\u56fd\u8a9e\",context=\"\u5168\u5b66\u5e74\u306b\u308f\u305f\u3063\u3066\u5c0f\u5b66\u6821\u306e\u56fd\u8a9e\u306e\u6559\u79d1\u66f8\u306b\u633f\u3057\u7d75\u304c\u7528\u3044\u3089\u308c\u3066\u3044\u308b\"))\n", "class TransformersUD(object):\n  def __init__(self,bert):\n    import os\n    from transformers import (AutoTokenizer,AutoModelForQuestionAnswering,\n      AutoModelForTokenClassification,AutoConfig,TokenClassificationPipeline)\n    self.tokenizer=AutoTokenizer.from_pretrained(bert)\n    self.model=AutoModelForQuestionAnswering.from_pretrained(bert)\n    x=AutoModelForTokenClassification.from_pretrained\n    if os.path.isdir(bert):\n      d,t=x(os.path.join(bert,\"deprel\")),x(os.path.join(bert,\"tagger\"))\n    else:\n      from transformers.utils import cached_file\n      c=AutoConfig.from_pretrained(cached_file(bert,\"deprel/config.json\"))\n      d=x(cached_file(bert,\"deprel/pytorch_model.bin\"),config=c)\n      s=AutoConfig.from_pretrained(cached_file(bert,\"tagger/config.json\"))\n      t=x(cached_file(bert,\"tagger/pytorch_model.bin\"),config=s)\n    self.deprel=TokenClassificationPipeline(model=d,tokenizer=self.tokenizer,\n      aggregation_strategy=\"simple\")\n    self.tagger=TokenClassificationPipeline(model=t,tokenizer=self.tokenizer)\n  def __call__(self,text):\n    import numpy,torch,ufal.chu_liu_edmonds\n    w=[(t[\"start\"],t[\"end\"],t[\"entity_group\"]) for t in self.deprel(text)]\n    z,n={t[\"start\"]:t[\"entity\"].split(\"|\") for t in self.tagger(text)},len(w)\n    r,m=[text[s:e] for s,e,p in w],numpy.full((n+1,n+1),numpy.nan)\n    v,c=self.tokenizer(r,add_special_tokens=False)[\"input_ids\"],[]\n    for i,t in enumerate(v):\n      q=[self.tokenizer.cls_token_id]+t+[self.tokenizer.sep_token_id]\n      c.append([q]+v[0:i]+[[self.tokenizer.mask_token_id]]+v[i+1:]+[[q[-1]]])\n    b=[[len(sum(x[0:j+1],[])) for j in range(len(x))] for x in c]\n    with torch.no_grad():\n      d=self.model(input_ids=torch.tensor([sum(x,[]) for x in c]),\n        token_type_ids=torch.tensor([[0]*x[0]+[1]*(x[-1]-x[0]) for x in b]))\n    s,e=d.start_logits.tolist(),d.end_logits.tolist()\n    for i in range(n):\n      for j in range(n):\n        m[i+1,0 if i==j else j+1]=s[i][b[i][j]]+e[i][b[i][j+1]-1]\n    h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n    if [0 for i in h if i==0]!=[0]:\n      i=([p for s,e,p in w]+[\"root\"]).index(\"root\")\n      j=i+1 if i<n else numpy.nanargmax(m[:,0])\n      m[0:j,0]=m[j+1:,0]=numpy.nan\n      h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n    u=\"# text = \"+text.replace(\"\\n\",\" \")+\"\\n\"\n    for i,(s,e,p) in enumerate(w,1):\n      p=\"root\" if h[i]==0 else \"dep\" if p==\"root\" else p\n      u+=\"\\t\".join([str(i),r[i-1],\"_\",z[s][0][2:],\"_\",\"|\".join(z[s][1:]),\n        str(h[i]),p,\"_\",\"_\" if i<n and e<w[i][0] else \"SpaceAfter=No\"])+\"\\n\"\n    return u+\"\\n\"\n\nnlp=TransformersUD(\"KoichiYasuoka/deberta-base-japanese-aozora-ud-head\")\nprint(nlp(\"\u5168\u5b66\u5e74\u306b\u308f\u305f\u3063\u3066\u5c0f\u5b66\u6821\u306e\u56fd\u8a9e\u306e\u6559\u79d1\u66f8\u306b\u633f\u3057\u7d75\u304c\u7528\u3044\u3089\u308c\u3066\u3044\u308b\"))\n"], "anhnct/Gligen_Inpainting_Text_Image": ["# Using reference image to add object in generated image\nimport torch\nfrom diffusers import StableDiffusionGLIGENTextImagePipeline\nfrom diffusers.utils import load_image\n\npipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(\"anhnct/Gligen_Inpainting_Text_Image\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\ninput_image = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/livingroom_modern.png\"\n)\n\nprompt = \"a backpack\"\nboxes = [[0.2676, 0.4088, 0.4773, 0.7183]]\nphrases = None\ngligen_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/backpack.jpeg\"\n)\n\nimages = pipe(\n    prompt=prompt,\n    gligen_phrases=phrases,\n    gligen_inpaint_image=input_image,\n    gligen_images=[gligen_image],\n    gligen_boxes=boxes,\n    gligen_scheduled_sampling_beta=1,\n    output_type=\"pil\",\n    num_inference_steps=50,\n).images\n\nimages[0].save(\"./gligen-inpainting-text-image-box.jpg\")\n"], "Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat-GPTQ-Int8\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "laion/CLIP-ViT-B-32-DataComp.S-s13M-b4K": [], "yjernite/bart_eli5": [], "nvidia/nemotron-3-8b-base-4k": ["from nemo.deploy import NemoQuery\n\n# In this case, we run inference on the same machine\nnq = NemoQuery(url=\"localhost:8000\", model_name=\"Nemotron-3-8B-4K\")\n\noutput = nq.query_llm(prompts=[\"The meaning of life is\"], max_output_token=200, top_k=1, top_p=0.0, temperature=0.1)\nprint(output)\n"], "google/t5-xxl-lm-adapt": [], "Qwen/Qwen1.5-4B-Chat-GPTQ-Int8": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-4B-Chat-GPTQ-Int8\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-4B-Chat-GPTQ-Int8\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "Qwen/Qwen1.5-1.8B-Chat-AWQ": ["from transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen1.5-1.8B-Chat-AWQ\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat-AWQ\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"], "google/pix2struct-textcaps-large": ["from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\n\nmodel.push_to_hub(\"USERNAME/MODEL_NAME\")\nprocessor.push_to_hub(\"USERNAME/MODEL_NAME\")\n", "import requests\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n\n# image only\ninputs = processor(images=image, return_tensors=\"pt\")\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n>>> A street scene with a sign that says \"STOP\".\n", "import requests\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-large\").to(\"cuda\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-large\")\n\n# image only\ninputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n>>> A street scene with a sign that says \"STOP\".\n", "import requests\nimport torch\n\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-large\", torch_dtype=torch.bfloat16).to(\"cuda\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-large\")\n\n# image only\ninputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\", torch.bfloat16)\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n>>> A street scene with a sign that says \"STOP\".\n", "import requests\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = \"A picture of\"\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-large\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-large\")\n\n# image only\ninputs = processor(images=image, text=text, return_tensors=\"pt\")\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n"], "sakasa007/finetuning-sentiment-text-mining": [], "laion/CLIP-ViT-B-16-CommonPool.L.text-s1B-b8K": [], "google/tapas-large-finetuned-tabfact": [], "shi-labs/nat-mini-in1k-224": ["from transformers import AutoImageProcessor, NatForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoImageProcessor.from_pretrained(\"shi-labs/nat-mini-in1k-224\")\nmodel = NatForImageClassification.from_pretrained(\"shi-labs/nat-mini-in1k-224\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"], "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1": ["import torch\nfrom transformers import pipeline, BitsAndBytesConfig, AutoTokenizer\n\nmodel_kwargs = {}\n\nquantization_config = None\n# optional quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0,\n)\nmodel_kwargs[\"quantization_config\"] = quantization_config\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\",\n    use_fast=False,\n    padding_side=\"left\",\n    trust_remote_code=True,\n)\n\ngenerate_text = pipeline(\n    model=\"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\",\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    use_fast=False,\n    device_map={\"\": \"cuda:0\"},\n    model_kwargs=model_kwargs,\n)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=1,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)\nprint(res[0][\"generated_text\"])\n", "import torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nquantization_config = None\n# optional quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\",\n    use_fast=False,\n    padding_side=\"left\",\n    trust_remote_code=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cuda:0\"},\n    quantization_config=quantization_config\n).eval()\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=1,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)\nprint(res[0][\"generated_text\"])\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# Important: The prompt needs to be in the same format the model was trained with.\n# You can find an example prompt in the experiment logs.\nprompt = \"<|prompt|>How are you?<|endoftext|><|answer|>\"\n\nquantization_config = None\n# optional quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\",\n    use_fast=False,\n    padding_side=\"left\",\n    trust_remote_code=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cuda:0\"},\n    quantization_config=quantization_config\n).eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n\n# generate configuration can be modified to your needs\ntokens = model.generate(\n    **inputs,\n    min_new_tokens=2,\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=1,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)[0]\n\ntokens = tokens[inputs[\"input_ids\"].shape[1]:]\nanswer = tokenizer.decode(tokens, skip_special_tokens=True)\nprint(answer)\n"], "Qwen/Qwen-14B-Chat-Int8": ["from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-14B-Chat-Int8\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-14B-Chat-Int8\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\nprint(response)\n# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n"], "bhadresh-savani/electra-base-squad2": ["from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/electra-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n", "from farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.infer import Inferencer\n\nmodel_name = \"deepset/electra-base-squad2\"\n\n# a) Get predictions\nnlp = Inferencer.load(model_name, task_type=\"question_answering\")\nQA_input = [{\"questions\": [\"Why is model conversion important?\"],\n             \"text\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}]\nres = nlp.inference_from_dicts(dicts=QA_input)\n\n# b) Load model & tokenizer\nmodel = AdaptiveModel.convert_from_transformers(model_name, device=\"cpu\", task_type=\"question_answering\")\ntokenizer = Tokenizer.load(model_name)\n"], "laion/CLIP-ViT-B-16-CommonPool.L-s1B-b8K": []}