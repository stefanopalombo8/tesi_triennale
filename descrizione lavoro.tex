\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cite}

\title{Descrizione lavoro tesi}
\author{Stefano Palombo}
\date{13 February 2025}

\begin{document}

\maketitle

\section{Introduzione}
Negli ultimi anni, il campo dell'intelligenza artificiale e in particolare del machine learning ha subito una crescita esponenziale portando allo sviluppo di modelli sempre più complessi e performanti.\\
Tra le varie piattaforme che rendono questi modelli più accessibili agli sviluppatori emerge Hugging Face, che offre, tramite librerie open-source, la possibilità di provarli e testarli per attività che spaziano tra la generazione e classificazione di testo naturale, classificazione di immagini, rilvezione di oggetti e molto altro.\\
L'utilità di Hugging Face non risiede soltanto nella disponibilità di modelli, ma anche sulla vasta community che supporta ciascun modello impegnandosi non solo nello sviluppo, ma anche a renderlo fruibile ai developer tramite una documentazione dettagliata del modello, denominata model card. Quest'ultima ne descrive le caratteristiche, le limitazioni e i possibili casi d'uso del modello, includendo esempi pratici in linguaggio Python.\\
L'esperimento di tesi si colloca proprio in questa sezione, dedicata a mostrare come applicare i modelli in scenari di applicazione pratica soffermadosi su metodi e modalità di caricamento, addestramento e inferenza.\\
L'obbietivo è misurare quanto il codice presente sulle model card di Hugging Face sia effettivamente simile all'utilizzo reale del modello in progetti open-source presenti sulla piattaforma Github. Il processo si basa sulla generazione automatica di pattern ricorrenti di codice sorgente Python, intesi come schemi e sequenze formate prevalentemente da funzioni che mostrano la pipeline di impiego di un modello: dal caricamento del modello stesso, alla preparazione dei dati, fino alle fasi di addestramento e inferenza con eventuali parametri specifici...\\
...\\
Per raggiungere questo scopo è stata condotta una ricerca approfondita, tramite data-mining, degli script presenti nei repository GitHub che presentassero l'impiego di modelli di Hugging Face al fine di costruire un dataset strutturato da cui partire con l'analisi successiva...\\
La ricerca di queste sequenze di codice è utile per comprendere come i modelli vengono generalmente implementati dagli sviluppatori, scoprendo potenzialmente tecniche specifiche d'utilizzo, best practise ed eventualti iperparametri che non sono direttamente mostrati nelle varie documentazioni.


\section{Tecnologie e Motivazione}
In questa sezione verrano mostrate le principali tecnologie e i mezzi ricorrenti alla base del processo di approccio:
\begin{itemize}
    \item \textit{PTMs} (\textit{P}re-\textit{T}rained \textit{M}odel\textit{s}), ovvero i modelli pre-addestrati presenti sulla piattaforma HF, oggetto dell'esperimento
    \item \textit{Hugging Face Model card}, rappresenta l'intera sezione di descrizione delle caratteristiche di un particolare modello con annessi eventuali esempi di codice Python sull'utilizzo. È visionabile direttamente al link di un derminato PTM, ex. \textit{https://huggingface.co/sentencetransformers/all-MiniLM-L6-v2}.\\
    In particolare, le principali informazioni inserite nella card riguardano le licenze di sviluppo, task di utilizzo, riferimenti ai dataset di pre-training, limitazioni sui token di elaborazione, eventuali modalità di fine-tuning, inferenza ecc...
    \item Data mining
    \item gli LLM \textit{L}arge \textit{L}anguage \textit{M}odels sono modelli di linguaggio che rappresentano un'innovazione significativa nell'ambito del \(NLP\) (Natural Language Processing), sono costruiti su reti neurali profonde, in particolare sull'architettura \textit{Transformer} che comprende le sequenti componenti principali \cite{di2025use}:
\begin{itemize}
    \item \textit{Encoder} che elabora il testo in input generando rappresentazioni semantica. Nel dettaglio ogni parola viene convertita in token (porzioni parziali o totali della parola) e poi in un vettore (embedding) a cui si aggiunge un ulteriore vettore che ne indica la posizione all'interno della frase permettendo di mantenere l'ordine, successivamente vengono passati al primo sotto-modulo dell'encorder, \textit{Self-Attention Layer} che permette ad ogni parola di guardare tutte le altre per capire il contesto e poi la rete neurale \textit{Feed-Forward Network} trasforma la rappresentazione dell'attenzione in una forma più informativa ovvero catturando aspettiti sintattici più complessi e astratti.
    
    \item \textit{Decoder} che partendendo da queste rappresentazioni genera il testo dell'output. Il funzionamento prevede il modulo \textit{Masked Self-Attention Layer} che impedisce al modello di "guardare" le parole future durante la generazione, garantendo che la predizione successiva del prossimo token si basi solo su quelli già generati. La scelta su quale generare avviene grazie al \textit{Encoder-Decoder Attention Layer} che collega l'output al contesto dell'input ovvero alle rappresentazioni semantiche prodotte dall'encorder, fornendo in output una sorta di "mappa di attivazione" ovvero per ogni parole (token) c'è un punteggio che indica quele sia la più sensata ad essere scelta. Infine queste informazioni vengono ancora raffite dalla stessa \textit{Feed-Forward Network} che le passa alla funzione softmax per calcolare quale parola sia la più popolare.

    \item \textit{Meccanisco dell'attenzione} sia che nell'encoder che nel decoder è presente questo sistema di calcolo dell'attenzione che consente all'LLM di pesare delle parole rispetto ad altre all'interno di una frase. Ogni vocabolo del linguaggio naturale viene visto su tre prospettive diverse Q,V,K \cite{vaswani2017attention}
\end{itemize}
Nella descrizione delle principali componenti si è parlato indistintamente tra parole e token, ma è fondamentale sottolineare che non sono equivalenti. Un token può essere:
\begin{itemize}
    \item una parola intera
    \item porzioni di una parola cioè frammenti come prefissi e suffissi 
    \item singoli caratteri per lingue come il cinese oppure per simboli speciali
    \item subword ovvero combinazioni di lettere che si trovano in più parole
\end{itemize}
Per trasformare le parole del linguaggio naturale in token si utilizzare un tokenizer ovvero una componente che attraverso un pre-processing del testo (rimuozione spazi e normalizzazione) 
Gli LLM sono stati addestrati su un'enorme quantità di dati testuali provenienti da libri, articoli e pagine web acquisendo una forte conoscienza e trovando impiego in numerosi ambiti tra cui la generazione e completamento di codice, assistenza nella documentazione software e traduzioni, sintesi di testuali partendo sempre da un input testuale.\\
Impartire al modello il compito da compiere non è banale per questo si parla di \textit{prompt engineering} come l'arte del formulare input che guidino l'LLM verso le risposte desiderate. Questo può avvenire secondo queste principali tecniche:
\begin{itemize}
    \item \textit{Chain-of-Thought (CoT)} ovvero suddivide problemi complessi in passaggi più semplici per favorire nei confronti dell'LLM un ragionamento sequenziale e coerente.
    \item \textit{Few-shot prompting}: inserire nell'input pochi esempi per orientarlo nello svolgimento di un compito, riducendo la necessità di un addestramento approfondito (fine-tuning)
    \item \textit{Zero-shot prompting} al contrario del precedente, affida al modello un compito senza fornire esempi, sfruttandone la sua capacità di generalizzare.
\end{itemize}
Nel prompt, per ottenere risposte accurate e contestualizzate in specifici ambiti, solitamente si assegna un ruolo 

Un'importante limitazione di questi modelli di linguaggio sono le \textit{allucinazioni} ovvero un comportanmento del LLM che tende a generare risposte plausibili ma sbagliate o prive di fondamento logico. Ciò può essere dovuto a comprensione errata del contesto oppure alla difficoltà nell’interpretare input ambigui o complessi. Nel contesto di questo esperimento un esempio di allucizione si potrebbe verificare quando l'LLM riceve in input del codice Python e deve estrapolarne pattern d'uso ricorrenti ma genera enera funzioni che non hanno alcun collegamento con il codice originale. Questo può essere dovuto all’incapacità del modello di generalizzare correttamente il compito assegnato. Per mitigare questo problema, si è specificato nel prompt cosa fare quando il modello non è in grado di rispondere. Ad esempio, si può esplicitamente indicare che, in caso di incertezza, bisogna restituire una stringa vuota in modo che la successiva analisi dell'output sia più semplice.\\

- parametri (temperatura, top p)



\end{itemize}

Oltre alla misurazione di quanto la generazione automatica di codice che rappresenti l'uso del modello sia simile a quella ufficiale presente sulla piattaforma Huggig Face, i risultati condotti dall'esperimento possono, a mio avviso, essere una fonte di arricchimento soprattutto nelle model card in cui non ci sono esempi di codice.

ESEMPIO DI CARD senza codice

\section{Stato dell'arte}
- paper teams (come mi differenzio)
- llm fine tune 
invio al llm
- riassunto del codice con un llm e un altro llm analizza il riassunto
- ricerca pattern chunk dopo chunk

\section{Produzione dataset file Github}



\section{Approccio}

\subsection{Filtro modelli popolari}
(AGGIUNGERE IMMAGINE DISTRIBUZIONE E METRICHE)\\
\\
Il database prodotto nella parte di tirocinio contiene per 7,325 PTM un totale di 453,260 file sorgenti Python. La distribuzione dei dati è fortemente asimmetrica presentando un'elevata concentrazione di modelli con un numero molto basso di codici mentre solo un ristretto numero di PTM è utilizzato in una quantità significatamente maggiore di file.\\
In particolare, la distribuzione segue un andamento tipico delle distribuzioni \textit{long tail} che si manifesta con una forte asimmetria positiva (right-skewed distribution) evidenziando che la maggior parte dei modelli è utilizzata in pochissimi file mentre modelli altamente popolari sono in una frequenza molto minore.\\
Infatti dalle informazioni statiche la mediana è molto inferiore alla media e la deviazione standard elevata conferma l'alta variabilità tra i modelli. Inoltre, la notevole skewness evidenzia che la distribuzione è sbilanciata a destra, con alcuni modelli molto più utilizzati rispetto alla maggioranza ed infine La curtosi indica la presenza di numerosi outlier, ovvero modelli con un utilizzo estremamente superiore rispetto alla norma.\\
Per evitare possibili distorsioni negli esperimenti successivi dell'approccio, si è deciso quindi di selezionare un sottoinsieme di PTM aventi un numero di file arbitrariamente maggiore di 100 formato da 1,064 modelli per un totale complessivo di 393,484 script.

\subsection{Filtro dei codici sorgenti per lunghezza}
Per migliorare l'efficienza della successiva ricerca delle porzioni più rilevanti dei file si è implementato un processo di selezione degli script basato sulla lunghezza degli stessi. L'idea di base è che alcuni codici troppo brevi potrebbero non contenere informazioni significative sull'utilizzo di un particolare modello e allo stesso modo codici troppo lunghi potrebbero includere porzioni non pertinenti come ad esempio descrizioni e lunghi commenti.\\
Il processo di filtraggio si basa nel considerare solo gli script che hanno una lunghezza in termini di linee di codice comprese nell'intervallo interquantile (IQR) utilizzando la mediana come valore centrale. In particolare, l'IQR è una misura statistica della dispersione dei dati così definita:
\[
IQR = Q3 - Q1
\]
dove Q1 (primo quartile) indica il valore sotto il quale si trova il 25\% dei file più corti mentre Q3 (terzo quartile) il valore sotto il quale si trova il 75\% dei file, quindi il risultato rappresenta l'ampiezza della fascia centrale della distribuzione pari al 50\% dei dati.\\
Nella pratica la mediana e l'IQR sono impiegati nel calcolo di questo intervallo:
\[
min\_length =max(mediana-0.25*IQR,1)
\]
\[
max\_length =mediana+0.25*IQR
\]
dove il fatore 0.25 permette di restringere l'intervallo intorno alla mediana senza essere troppo rigido.\\
Quindi, questo range permette di selezione i file in base alla loro lunghezza attraverso una misura più robusta della media che permette di ecludere la presenza di eventuali outlier concentrandosi su insieme più omogeneo.\\
Per ottimizzare il tempo di elaborazione dei vari calcoli, il processo viene eseguito in parallelo utilizzando il multithreading perché si tratta maggiormente di operazioni di lettura e scrittura du disco (I/O bound), in cui ogni thread lavora su un sottoinsieme di file riducendo l'attesa rispetto ad un'elaborazione sequenziale. Inoltre, l'uso di un \textit{ThreadPoolExecutor} garantisce una gestione automatica delle risorse senza preoccuparsi di eventuali race condition sui file e bilanciando il carico di lavoro tra i thread attivi.

\subsection{Ricerca dei migliori \textit{code snippet}}
L'LLM preso in considerazione per questo esperimento ha una finestra di contesto molto limitata, ovvero non riesce a gestire per ogni chiamata più di un certo numero di token in input e in output. Dove per chiamata si intende complessivamente la richiesta (prompt) comprensiva del codice da analizzare più la riposta.\\
L'ipotecico approccio di far valutare al modello direttamente interi script Python, senza un pre-processing, potrebbe essere molto restrittivo in termini di possibili pattern di codice da scoprire nonché inefficiente perché gli stessi script potrebbero non avere un interessante senso semantico; limitandosi per esempio a menzionare il modello in esame con commenti e descrizioni senza poi impiegarlo realmente.\\
Quindi, si è deciso di analizzare in maniera automatica ogni singolo file filtrato dalla sezione precedente con l'obiettivo di estrapolarne una porzione che fosse la più rappresentativa e rilevante in termini di utilizzo del modello, provando cioè a non considerare per esempio sezioni di debugging o funzioni di utility. Il processo di estrazione è basato sulla ricerca di parole chiavi legate fortemente all'addestramento, tokenizzazione e inferenza standard dei PTM presenti su HF. In particolare, l'implementazione è suddivisa in 5 fasi:\\
\begin{itemize}
    \item \textbf{Analisi della struttura del codice}: per ogni file Python risultante dal calcolo della mediana viene utilizzo un parser per l'Abstract Syntax Tree (AST), ovvero una rappresentazione strutturata sintattica del codice sorgente, che permettere di iterare sui nodi identificando importazioni di librerie e definizioni di funzioni (incluso il corpo) che facciano match con almeno una delle keyword precedentemente definite. La corrispondenza avviene in una determinata linea di codice, quindi per avere un contesto sufficientemente comprensile sono state aggiunte allo snippet due righe prima e dopo.\\
    Tuttavia, può accadere che il parser sull'AST incontri errori di sintassi che impediscono la succesiva analisi del codice, come ad esempio parentesi mancanti, indentazioni errate, stringhe non chiuse correttamente ecc. In questi casi l'errore viene solo mostrato a schermo e il file scartato per il processo.
    
    \item \textbf{Raggrupamento delle informazioni} in seguito all'individuazione delle diverse porzioni pertinenti è necessario unificare eventuali frammenti vicini tra loro (che condividono alcune linee di codice) per evitare ripetizioni rindondanti. 

    \item \textbf{Ricerca del miglior snippet} l'analisi sintattica con il raggruppamento restituisce per ogni file un insieme di n snippet che contengono ognuno almeno un match con una delle stringhe di keyword, ma per scegliere il migliore tra i candidati è necessario considerare il significato semantico del codice.
    Questo è reso possibile dall'algoritmo K-means implementato con la libreria \textit{scikit-learn} che lavora con una rappresentanzione numerica delle informazioni. Essendo gli snippet espressi sotto forma di stringhe si è adottato un modello di embeddings, reso disponibile proprio da HF con la libreria \textit{sentence\_transformers}, il cui compito è proprio di fare l'encoding della stringa fasi di  tokenizzazione delle parole, calcoli con l'architettura \textit{Transformer} e produzione di un vettore in una dimensione di precisamente 384 componenti.\\
    Quindi, gli embeddings vengono passati al K-means che li assegna in un numero cluster (scelto empiricamente dal minino tra un valore euristico fisso a 3 e la lunghezza degli snippet) il cui centroide (vettore medio) è più vicino (calcolando la distanza euclidea) ad essi. Inizialmente i centroidi vengono inizializzati in modo che siano più distanziati possibile uno dall'altro e poi iteretivamente man mano che si aggiungono embedding ai cluster si ricalcolano i centroidi finchè non si stabilizzano.\\
    Infine, si itera nuovamente sui cluster scegliendo tra gli elementi presenti quello che si trova più vicino al centroide di riferimento ovvero lo snippet che riassume al meglio le caratteristiche del gruppo.

    \item \textbf{Elaborazione parallela} anche in questa sezione per gestire un gran numero di file, il processo viene eseguito in parallelo sfruttando il multithreading per un'elaborazione più veloce.

    \item \textbf{Salvataggio informazioni}




\subsection{Utilizzo dell'LLM}

\subsubsection{Flusso di lavoro}
L'idea di base è fornire, per ogni modello in esame, il maggior numero possibile di snippet, ccercando di massimizzare l’utilizzo dei token di input disponibili per l’LLM. Per garantire coerenza e confrontabilità tra le varie chiamate, viene utilizzato un \textit{prompt fisso} per tutti i modelli. Il prompt ha come unico scopo di istruire il modello a individuare eclusivamente pattern di codice, come importazioni comuni, definizioni di funzione e strutture tipiche di utilizzo dei modelli PTM.
(foto prompt con annessa spiegazione)

Per la scelta delle porzioni di codice da inviare al modello, viene effettuato un primo filtro: tramite espressioni regolari vengono selezionati solo gli snippet contenenti almeno il nome del modello e la keyword "def". Questo controllo permette agli snippet di essere pertinenti con il modello in esame e di includere almeno una definizione di funzione rendendolo rilevante per l'identificazione di pattern funzionali.\\
L'input per l'LLM viene costruito in maniera incrementale, ovvero si aggiunge uno snippet alla stringa da inviare fin quando non si supera il limite di token previsto, calcolando con il \textit{tokenizer} le varie stringhe:
\[
allowed\_input\_tokens = max\_total\_tokens - system\_tokens - reserved\_output\_tokens
\]
dove
\begin{itemize}
    \item \textit{allowed\_input\_tokens} indica il numero di token disponibili, che aumenta con il crescere degli snippet selezionati
    \item \textit{max\_total\_tokens} limite imposto dal modello ovvero 4096
    \item \textit{system\_tokens} numero token riservati al prompt di sistema, considerabile costante trascurando il nome del modello
    \item \textit{reserved\_output\_tokens} termine costante pari a 400
\end{itemize}
Per ogni risposta ricevuta dall'LLM, l'algoritmo implementa una semplice logica di validazione che consiste nel verificare se l'output contiene il nome del modello (sempre con l'intento di ricercare pattern pertinenti) e almeno le keyword "import" e "def" per assicurarsi che il risultato contenga sezioni di codice Python; se l'esito è negativo si effettuano 2 nuovi tentativi con lo stesso input (perché) ripentendo il controllo. Nel caso in cui anche dopo 3 tentivi complessivi il risultato non è pertinente si considera l'ultimo prodotto  


L’output valido viene infine salvato in un file JSON strutturato, contenente:  
\begin{itemize}
    \item Il prompt utilizzato.
    \item L'input inviato cioè gli snippet scelti randomicamente.
    \item La risposta generata dall’LLM.
    \item Il numero totale di token utilizzati e i tentativi effettuati.
\end{itemize}

\subsubsection{Esempio di card generata}


    
\end{itemize}

\section{Validazione e risultati}



\subsection{Scelta LLM e modalità}
Nel contesto di questo esperimento, è stato sfruttuano un particolare LLM opensource per analizzare gli snippet di codice Python estratti precedentemente, al fine di indiduare schemi comuni nell'utilizzo dei PTM, tramite le principali librerie della piattaforma di Hugging Face.
L'obiettivo è di isolare pattern significativi senza dover eseguire un'analisi manuale per ciascun modello esaminato, riducendo tempi e sforzi operativi.\\
L'LLM adoperato per l'intero processo è \textbf{meta-llama/Llama-3.2-3B-Instruct}, sviluppato appunto da meta e appartenente alla collezione \textit{Llama 3.2}. Questa versione è progettata per essere utilizzata in compiti di tipo "instruct" (ovvero guidata da prompt specifici) e supporta l’elaborazione di un massimo di 4.096 token per ogni chiamata, suddivisi tra input e output.  
Per accedere al modello, è stato utilizzato il servizio \textit{Hugging Face Inference API} un sistema consente di inviare richieste autenticate tramite token personale e ottenere risposte direttamente dai modelli open-source disponibili .\\
Questo approccio offre diversi vantaggi:
\begin{itemize}
    \item Eliminazione vincoli hardware: eseguire un LLM di queste dimensioni su una macchina locale richiede un potenza di calcolo media-alta con la necessità di disporre di GPU dedicata con ampie quantità di VRAM
    \item Semplicità d'uso e deployment immediato: non è neccassario installare localmente il modello nè configurare l'ambiente perché l'API fornisce un'interfaccia semplice per inviare e ricevere risposte facilmente integrabile nel proprio sistema
    \item Ottimizzazione dei tempi di risposta: la richiesta remota comporta, ovviamente, una latenza di rete ma il tempo complessivo rimane sempre inferiore rispetto al gestire locale il modello (con un hardware limitato)
\end{itemize}
\subsection{Valutazione}
L'obbietivo principale è valutare l'efficacia e la qualità delle model card generate automaticamente dall'LLM rispetto a quelle ufficiali presenti sulla piattaforma di Hugging Face.  Questo confronto ha lo scopo di misurare quanto i codici creati siano pertinenti e, soprattutto, utili per gli sviluppatori finali, che si potranno affidare a questi esempi per comprendere come utilizzare correttamente i PTM.

\subsubsection{Costruzione dataset di test}
Nel dump \cite{ait_hfcommunity_2023} nonostance ci fosse la feature "card data" ovvero le informazioni riguardanti la model card di ogni PTM, non erano presenti gli snippet di codice Python che mostrano l'utilizzo del modello specifico in un esempio di applicazione.\\  
Quindi, per ottenere queste sezioni della card di HF, si è sviluppato un semplice script che utilizza la libreria \textit{BeautifulSoup}\cite{Richardson_Beautiful_Soup} per effettuare web scarping delle pagine HTML dei PTM popolari selezioni per l'esperimento. Ad ogni iterazione si ricerca il tag \textit{\textless code\textgreater} e quando avviene la corrispondenza si controlla che il risultato sia effettivamente appartenente al linguaggio Python, quindi semplicemente verificando che nella stringa siano presenti le keyword "import", "from" e "def".\\
Si noti che per ogni PTM analizzato lo scraper potrebbe trovare anche più di uno snippet Python, perché nella stessa model card potrebbe venir illustrate diverse pipeline di utilizzo: dall'importazione con diverse librerie (per esempio \textit{transformers} e \textit{sentence\_transformers}) fino a metodi di encoding tramite embeddings.\\
Il dataset è strutturato secondo il seguente formato JSON:\\
\\
Dall'analisi effettuata il risultato è di 751 modelli che nelle proprie model card contengano almeno uno snippet di codice Python rispetto ai 1064 di partenza. La differenza è composta quindi da PTM che nonostante siano mediamente popolari sull'utilizzo pratico in progetti open-source abbiano una documentazione probabilmente non completa ed efficace agli sviluppatori; ciò motiva maggiormente lo scopo generale di questo studio.

\subsubsection{Metriche}
Dopo aver raccolto le card ufficiali, si è proceduti nel valutare quanto fossero simili a quelle generate dall'LLM. Sono state impiegate tre metriche di confronto che valutano la somiglianza a livello lessicale, semantico e strutturale del codice:
\begin{itemize}
    \item \textbf{METEOR} (Metric for Evaluation of Translation with Explicit ORdering) è progettata principalmente per la valuzione di traduzioni automatiche confrontando le parole secondo un matching flessibile ovvero la corrispondenza si basa su sinonimi, stemmatura e ordine con cui compaiono. Questa flessibilità permette di catturare non solo l'aspetto sintattico ma anche una parziale sensibilità semantica perché, per esempio, la definizione di una funzione potrebbe essere scritta in due modi diversi ma rappresentare lo stesso significato.\\
    Il calcolo della metriche è il seguente:
    \[
    \text{METEOR} = (1 - \text{penalty}) \cdot F_{\text{mean}}
    \]
    dove:
    \[
        \text{penalty} = \gamma \left( \frac{\text{numero di chunk}}{\text{numero di parole corrispondenti}} \right)^\beta
    \]
    \[
        F_{\text{mean}} = \frac{10 \cdot P \cdot R}{9P + R}
    \]
    \begin{itemize}
        \item \textbf{P (Precisione)} = $\frac{\text{numero di parole corrispondenti}}{\text{numero totale di parole nel candidato}}$
        \item \textbf{R (Richiamo)} = $\frac{\text{numero di parole corrispondenti}}{\text{numero totale di parole nella referenza}}$
    \end{itemize}
    La penalità considera la frammentazione e la disposizione delle parole tra il testo canditato (la card generata dall'LLM) e quello referente (card ufficiale su HF). In particolare il numero di chunk rappresenta quante sequenze di parole corrispondenti ci sono, quindi, più è frammentata l'affinità, maggiori saranno i chunk. Gamma e beta, invece, sono parametri scelti empiricamente in base all'implementazione e bilanciano il livello della penalità che, in generale, è bassa o quasi nulla quando le parole che fanno match sono sono tutte in questa, viceversa se sono sparse in disordine aumenta.\\
    Successivamente la media armonica pesata è progettata per dare più importanza al richiamo rispetto alla precisione perché nelle applicazioni pratiche di traduzioni e descrizioni è spesso più dannosso non considerare informazioni importanti (basso richiamo) rispetto ad includere termini e dettagli aggiungi non necessari (bassa precisione). Quindi, si preferisce considerare le descrizioni (card) potenzialmente più utili ovvero quelle che forniscono più informazioni dettagliate a costo di qualche aggiunta superflua e non strettamente necessaria, come posso essere, in questo contesto, l'eventuale presenza di funzioni Python di utility che non riguardano l'utilizzo dei PTM in maniera diretta.\\
    La metrica METEOR, in generale, non è ideale per il confronto di codice sorgente in quanto è progettata per la valuzione tra stringhe del linguaggio naturale attraverso un dizionario che non è specifico del linguaggio Python. Ma ci sono comunque dei motivi validi per considerlarla nell'esperimento:
    \begin{itemize}
        \item parziale sensibilità semantica con l'utilizzo dei sinonimi per variabili e soprattutto per definizione di funzioni
        \item l'importanza del richiamo per verificare che il codice generato copra tutti gli elementi del codice di riferimento (pipeline di utilizzo del PTM)
        \item la penalizzazione sull'ordine delle istruzioni del codice sorgente può essere abbastanza rilevante che se in genere l'organizzazione delle istruzioni è flessbile
    \end{itemize}
    Nel complesso potrebbe aiutare a catturare aspetti che altre metriche non riescono, ma deve essere considerata come parte di un multi-approccio e non come una soluzione unica ed ideale.
    
    \item CodeBLEU \cite{ren2020codebleumethodautomaticevaluation} è una metrica progettata, invece, per valutare la qualità di un codice generato candidato rispetto a un riferimento
    \item cosine similarity con Embedding
\end{itemize}


\subsection{Risultati}
research question
grafici e statiche e motivazioni perché sono basse (pensate per utilizzo umano)


\section{Limitazioni}
- llm scarso\\
- rate limit API


\section{Conclusioni e sviluppi futuri}
sviluppi:
\begin{itemize}
    \item far generare tutta la card non solo codice
    \item possibile applicazione pratica è predirre proprio la card nel proprio progetto
\end{itemize}

\bibliographystyle{plainurl}
\bibliography{riferimenti}
\appendix


\end{document}