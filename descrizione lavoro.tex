\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Descrizione lavoro tesi}
\author{Stefano Palombo}
\date{13 February 2025 : 23:45}

\begin{document}

\maketitle

\section{Filtro modelli popolari}
(AGGIUNGERE IMMAGINE DISTRIBUZIONE E METRICHE)\\
\\
Il database prodotto nella parte di tirocinio contiene per 7,325 PTM un totale di 453,260 file sorgenti Python. La distribuzione dei dati è fortemente asimmetrica presentando un'elevata concentrazione di modelli con un numero molto basso di codici mentre solo un ristretto numero di PTM è utilizzato in una quantità significatamente maggiore di file.\\
In particolare, la distribuzione segue un andamento tipico delle distribuzioni \textit{long tail} che si manifesta con una forte asimmetria positiva (right-skewed distribution) evidenziando che la maggior parte dei modelli è utilizzata in pochissimi file mentre modelli altamente popolari sono in una frequenza molto minore.\\
Infatti dalle informazioni statiche la mediana è molto inferiore alla media e la deviazione standard elevata conferma l'alta variabilità tra i modelli. Inoltre, la notevole skewness evidenzia che la distribuzione è sbilanciata a destra, con alcuni modelli molto più utilizzati rispetto alla maggioranza ed infine La curtosi indica la presenza di numerosi outlier, ovvero modelli con un utilizzo estremamente superiore rispetto alla norma.\\
Per evitare possibili distorsioni negli esperimenti successivi dell'approccio, si è deciso quindi di selezionare un sottoinsieme di PTM aventi un numero di file arbitrariamente maggiore di 100 formato da 1,064 modelli per un totale complessivo di 393,484 script.

\section{Filtro dei codici sorgenti per lunghezza}
Per migliorare l'efficienza della successiva ricerca delle porzioni più rilevanti dei file si è implementato un processo di selezione degli script basato sulla lunghezza degli stessi. L'idea di base è che alcuni codici troppo brevi potrebbero non contenere informazioni significative sull'utilizzo di un particolare modello e allo stesso modo codici troppo lunghi potrebbero includere porzioni non pertinenti come ad esempio descrizioni e lunghi commenti.\\
Il processo di filtraggio si basa nel considerare solo gli script che hanno una lunghezza in termini di linee di codice comprese nell'intervallo interquantile (IQR) utilizzando la mediana come valore centrale. In particolare, l'IQR è una misura statistica della dispersione dei dati così definita:
\[
IQR = Q3 - Q1
\]
dove Q1 (primo quartile) indica il valore sotto il quale si trova il 25\% dei file più corti mentre Q3 (terzo quartile) il valore sotto il quale si trova il 75\% dei file, quindi il risultato rappresenta l'ampiezza della fascia centrale della distribuzione pari al 50\% dei dati.\\
Nella pratica la mediana e l'IQR sono impiegati nel calcolo di questo intervallo:
\[
min\_length =max(mediana-0.25*IQR,1)
\]
\[
max\_length =mediana+0.25*IQR
\]
dove il fatore 0.25 permette di restringere l'intervallo intorno alla mediana senza essere troppo rigido.\\
Quindi, questo range permette di selezione i file in base alla loro lunghezza attraverso una misura più robusta della media che permette di ecludere la presenza di eventuali outlier concentrandosi su insieme più omogeneo.\\
Per ottimizzare il tempo di elaborazione dei vari calcoli, il processo viene eseguito in parallelo utilizzando il multithreading, in cui ogni thread lavora su un sottoinsieme di file riducendo l'attesa rispetto ad un'elaborazione sequenziale. Inoltre, l'uso di un \textit{ThreadPoolExecutor} garantisce una gestione automatica delle risorse senza preoccuparsi di eventuali race condition sui file e bilanciando il carico di lavoro tra i thread attivi.

\section{Ricerca dei migliori \textit{code snippet}}
L'LLM preso in considerazione per questo esperimento ha una finestra di contesto molto limitata, ovvero non riesce a gestire per ogni chiamata più di un certo numero di token in input e in output. Dove per chiamata si intende complessivamente la richiesta (prompt) comprensiva del codice da analizzare più la riposta.\\
L'ipotecico approccio di far valutare al modello direttamente interi script Python, senza un pre-processing, potrebbe essere molto restrittivo in termini di possibili pattern di codice da scoprire nonché inefficiente perché gli stessi script potrebbero non avere un interessante senso semantico; limitandosi per esempio a menzionare il modello in esame con commenti e descrizioni senza poi impiegarlo realmente.\\
Quindi, si è deciso di analizzare in maniera automatica ogni singolo file filtrato dalla sezione precedente con l'obiettivo di estrapolarne una porzione che fosse la più rappresentativa e rilevante in termini di utilizzo del modello, provando cioè a non considerare per esempio sezioni di debugging o funzioni di utility. Il processo di estrazione è basato sulla ricerca di parole chiavi legate fortemente all'addestramento, tokenizzazione e inferenza standard dei PTM presenti su HF. In particolare, l'implementazione è suddivisa in 5 fasi:\\
\begin{itemize}
    \item \textbf{Analisi della struttura del codice}: per ogni file Python risultante dal calcolo della mediana viene utilizzo un parser per l'Abstract Syntax Tree (AST), ovvero una rappresentazione strutturata sintattica del codice sorgente, che permettere di iterare sui nodi identificando importazioni di librerie e definizioni di funzioni (incluso il corpo) che facciano match con almeno una delle keyword precedentemente definite. La corrispondenza avviene in una determinata linea di codice, quindi per avere un contesto sufficientemente comprensile sono state aggiunte allo snippet due righe prima e dopo.\\
    Tuttavia, può accadere che il parser sull'AST incontri errori di sintassi che impediscono la succesiva analisi del codice, come ad esempio parentesi mancanti, indentazioni errate, stringhe non chiuse correttamente ecc. In questi casi l'errore viene solo mostrato a schermo e il file scartato per il processo.
    
    \item \textbf{Raggrupamento delle informazioni} in seguito all'individuazione delle diverse porzioni pertinenti è necessario unificare eventuali frammenti vicini tra loro (che condividono alcune linee di codice) per evitare ripetizioni rindondanti. 

    \item \textbf{Ricerca del miglior snippet} l'analisi sintattica con il raggruppamento restituisce per ogni file un insieme di n snippet che contengono ognuno almeno un match con una delle stringhe di keyword, ma per scegliere il migliore tra i candidati è necessario considerare il significato semantico del codice.
    Questo è reso possibile dall'algoritmo K-means implementato con la libreria \textit{scikit-learn} che lavora con una rappresentanzione numerica delle informazioni. Essendo gli snippet espressi sotto forma di stringhe si è adottato un modello di embeddings, reso disponibile proprio da HF con la libreria \textit{sentence\_transformers}, il cui compito è proprio di fare l'encoding della stringa fasi di  tokenizzazione delle parole, calcoli con l'architettura \textit{Transformer} e produzione di un vettore in una dimensione di precisamente 384 componenti.\\
    Quindi, gli embeddings vengono passati al K-means che li assegna in un numero cluster (scelto empiricamente dal minino tra un valore euristico fisso a 3 e la lunghezza degli snippet) il cui centroide (vettore medio) è più vicino (calcolando la distanza euclidea) ad essi. Inizialmente i centroidi vengono inizializzati in modo che siano più distanziati possibile uno dall'altro e poi iteretivamente man mano che si aggiungono embedding ai cluster si ricalcolano i centroidi finchè non si stabilizzano.\\
    Infine, si itera nuovamente sui cluster scegliendo tra gli elementi presenti quello che si trova più vicino al centroide di riferimento ovvero lo snippet che riassume al meglio le caratteristiche del gruppo.

    \item \textbf{Elaborazione parallela} anche in questa sezione per gestire un gran numero di file, il processo viene eseguito in parallelo sfruttando il multithreading per un'elaborazione più veloce.

    \item \textbf{Salvataggio informazioni} JSON
    

\section{LLM}
    

    
\end{itemize}


\end{document}
